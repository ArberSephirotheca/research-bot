# GPU Research Digest — 2026-01-09
Generated: 2026-01-09T14:09:07.585015371+00:00
Sources: 12
New items: 2
Already seen: 20
Window: last 90 days
Sources failed: 3

## arXiv: GPU OR CUDA
- No new items found in the last 90 days.

## arXiv: ML + GPU
- No new items found in the last 90 days.

## arXiv: Matrix Multiplication + GPU
- No new items found in the last 90 days.

## arXiv: AI/ML + GPU
- No new items found in the last 90 days.

## Search: last 90 days Hopper H100 tensor cores FP8 GEMM microarchitecture paper
- No new items found in the last 90 days.

## Search: last 90 days Blackwell B200 tensor core matmul performance analysis CUDA (error)
- Fetch error: web_search returned no in-window results (cutoff 2025-10-11)

## Search: last 90 days CUTLASS 3.x new GEMM kernels SM90 SM100 release notes (new: 1, showing: 1)
- [CUTLASS 4.3.5 (GitHub Release)](https://github.com/NVIDIA/cutlass/releases/tag/v4.3.5) — 2026-01-09
  - Authors: n/a
  - Affiliations: not provided
  - CUTLASS 4.3.5 is a maintenance release focused on correctness/stability, including CuTe DSL bug fixes that can affect generated kernel behavior for GEMM and related tensor ops.
  - Addresses a CPU-side overhead regression introduced in 4.3.4, improving host launch/setup costs for CUTLASS workloads (relevant when dispatching many small/medium GEMMs).
  - Overall: no new GEMM features highlighted; primarily fixes to keep matmul pipelines reliable and reduce non-GPU bottlenecks.

## Search: last 90 days cuBLASLt GEMM autotuning heuristics FP8 BF16 performance update
- No new items found in the last 90 days.

## Search: last 90 days Triton compiler matmul kernel fusion persistent kernels research
- No new items found in the last 90 days.

## Search: last 90 days FlashAttention-3 CUDA kernel implementation tensor core GEMM details (new: 1, showing: 1)
- [FlashAttention-3:Fast and Accurate Attention with Asynchrony and Low-precision——利用异步和低精度实现快速准确的注意力机制_flashattention3-CSDN博客](https://blog.csdn.net/Together_CZ/article/details/143786789) — 2025-12-19
  - Authors: n/a
  - Affiliations: not provided
  - FlashAttention-3 通过将 attention 拆成多阶段流水：在不同 warp-group 间做“ping-pong”调度，把 softmax/归一化等计算与 QKᵀ、PV 等 GEMM（或类 GEMM 的 MMA）异步重叠，以提高 SM 利用率并隐藏内存/指令延迟。
  - 讨论了 SFU（special function unit）吞吐成为瓶颈的情况（如 exp/log/rsqrt 等），指出即使 GEMM 很快，softmax 相关特殊函数也可能限制整体速度；因此需要在调度上尽量把 SFU 阶段与矩阵乘阶段并行。
  - 低精度（如 FP16/BF16/FP8）用于提升 Tensor Core 吞吐，但需配

## Search: last 90 days GPU systolic array mapping matrix multiplication tensor cores research (error)
- Fetch error: web_search returned no in-window results (cutoff 2025-10-11)

## Search: last 90 days MLIR LLVM CUDA GPU compiler matmul codegen tensor cores research (error)
- Fetch error: web_search returned no in-window results (cutoff 2025-10-11)
