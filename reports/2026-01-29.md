# GPU Research Digest — 2026-01-29
Generated: 2026-01-29T14:20:00.243596837+00:00
Sources: 12
New items: 2
Already seen: 13
Window: last 90 days
Sources failed: 5

## arXiv: GPU OR CUDA (new: 1, showing: 1)
- [GPU-Augmented OLAP Execution Engine: GPU Offloading](https://arxiv.org/abs/2601.19911v1) — 2025-12-24
  - Authors: Ilsun Chang
  - Affiliations: not provided
  - Selectively offloads OLAP execution hot spots (Top‑K selection, join probe) from a vectorized CPU engine to GPU, using a risk-aware “Risky Gate” that enables GPU only when predicted gain outweighs transfer/kernel/post-processing risk—improving tail latency (P95/P99) vs always-on offload.
  - Minimizes PCIe/NVLink traffic via key-only transfer (keys + pointers) and late materialization, keeping full tuples on CPU until needed; gating also accounts for candidate-set complexity (e.g., K for Top‑K, M for join probe).
  - No matrix multiplication focus; GPU use is for database primitives (selection/probe/Top‑K), not GEMM/ML workloads.

## arXiv: ML + GPU
- No new items found in the last 90 days.

## arXiv: Matrix Multiplication + GPU
- No new items found in the last 90 days.

## arXiv: AI/ML + GPU
- No new items found in the last 90 days.

## Search: last 90 days Hopper H100 tensor cores FP8 GEMM microarchitecture paper
- No new items found in the last 90 days.

## Search: last 90 days Blackwell B200 tensor core GEMM performance analysis CUDA (error)
- Fetch error: web_search returned no in-window results (cutoff 2025-10-31)

## Search: last 90 days CUTLASS 3.x new GEMM kernels SM90 SM100 release notes (error)
- Fetch error: OpenAI response missing output text for web_search: {
  "id": "resp_079afd5b19a27fa900697b6c462cec819c97d7c7f45fd248c9",
  "object": "response",
  "created_at": 1769696326,
  "status": "completed",
  "background": false,
  "billing": {
    "payer": "developer"
  },
  "completed_at": 1769696337,
  "error": null,
  "frequency_penalty": 0.0,
  "incomplete_details": null,
  "instructions": null,
  "max_output_tokens": 500,
  "max_tool_calls": null,
  "model": "gpt-5.2-2025-12-11",
  "output": [
    {
      "id": "ws_079afd5b19a27fa900697b6c46eae4819c9d3ccd71051b7249",
      "type": "web_search_call",
      "status": "completed",
      "action": {
        "type": "search",
        "queries": [
          "CUTLASS 3.x new GEMM kernels SM90 SM100 release notes",
          "CUTLASS 3.x SM100 GEMM kernel release notes",
          "CUTLASS 3.x SM90 GEMM kernel release notes",
          "site:github.com/NVIDIA/cutlass release notes 3.x SM90 SM100 GEMM"
        ],
        "query": "CUTLASS 3.x new GEMM kernels SM90 SM100 release notes"
      }
    }...

## Search: last 90 days cuBLASLt GEMM autotuning heuristics FP8 BF16 Hopper Blackwell
- No new items found in the last 90 days.

## Search: last 90 days Triton compiler matmul kernel fusion shared memory pipelining research (error)
- Fetch error: web_search returned no in-window results (cutoff 2025-10-31)

## Search: last 90 days FlashAttention-3 CUDA kernel GEMM epilogue fusion tensor cores (error)
- Fetch error: web_search returned no in-window results (cutoff 2025-10-31)

## Search: last 90 days systolic array vs GPU tensor cores matrix multiplication throughput study (new: 1, showing: 1)
- [Architectural Divergence in AI Accelerators: Beyond the Memory Wall and Systolic Arrays](https://medium.com/@kvnagesh/architectural-divergence-in-ai-accelerators-beyond-the-memory-wall-and-systolic-arrays-de19dd2d973f) — 2026-01-01
  - Authors: n/a
  - Affiliations: not provided
  - Contrasts GPU tensor cores (SIMT-friendly, flexible tiling) with systolic-array tensor engines (fixed dataflow), showing how each maps GEMM and where utilization drops when shapes/strides don’t match the preferred tile/dataflow.
  - Argues GEMM performance is increasingly bounded by memory bandwidth and data movement (not just FLOPs), so practical speedups hinge on raising arithmetic intensity via better reuse (tiling, fusion, on-chip buffering) rather than larger compute arrays alone.
  - For GPU practitioners: highlights when tensor-core GEMMs become memory-wall limited (small/skinny matrices, low batch, poor reuse) and suggests prioritizing layout/tiling choices that maximize L2/SMEM reuse and avoid bandwidth-dominated regimes.

## Search: last 90 days GPU compiler MLIR LLVM CUDA matmul scheduling tensor core lowering (error)
- Fetch error: web_search returned no in-window results (cutoff 2025-10-31)
