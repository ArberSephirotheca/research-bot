# GPU Research Digest — 2026-02-16
Generated: 2026-02-16T14:23:02.347458820+00:00
Sources: 12
New items: 4
Already seen: 17
Window: last 90 days
Sources failed: 5

## arXiv: GPU OR CUDA
- No new items found in the last 90 days.

## arXiv: ML + GPU (new: 1, showing: 1)
- [GPU-Fuzz: Finding Memory Errors in Deep Learning Frameworks](https://arxiv.org/abs/2602.10478v2) — 2026-02-11
  - Authors: Zihao Li, Hongyi Lu, Yanan Guo, Zhenkai Zhang, Shuai Wang, Fengwei Zhang
  - Affiliations: not provided
  - Proposes **GPU-Fuzz**, a GPU-kernel–focused fuzzer that models DL operator parameters as **formal constraints** and uses a **constraint solver** to generate boundary-condition test cases that trigger GPU memory errors (e.g., OOB, illegal access) efficiently.
  - Demonstrates practical impact on major GPU-backed DL frameworks (**PyTorch, TensorFlow, PaddlePaddle**), finding **13 previously unknown GPU memory bugs**, highlighting risk to stability and security in CUDA kernels.
  - No specific emphasis on **matrix multiplication/GEMM** in the abstract; approach targets GPU operators broadly by systematically exploring parameter edge cases that stress kernel indexing and memory bounds.

## arXiv: Matrix Multiplication + GPU
- No new items found in the last 90 days.

## arXiv: AI/ML + GPU (new: 1, showing: 1)
- [OptiML: An End-to-End Framework for Program Synthesis and CUDA Kernel Optimization](https://arxiv.org/abs/2602.12305v1) — 2026-02-12
  - Authors: Arijit Bhattacharjee, Heng Ping, Son Vu Le, Paul Bogdan, Nesreen K. Ahmed, Ali Jannesari
  - Affiliations: not provided
  - End-to-end CUDA kernel synthesis + optimization: LLM proposes an initial kernel (from natural language or existing CUDA), then a Monte Carlo Tree Search explores LLM-generated code edits, compiling + verifying correctness and profiling each candidate with Nsight Compute.
  - GPU-performance-driven search: uses a composite reward combining runtime with hardware bottleneck proxies from profiler counters (and guardrails to avoid regressions), enabling systematic navigation of low-level CUDA optimization choices under noisy feedback.
  - Not specifically focused on matrix multiplication (GEMM); targets a diverse set of CUDA kernels and produces profiler-grounded, interpretable optimization trajectories.

## Search: last 90 days Hopper H100 tensor cores FP8 GEMM microarchitecture paper (error)
- Fetch error: web_search returned no in-window results (cutoff 2025-11-18)

## Search: last 90 days Blackwell B200 tensor core architecture GEMM throughput research (error)
- Fetch error: web_search returned no in-window results (cutoff 2025-11-18)

## Search: last 90 days CUTLASS 3.x new GEMM kernels SM90 SM100 blog paper (new: 1, showing: 1)
- [NATTEN · PyPI](https://pypi.org/project/NATTEN/) — 2026-02-08
  - Authors: n/a
  - Affiliations: not provided
  - Latest NATTEN PyPI release notes explicitly acknowledge NVIDIA CUTLASS, indicating reliance on/benefit from CUTLASS-optimized GPU kernels (notably GEMM/matrix-multiply–style building blocks) for performance.
  - The release history provides the exact published date for the newest version, useful for tracking when GPU-kernel/performance changes landed.
  - As a packaging/release page, it doesn’t present new algorithmic results, but it signals ongoing GPU-focused optimization work via CUTLASS integration.

## Search: last 90 days cuBLASLt FP8 GEMM autotuning heuristics performance analysis
- No new items found in the last 90 days.

## Search: last 90 days Triton compiler matmul kernel fusion epilogue scheduling research (new: 1, showing: 1)
- [Warp Specialization in Triton: Design and Roadmap](https://pytorch.org/blog/warp-specialization-in-triton-design-and-roadmap/) — 2026-01-08
  - Authors: n/a
  - Affiliations: not provided
  - Introduces Triton “autoWS” warp specialization: at JIT time, the compiler explores scheduling/specialization choices to split work across warps (e.g., compute vs. memory/epilogue roles) to improve latency hiding and utilization on NVIDIA GPUs.
  - Describes new lowering/search passes that automatically pick warp-level schedules for large fused “megakernels,” aiming to keep data in registers/shared memory longer and reduce global-memory traffic and launch overhead.
  - For GEMM-like workloads, positions warp specialization as a path to better matmul+epilogue fusion (bias/activation/normalization) by coordinating warp responsibilities and pipelining loads/compute to sustain higher throughput.

## Search: last 90 days FlashAttention-3 GPU kernel matmul softmax tensor cores benchmark (error)
- Fetch error: web_search returned no in-window results (cutoff 2025-11-18)

## Search: last 90 days systolic array vs tensor cores matrix multiplication mapping GPU research (error)
- Fetch error: web_search returned no in-window results (cutoff 2025-11-18)

## Search: last 90 days GPU compiler MLIR LLVM CUDA GEMM codegen tensor core MMA research (error)
- Fetch error: web_search returned no in-window results (cutoff 2025-11-18)
