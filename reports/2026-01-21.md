# GPU Research Digest — 2026-01-21
Generated: 2026-01-21T14:14:41.493083151+00:00
Sources: 12
New items: 5
Already seen: 19
Window: last 90 days
Sources failed: 3

## arXiv: GPU OR CUDA
- No new items found in the last 90 days.

## arXiv: ML + GPU
- No new items found in the last 90 days.

## arXiv: Matrix Multiplication + GPU
- No new items found in the last 90 days.

## arXiv: AI/ML + GPU
- No new items found in the last 90 days.

## Search: last 90 days Hopper H100 tensor core WMMA GMMA matrix multiplication research paper (new: 1, showing: 1)
- [Wmma vs Wgmma On H100 GPU - CUDA Programming and Performance - NVIDIA Developer Forums](https://forums.developer.nvidia.com/t/wmma-vs-wgmma-on-h100-gpu/354730) — 2025-12-15
  - Authors: n/a
  - Affiliations: not provided
  - On H100, WGMMA (warp-group MMA) is the preferred path for FP16 HGEMM versus legacy WMMA: it maps to newer Tensor Core instructions, enables higher throughput, and is designed to better utilize SM resources when kernels are structured around warp-groups.
  - Profiling/optimization discussion centers on whether the kernel is Tensor Core–bound or limited by memory/launch/occupancy; achieving WGMMA gains typically requires careful tiling, shared-memory staging, and enough work per warp-group to hide latency.
  - Practical takeaway: WMMA may underperform on H100 unless heavily tuned, while WGMMA can reach closer to peak Tensor Core performance when using appropriate tile sizes/layouts and minimizing data-movement overhead.

## Search: last 90 days Blackwell B200 tensor cores GEMM performance analysis CUDA kernels (new: 1, showing: 1)
- [HPC-AI Tech Blog | Insights on GPU Cloud, AI Training & HPC compute](https://www.hpc-ai.com/blog/b200) — 2025-11-26
  - Authors: n/a
  - Affiliations: not provided
  - Benchmarks GEMM (matrix multiply) throughput as a proxy for AI training/inference performance, highlighting how batch size and matrix dimensions drive achieved FLOPs and utilization.
  - Compares NVIDIA B200 vs H200 GEMM performance across varied shapes/batches, showing where each GPU sustains higher throughput and where performance is shape‑sensitive.
  - Emphasizes GEMM operator efficiency as a key GPU selection/optimization signal for transformer-style workloads dominated by matmul.

## Search: last 90 days CUTLASS 3.x new features FP8 GEMM kernel schedule research
- No new items found in the last 90 days.

## Search: last 90 days cuBLASLt FP8 GEMM autotuning heuristics performance study (error)
- Fetch error: web_search returned no in-window results (cutoff 2025-10-23)

## Search: last 90 days Triton compiler matmul kernel fusion shared memory pipelining research
- No new items found in the last 90 days.

## Search: last 90 days FlashAttention-3 GPU kernel matmul attention tensor cores benchmark (error)
- Fetch error: web_search returned no in-window results (cutoff 2025-10-23)

## Search: last 90 days systolic array vs GPU tensor cores matrix multiplication architecture comparison (error)
- Fetch error: web_search returned no in-window results (cutoff 2025-10-23)

## Search: last 90 days GPU compiler MLIR LLVM CUDA matmul codegen tensor core instruction selection (new: 3, showing: 3)
- [[llvm] 14b1d77 - [NVPTX] Add intrinsics and codegen for tensormap.replace (#172458)](https://lists.llvm.org/pipermail/llvm-commits/Week-of-Mon-20251229/1809915.html) — 2025-12-31
  - Authors: n/a
  - Affiliations: not provided
  - Adds NVVM/NVPTX support for the `tensormap.replace` operation via new intrinsics plus backend codegen, enabling LLVM-based CUDA toolchains to emit this tensor-map update instruction directly.
  - Improves coverage of NVIDIA tensor/memory-mapping features in the NVPTX backend, which is relevant for advanced tensor-core workflows that rely on tensor maps (e.g., TMA-style data movement) feeding GEMM kernels.
  - Helps GPU practitioners targeting newer NVIDIA architectures by reducing reliance on inline PTX/handwritten intrinsics for tensor-map manipulation, potentially easing integration in compiler-generated matmul pipelines.
- [User Guide for NVPTX Back-end — LLVM 22.0.0git documentation](https://llvm.org/docs/NVPTXUsage.html) — 2026-01-14
  - Authors: n/a
  - Affiliations: not provided
  - Documents LLVM’s NVPTX backend codegen path to NVIDIA GPUs, including supported PTX/NVVM features, target options, and how LLVM IR maps to PTX/SASS—useful for understanding/controlling CUDA compilation behavior from LLVM-based toolchains.
  - Highlights NVVM intrinsics for modern memory movement and tensor workflows (e.g., `cp.async.bulk.tensor.prefetch.*`), enabling asynchronous/bulk/tensor prefetch patterns that can reduce latency and improve throughput in GEMM/matrix-multiply pipelines.
  - Provides guidance on using these intrinsics and backend capabilities to access newer GPU instructions (async copies, tensor-related ops), which are key building blocks for high-performance matmul kernels (tiling, prefetching, overlap of compute/memory).
- [GitHub - NVIDIA/cuda-tile: CUDA Tile IR is an MLIR-based intermediate representation and compiler infrastructure for CUDA kernel optimization, focusing on tile-based computation patterns and optimizations targeting NVIDIA tensor core units.](https://github.com/NVIDIA/cuda-tile) — 2025-12-31
  - Authors: n/a
  - Affiliations: not provided
  - MLIR-based “CUDA Tile IR” and compiler stack aimed at optimizing CUDA kernels around tile-based computation patterns, with explicit targeting of NVIDIA Tensor Cores for high-throughput GEMM/matrix-like workloads.
  - Provides an IR and transformation pipeline to express and optimize tiling, data movement, and Tensor Core-friendly layouts/scheduling—useful for practitioners tuning matmul-heavy kernels beyond hand-written CUDA.
  - Includes practical build/usage tooling to experiment with Tile IR lowering/compilation, enabling rapid iteration on tile/matmul optimizations and Tensor Core codegen.
