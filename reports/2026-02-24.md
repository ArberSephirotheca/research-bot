# GPU Research Digest — 2026-02-24
Generated: 2026-02-24T14:26:24.553608023+00:00
Sources: 12
New items: 1
Already seen: 19
Window: last 90 days
Sources failed: 6

## arXiv: GPU OR CUDA
- No new items found in the last 90 days.

## arXiv: ML + GPU
- No new items found in the last 90 days.

## arXiv: Matrix Multiplication + GPU
- No new items found in the last 90 days.

## arXiv: AI/ML + GPU
- No new items found in the last 90 days.

## Search: last 90 days Hopper H100 tensor cores FP8 GEMM performance analysis CUTLASS cuBLASLt (error)
- Fetch error: web_search returned no in-window results (cutoff 2025-11-26)

## Search: last 90 days Blackwell B200 GB200 tensor core architecture details GEMM throughput CUDA (error)
- Fetch error: web_search returned no in-window results (cutoff 2025-11-26)

## Search: last 90 days CUTLASS 3.x new features warp-specialized GEMM kernels SM90 SM100
- No new items found in the last 90 days.

## Search: last 90 days cuBLASLt matmul epilogue fusion bias gelu swish performance update (error)
- Fetch error: web_search returned no in-window results (cutoff 2025-11-26)

## Search: last 90 days Triton compiler matmul autotuning persistent kernels FP8 BF16 GPU (error)
- Fetch error: web_search returned no in-window results (cutoff 2025-11-26)

## Search: last 90 days FlashAttention-3 GPU kernel implementation tensor cores matmul tiling SM90 (error)
- Fetch error: web_search returned no in-window results (cutoff 2025-11-26)

## Search: last 90 days GPU systolic array vs SIMT tensor core mapping matrix multiplication research paper (error)
- Fetch error: web_search returned no in-window results (cutoff 2025-11-26)

## Search: last 90 days CUDA 12.4 12.5 PTX wgmma mma instructions GEMM kernel optimization (new: 1, showing: 1)
- [KernelBlaster: Continual Cross-Task CUDA Optimization via Memory-Augmented In-Context Reinforcement Learning](https://arxiv.org/abs/2602.14293) — 2026-02-15
  - Authors: n/a
  - Affiliations: not provided
  - Introduces **KernelBlaster**, a **memory-augmented in-context reinforcement learning** system that continually optimizes **CUDA kernels across tasks and GPU generations**, aiming to reuse prior optimization experience rather than tuning from scratch per device.
  - GPU-practitioner relevance: targets practical kernel-level decisions (e.g., tiling/unrolling, memory hierarchy usage, launch configuration) and reports **speedups/performance gains** on multiple NVIDIA GPU generations, positioning it as an automated alternative to manual tuning.
  - Provides an **open-source framework** to reproduce results and extend cross-task optimization; the abstract does **not explicitly call out matrix multiplication/GEMM**, but the approach is applicable to GEMM-like kernels where memory/tiling choices dominate.
