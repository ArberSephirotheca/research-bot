# GPU Research Digest — 2026-02-03
Generated: 2026-02-03T14:23:20.463788792+00:00
Sources: 12
New items: 4
Already seen: 18
Window: last 90 days
Sources failed: 3

## arXiv: GPU OR CUDA
- No new items found in the last 90 days.

## arXiv: ML + GPU (new: 1, showing: 1)
- [Harvest: Opportunistic Peer-to-Peer GPU Caching for LLM Inference](https://arxiv.org/abs/2602.00328v1) — 2026-01-30
  - Authors: Nikhil Gopal, Kostis Kaffes
  - Affiliations: not provided
  - Uses unused memory on *other GPUs* as an opportunistic, transient cache tier for LLM inference state (expert weights + KV cache), leveraging high-bandwidth P2P interconnects (e.g., NVLink) to avoid slow PCIe/host offload.
  - Dynamically places/evicts weights and KV blocks based on moment-to-moment free memory while preserving correctness, reducing data movement overhead during autoregressive decoding.
  - Reports >2× throughput speedup by accelerating retrieval of expert-layer weights (feeding GEMM/MatMul-heavy MoE compute) and KV cache entries (attention), improving GPU utilization under memory pressure.

## arXiv: Matrix Multiplication + GPU
- No new items found in the last 90 days.

## arXiv: AI/ML + GPU
- No new items found in the last 90 days.

## Search: last 90 days Hopper H100 tensor cores FP8 GEMM microarchitecture paper (error)
- Fetch error: web_search returned no in-window results (cutoff 2025-11-05)

## Search: last 90 days Blackwell B200 tensor core MMA instruction set GEMM performance analysis (new: 1, showing: 1)
- [1.5x faster MoE training with custom MXFP8 kernels](https://cursor.com/cn/blog/kernels) — 2026-02-03
  - Authors: n/a
  - Affiliations: not provided
  - Microbenchmark results on Blackwell show how to drive high FP8/MXFP8 GEMM throughput using `tcgen05.mma`, with practical guidance on mapping MoE matmuls to the new tensor-core instruction path.
  - Highlights TMEM as a key limiter: storing MXFP8 block scale factors in TMEM can consume scarce capacity/bandwidth and measurably reduce effective MMA/GEMM throughput versus “pure” FP8.
  - Reports up to ~1.5× faster MoE training from custom MXFP8 kernels by restructuring data/scale handling to avoid TMEM pressure and keep tensor cores saturated.

## Search: last 90 days CUTLASS 3.x new GEMM kernels SM90 SM100 tutorial blog (new: 1, showing: 1)
- [Space Filling Curves is All You Need: Communication-Avoiding Matrix Multiplication Made Simple](https://arxiv.org/abs/2601.16294) — 2026-01-22
  - Authors: n/a
  - Affiliations: not provided
  - Proposes a communication-avoiding GEMM that reorders tiles using space-filling curves (e.g., Morton/Hilbert) to improve locality, reducing global-memory traffic and synchronization by keeping reused sub-blocks “near” in the traversal order.
  - GPU relevance: the SFC-based traversal maps well to hierarchical GPU memory (L2/shared/registers), improving cache hit rates and effective bandwidth without heavy, platform-specific tuning; can be layered onto existing tiled GEMM kernels.
  - Reports performance/portability across platforms, showing that locality-driven ordering can narrow the gap to highly tuned GEMM on some shapes while being more robust to problem size/layout and memory-system effects.

## Search: last 90 days cuBLASLt GEMM autotuning heuristics FP8 BF16 performance regression study (error)
- Fetch error: web_search returned no in-window results (cutoff 2025-11-05)

## Search: last 90 days Triton compiler matmul kernel fusion shared memory pipelining research
- No new items found in the last 90 days.

## Search: last 90 days FlashAttention-3 GPU kernel matmul softmax fused attention SM90 benchmarks (error)
- Fetch error: web_search returned no in-window results (cutoff 2025-11-05)

## Search: last 90 days GPU systolic array vs tensor core GEMM mapping research preprint
- No new items found in the last 90 days.

## Search: last 90 days NVIDIA PTX WGMMA warp-group MMA GEMM programming guide examples (new: 1, showing: 1)
- [learn-cutlass-3 - Tianyu Guo's homepage](https://gty111.github.io/2023/04/02/learn-cutlass-3/) — 2026-01-27
  - Authors: n/a
  - Affiliations: not provided
  - Practical notes on driving NVIDIA Tensor Cores directly via inline PTX `mma.sync`, with emphasis on how to structure GEMM microkernels around warp-level MMA operations.
  - Summarizes PTX ISA operand layouts/fragment mappings for MMA (register packing, matrix tile shapes, data types), helping practitioners avoid common pitfalls when wiring A/B/C fragments for matrix multiplication.
  - Updated pointers (2026) to CUTLASS 3 / low-level PTX resources relevant for implementing or understanding high-performance GEMM on modern NVIDIA GPUs.
