# GPU Research Digest — 2026-02-07
Generated: 2026-02-07T14:11:41.360036990+00:00
Sources: 12
New items: 1
Already seen: 16
Window: last 90 days
Sources failed: 6

## arXiv: GPU OR CUDA
- No new items found in the last 90 days.

## arXiv: ML + GPU
- No new items found in the last 90 days.

## arXiv: Matrix Multiplication + GPU
- No new items found in the last 90 days.

## arXiv: AI/ML + GPU
- No new items found in the last 90 days.

## Search: last 90 days Hopper H100 tensor cores GEMM FP8 CUDA kernel optimization research paper (error)
- Fetch error: web_search returned no in-window results (cutoff 2025-11-09)

## Search: last 90 days Blackwell B200 GPU architecture tensor cores matmul throughput research (error)
- Fetch error: web_search returned no in-window results (cutoff 2025-11-09)

## Search: last 90 days CUTLASS 3.x new features GMMA warpgroup matmul tutorial blog (error)
- Fetch error: web_search returned no in-window results (cutoff 2025-11-09)

## Search: last 90 days cuBLASLt matmul autotuning heuristics FP8 BF16 performance update (error)
- Fetch error: web_search returned no in-window results (cutoff 2025-11-09)

## Search: last 90 days Triton compiler matmul kernel fusion persistent kernels research (new: 1, showing: 1)
- [Accelerating Mamba2 with Kernel Fusion – PyTorch](https://pytorch.org/blog/accelerating-mamba2-with-kernel-fusion/) — 2026-02-06
  - Authors: n/a
  - Affiliations: not provided
  - Fuses the Mamba-2 SSD forward path (previously 5 separate PyTorch/Triton ops) into a single Triton kernel to cut kernel-launch overhead and eliminate redundant global-memory reads/writes, improving GPU occupancy and bandwidth efficiency.
  - Reports end-to-end speedups of ~1.50×–2.51× on NVIDIA A100/H100 by keeping intermediate state in registers/shared memory and reducing synchronization between kernels.
  - Work is primarily about custom kernel fusion for state-space sequence ops (not GEMM-heavy); gains come from memory/launch optimization rather than accelerating matrix multiplication.

## Search: last 90 days FlashAttention-3 CUDA Hopper tensor cores FP8 matmul implementation (error)
- Fetch error: web_search returned no in-window results (cutoff 2025-11-09)

## Search: last 90 days systolic array vs GPU tensor cores matrix multiplication accelerator research (error)
- Fetch error: web_search returned no in-window results (cutoff 2025-11-09)

## Search: last 90 days GPU compiler MLIR LLVM CUDA matmul codegen tensor core scheduling research
- No new items found in the last 90 days.
