# GPU Research Digest — 2026-01-06
Generated: 2026-01-06T14:08:41.460367326+00:00
Sources: 12
New items: 2
Already seen: 17
Window: last 90 days
Sources failed: 6

## arXiv: GPU OR CUDA
- No new items found in the last 90 days.

## arXiv: ML + GPU
- No new items found in the last 90 days.

## arXiv: Matrix Multiplication + GPU
- No new items found in the last 90 days.

## arXiv: AI/ML + GPU
- No new items found in the last 90 days.

## Search: last 90 days Hopper H100 tensor cores FP8 GEMM microarchitecture paper (error)
- Fetch error: web_search returned no in-window results (cutoff 2025-10-08)

## Search: last 90 days Blackwell B200 tensor core matmul FP4 FP8 research performance analysis (error)
- Fetch error: web_search returned no in-window results (cutoff 2025-10-08)

## Search: last 90 days CUTLASS 3.x new GEMM kernels SM90 SM100 tutorial blog (error)
- Fetch error: web_search returned no in-window results (cutoff 2025-10-08)

## Search: last 90 days cuBLASLt GEMM autotuning heuristics epilogue fusion SM90 SM100 (new: 2, showing: 2)
- [Changelog — NVIDIA CUTLASS Documentation (CUTLASS 4.3.0)](https://docs.nvidia.com/cutlass/4.3.0/CHANGELOG.html) — 2025-11-21
  - Authors: n/a
  - Affiliations: not provided
  - CUTLASS 4.3.0 adds heuristics-based GEMM kernel filtering/autotuning via **nvidia-matmul-heuristics**, helping practitioners pick high-performing matmul kernels with less manual tuning.
  - Includes **Blackwell-related updates**, implying improved support/paths for next-gen NVIDIA GPUs and their matrix-multiplication capabilities.
  - Overall changelog emphasizes matmul performance/selection workflow improvements (kernel pruning + autotune) relevant to deploying efficient GEMM on NVIDIA hardware.
- [Overview — NVIDIA CUTLASS Documentation (CUTLASS 4.3.0)](https://docs.nvidia.com/cutlass/4.3.0/overview.html) — 2025-11-01
  - Authors: n/a
  - Affiliations: not provided
  - CUTLASS 4.3.0 is NVIDIA’s CUDA C++ template library for high‑performance GEMM (matrix multiplication) and related linear algebra primitives, exposing composable building blocks (tiling, memory movement, epilogues) to reach near‑peak Tensor Core throughput.
  - The overview emphasizes modern GPU architecture support (including Hopper and Blackwell context), aligning kernels with newer Tensor Core data types/layouts and performance features to accelerate GEMM-heavy workloads (DL training/inference, HPC).
  - Provides both ready-to-use GEMM kernels and extensible kernel construction patterns, enabling practitioners to tailor matrix multiply (and fused epilogues like bias/activation) to specific shapes, precisions, and memory layouts for better utilization.

## Search: last 90 days Triton compiler matmul kernel fusion persistent tiling research (error)
- Fetch error: send OpenAI response for web_search

## Search: last 90 days FlashAttention-3 GPU kernel matmul attention tensor cores SM90 benchmarks (error)
- Fetch error: web_search returned no in-window results (cutoff 2025-10-08)

## Search: last 90 days systolic array vs GPU tensor core matrix multiplication comparative study (error)
- Fetch error: web_search returned no in-window results (cutoff 2025-10-08)

## Search: last 90 days GPU compiler MLIR LLVM CUDA matmul codegen tensor core MMA research
- No new items found in the last 90 days.
