# GPU Research Digest — 2026-02-21
Generated: 2026-02-21T14:10:37.827626254+00:00
Sources: 12
New items: 5
Already seen: 22
Window: last 90 days
Sources failed: 4

## arXiv: GPU OR CUDA
- No new items found in the last 90 days.

## arXiv: ML + GPU
- No new items found in the last 90 days.

## arXiv: Matrix Multiplication + GPU
- No new items found in the last 90 days.

## arXiv: AI/ML + GPU
- No new items found in the last 90 days.

## Search: last 90 days Hopper H100 tensor cores FP8 GEMM microarchitecture paper (error)
- Fetch error: web_search returned no in-window results (cutoff 2025-11-23)

## Search: last 90 days Blackwell B200 tensor core architecture GEMM performance analysis (new: 2, showing: 2)
- [NVIDIA GB10 Grace Blackwell Architecture](https://www.emergentmind.com/topics/nvidia-gb10-grace-blackwell) — 2026-01-31
  - Authors: n/a
  - Affiliations: not provided
  - Empirical results show B200 (Blackwell) delivering substantially higher dense GEMM throughput than H200 (Hopper), with the largest gains in FP8/FP16 Tensor Core paths; improvements are attributed to Blackwell’s updated Tensor Core datapaths and better utilization at large matrix sizes typical of LLM training/inference.
  - Microbenchmark references indicate higher effective bandwidth/compute balance on B200, reducing GEMM sensitivity to memory/launch overheads and improving scaling across problem sizes; practitioners should expect more consistent near-peak performance for well-tiled GEMM kernels compared to H200.
  - Across non-GEMM workloads (e.g., attention-like and mixed compute/memory kernels), B200 generally outperforms H200 but with smaller, more workload-dependent margins; the paper emphasizes profiling/roofline-style analysis to determine whether kernels are compute-bound (benefiting
- [Overview — NVIDIA CUTLASS Documentation](https://docs.nvidia.com/cutlass/4.3.2/overview.html) — 2025-12-21
  - Authors: n/a
  - Affiliations: not provided
  - CUTLASS 3.8 GEMM kernels on NVIDIA Blackwell SM100 achieve a high fraction of theoretical peak across multiple datatypes, indicating strong Tensor Core utilization and near-roofline matrix-multiply throughput when kernels are well-tuned.
  - The documentation frames performance as “% of peak” by datatype, making it easy for practitioners to gauge how close their GEMM workloads can get to SM100’s Tensor Core limits and where precision choice impacts achievable throughput.
  - Overall takeaway: on SM100, CUTLASS provides highly optimized GEMM building blocks that can deliver near-peak matmul performance, serving as a practical baseline/target for custom kernels and framework integrations.

## Search: last 90 days CUTLASS 3.x new GEMM kernels SM90 SM100 tutorial blog (new: 1, showing: 1)
- [Optimizing GEMM on RTX 4060: A Practical Guide to CUDA, Tensor Cores, CUTLASS, and cuBLAS](https://medium.com/%40rlee4408/optimizing-gemm-on-rtx-4060-a-practical-guide-to-cuda-tensor-cores-cutlass-and-cublas-93a52e84081a) — 2026-02-04
  - Authors: n/a
  - Affiliations: not provided
  - Benchmarks GEMM on Ada (RTX 4060, SM89) across a progression: naive CUDA tiling → WMMA/Tensor Core usage → CUTLASS kernels → cuBLAS, highlighting practical performance gaps and bottlenecks.
  - Shows how to map GEMM to Tensor Cores (warp-level fragments, MMA shapes, data layouts, alignment) and what changes are needed versus classic shared-memory GEMM to reach high throughput.
  - Provides practitioner guidance on when to rely on cuBLAS vs CUTLASS vs custom WMMA (e.g., problem sizes, precision/accumulation choices, epilogue/fusion needs) and how to validate/benchmark on RTX 4060.

## Search: last 90 days cuBLASLt GEMM autotuning heuristics FP8 BF16 performance regression study (error)
- Fetch error: web_search returned no in-window results (cutoff 2025-11-23)

## Search: last 90 days Triton compiler matmul kernel fusion shared memory pipelining research (new: 1, showing: 1)
- [FlashAttention-2 in Triton: From GPU Mental Models to Kernel Performance | Aleksandr Timashov](https://timashov.ai/blog/2026/flash-attention/) — 2026-01-31
  - Authors: n/a
  - Affiliations: not provided
  - Implements FlashAttention-2 in Triton and uses it to explain GPU mental models: how warps/SMs schedule work, how to structure tiles, and how to pipeline loads/compute to keep tensor cores/ALUs busy while hiding HBM latency.
  - Focuses on memory behavior critical to attention: minimizing HBM traffic via on-chip SRAM (shared memory) tiling, careful layout/strides for coalesced loads, and avoiding extra reads/writes when doing the softmax + value accumulation in a fused kernel.
  - Treats attention as a sequence of GEMM-like tiled matmuls (Q·Kᵀ and P·V) and discusses kernel-performance tradeoffs (tile sizes, stages, occupancy vs. register pressure) using profiling to diagnose bottlenecks and improve throughput.

## Search: last 90 days FlashAttention 3 GPU kernel implementation tensor cores matmul attention (error)
- Fetch error: web_search returned no in-window results (cutoff 2025-11-23)

## Search: last 90 days systolic array vs GPU tensor cores matrix multiplication roofline analysis (new: 1, showing: 1)
- [Architectural Divergence in AI Accelerators: Beyond the Memory Wall and Systolic Arrays](https://medium.com/%40kvnagesh/architectural-divergence-in-ai-accelerators-beyond-the-memory-wall-and-systolic-arrays-de19dd2d973f) — 2026-01-01
  - Authors: n/a
  - Affiliations: not provided
  - Systolic arrays (TPU-like) cut GEMM data movement by keeping weights/activations stationary and streaming operands through a fixed on-chip fabric, reducing reliance on GPU-style tiling + cache reuse to fight the memory wall.
  - Compile-time orchestration (static scheduling of dataflow, buffering, and interconnect usage) can replace much of the GPU’s dynamic caching/warp scheduling overhead for GEMM-heavy kernels, improving effective bandwidth/energy by making reuse explicit.
  - For GPU practitioners: the paper frames performance limits less as raw FLOPs and more as bytes moved per MAC; it suggests that for dense matmul, architectural choices that hard-wire reuse (systolic/dataflow) can outperform general-purpose cache/SM designs when memory traffic dominates.

## Search: last 90 days CUDA 12.4 12.5 WGMMA GMMA matmul instruction scheduling research (error)
- Fetch error: web_search returned no in-window results (cutoff 2025-11-23)
