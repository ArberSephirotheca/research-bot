# GPU Research Digest — 2026-02-17
Generated: 2026-02-17T14:24:03.639080804+00:00
Sources: 12
New items: 4
Already seen: 20
Window: last 90 days
Sources failed: 3

## arXiv: GPU OR CUDA
- No new items found in the last 90 days.

## arXiv: ML + GPU
- No new items found in the last 90 days.

## arXiv: Matrix Multiplication + GPU
- No new items found in the last 90 days.

## arXiv: AI/ML + GPU (new: 1, showing: 1)
- [KernelBlaster: Continual Cross-Task CUDA Optimization via Memory-Augmented In-Context Reinforcement Learning](https://arxiv.org/abs/2602.14293v1) — 2026-02-15
  - Authors: Kris Shengjun Dong, Sahil Modi, Dima Nikiforov, Sana Damani, Edward Lin, Siva Kumar Sastry Hari, Christos Kozyrakis
  - Affiliations: not provided
  - Proposes **KernelBlaster**, an LLM-based CUDA optimization agent that uses **memory-augmented in-context RL** to retain and retrieve prior optimization experience (Persistent CUDA Knowledge Base), improving cross-kernel and cross-GPU-architecture search beyond fixed compiler heuristics.
  - Uses a **profile-guided, textual-gradient** loop to steer systematic exploration of high-impact CUDA optimizations (rather than naive rewrites), targeting performance portability across GPU generations.
  - Reports **geomean speedups vs PyTorch** on KernelBench: **1.43× (L1), 2.50× (L2), 1.50× (L3)**; framework includes open-source harness + verification for reproducible evaluation.

## Search: last 90 days Hopper H100 tensor cores FP8 GEMM microarchitecture paper
- No new items found in the last 90 days.

## Search: last 90 days Blackwell B200 tensor core architecture GEMM throughput research (error)
- Fetch error: web_search returned no in-window results (cutoff 2025-11-19)

## Search: last 90 days CUTLASS 3.x FP8 GEMM epilogue fusion performance analysis
- No new items found in the last 90 days.

## Search: last 90 days cuBLASLt grouped GEMM FP8 BF16 autotuning research (new: 1, showing: 1)
- [Mixture of Experts — Megatron Core (Developer Guide)](https://docs.nvidia.com/megatron-core/developer-guide/latest/user-guide/features/moe.html) — 2026-02-15
  - Authors: n/a
  - Affiliations: not provided
  - Details FP8 training recipes in Megatron Core, including blockwise FP8 on Hopper/Blackwell, with guidance on scaling/amax handling and accumulation choices that directly impact GEMM throughput and numerical stability.
  - Explains MoE performance considerations centered on grouped GEMM (many small expert matmuls): emphasizes token/expert grouping, padding/alignment, and kernel selection to improve tensor-core utilization and reduce launch/dispatch overhead.
  - Highlights hardware-aware tuning for MoE workloads (e.g., Hopper/Blackwell tensor core paths, communication/compute overlap) to maximize end-to-end throughput when routing induces irregular matmul shapes.

## Search: last 90 days Triton compiler matmul kernel fusion shared memory pipelining paper (new: 1, showing: 1)
- [Deep Kernel Fusion for Transformers](https://arxiv.org/abs/2602.11808) — 2026-02-12
  - Authors: n/a
  - Affiliations: not provided
  - Introduces a “deeply fused” Transformer inference kernel that collapses multiple sub-ops (e.g., attention/MLP components) into fewer launches to cut HBM reads/writes and improve on-chip cache reuse—targeting higher effective bandwidth utilization on A100/H100.
  - Emphasizes scheduling/fusion around GEMM-heavy paths (QKV/attention projections and MLP matmuls) to keep intermediates in registers/shared memory where possible, reducing materialization between matrix multiplications.
  - Reports end-to-end inference speedups on H100/A100 and provides practical integration (SGLang + a kernel scheduler) to select/compose fused kernels for real workloads.

## Search: last 90 days FlashAttention-3 CUDA kernel tensor cores FP8 performance study (error)
- Fetch error: web_search returned no in-window results (cutoff 2025-11-19)

## Search: last 90 days GPU systolic array vs SIMT tensor core matmul mapping research (error)
- Fetch error: web_search returned no in-window results (cutoff 2025-11-19)

## Search: last 90 days MLIR LLVM NVPTX GPU compiler matmul tiling tensor core codegen research (new: 1, showing: 1)
- [NVIDIA TileIR Internals: from CuTile to MLIR/LLVM to SASS | Henry Zhu](https://maknee.github.io/blog/2026/NVIDIA-TileIR-Internals-from-CuTile-to-MLIR-LLVM-to-SASS/) — 2026-02-03
  - Authors: n/a
  - Affiliations: not provided
  - Explains TileIR’s end-to-end lowering pipeline (CuTile → MLIR/LLVM/NVVM → NVPTX → SASS), highlighting where GPU-specific semantics are preserved vs. legalized during compilation.
  - Details how Tensor Core GEMM is expressed and lowered via NVVM/LLVM intrinsics (e.g., `mma.sync` for MMA ops, `ldmatrix` for fragment loads), mapping high-level tile math to warp-level instructions.
  - Covers performance-critical memory/synchronization primitives used in tiled matmul kernels—`cp.async` for async shared-memory staging and `mbarrier` for coordination—showing how they appear in IR and influence final SASS.
