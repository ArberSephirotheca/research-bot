# GPU Research Digest — 2026-01-19
Generated: 2026-01-19T14:12:45.961252115+00:00
Sources: 12
New items: 4
Already seen: 17
Window: last 90 days
Sources failed: 5

## arXiv: GPU OR CUDA
- No new items found in the last 90 days.

## arXiv: ML + GPU
- No new items found in the last 90 days.

## arXiv: Matrix Multiplication + GPU
- No new items found in the last 90 days.

## arXiv: AI/ML + GPU
- No new items found in the last 90 days.

## Search: last 90 days Hopper H100 tensor cores FP8 GEMM microarchitecture paper (new: 1, showing: 1)
- [The Native Spiking Microarchitecture: From Iontronic Primitives to Bit-Exact FP8 Arithmetic](https://arxiv.org/abs/2512.07724) — 2025-12-08
  - Authors: n/a
  - Affiliations: not provided
  - Proposes a neuromorphic “Native Spiking Microarchitecture” that maps iontronic/spiking primitives to conventional numeric compute, aiming to make spiking hardware usable like GPU-style accelerators rather than a separate programming model.
  - Validates **bit-exact FP8 arithmetic** across all FP8 format pairs with **PyTorch-aligned** results, which is directly relevant to GPU practitioners relying on FP8 kernels (e.g., GEMM/attention) where exact rounding/accumulation behavior matters for reproducibility.
  - Implication for matrix multiplication: if their FP8 pipeline is truly bit-exact, it should enable drop-in compatibility with existing FP8 **GEMM**/tensor-core-style matmul workflows while exploring spiking-native execution underneath.

## Search: last 90 days Blackwell B200 tensor core MMA instruction set GEMM performance analysis (new: 1, showing: 1)
- [1.5x faster MoE training with custom MXFP8 kernels](https://cursor.com/blog/kernels) — 2026-01-19
  - Authors: n/a
  - Affiliations: not provided
  - Shows how custom Blackwell FP8/MXFP8 GEMM kernels using `tcgen05.mma` can materially speed up MoE training (~1.5×), with microbenchmarks tying gains directly to tensor-core utilization and kernel scheduling choices.
  - Highlights TMEM as a key bottleneck: when MXFP8 scale factors (and related metadata) consume TMEM, effective GEMM throughput drops; careful placement/packing of scales is critical to keep MMA pipelines fed.
  - Provides measured FP8 vs MXFP8 GEMM throughput deltas and practical kernel-engineering guidance (tile shapes, TMEM budgeting, data movement) for maximizing matrix-multiply performance on Blackwell.

## Search: last 90 days CUTLASS 3.x new GEMM kernels FP8 BF16 SM90 SM100 blog (error)
- Fetch error: web_search returned no in-window results (cutoff 2025-10-21)

## Search: last 90 days cuBLASLt GEMM autotuning heuristics epilogue fusion release notes (error)
- Fetch error: OpenAI response missing output text for web_search: {
  "id": "resp_0f6d861fbaf640af00696e3ba682848193ae1b4a5f28c349d7",
  "object": "response",
  "created_at": 1768831910,
  "status": "incomplete",
  "background": false,
  "billing": {
    "payer": "developer"
  },
  "completed_at": null,
  "error": null,
  "frequency_penalty": 0.0,
  "incomplete_details": {
    "reason": "max_output_tokens"
  },
  "instructions": null,
  "max_output_tokens": 500,
  "max_tool_calls": null,
  "model": "gpt-5.2-2025-12-11",
  "output": [
    {
      "id": "ws_0f6d861fbaf640af00696e3ba701448193b150db4f51c1b164",
      "type": "web_search_call",
      "status": "completed",
      "action": {
        "type": "search",
        "queries": [
          "cuBLASLt GEMM autotuning heuristics epilogue fusion release notes"
        ],
        "query": "cuBLASLt GEMM autotuning heuristics epilogue fusion release notes"
      }
    },
    {
      "id": "ws_0f6d861fbaf640af00696e3ba838a88193875161d13e734e77",
      "type": "web_search_call",
      "status": "completed"...

## Search: last 90 days Triton compiler matmul kernel fusion pipelining shared memory swizzle (error)
- Fetch error: web_search returned no in-window results (cutoff 2025-10-21)

## Search: last 90 days FlashAttention-3 CUDA kernel implementation tensor cores FP8 benchmarks (error)
- Fetch error: web_search returned no in-window results (cutoff 2025-10-21)

## Search: last 90 days GPU systolic array matmul mapping tensor core warp-level MMA scheduling (error)
- Fetch error: web_search returned no in-window results (cutoff 2025-10-21)

## Search: last 90 days MLIR LLVM NVPTX GPU compiler matmul tiling tensor core codegen research (new: 2, showing: 2)
- ['nvvm' Dialect - MLIR](https://mlir.llvm.org/docs/Dialects/NVVMDialect/) — 2026-01-14
  - Authors: n/a
  - Affiliations: not provided
  - Documents the MLIR **NVVM dialect** as the CUDA/NVPTX-facing layer, detailing how GPU-specific ops/attributes map down through **LLVM IR to NVPTX**, which is key for controlling codegen and performance on NVIDIA GPUs.
  - Covers **tensor core matrix-multiply primitives**, including **`mma.sync`** (warp-level MMA) and newer **`wgmma`** (warp-group MMA), exposing low-level knobs for high-throughput GEMM/ML workloads.
  - Describes the **lowering path and constraints** (types, layouts, synchronization/warp semantics) needed to correctly and efficiently generate tensor-core MMA code from MLIR.
- ['gpu' Dialect - MLIR](https://mlir.llvm.org/docs/Dialects/GPU/) — 2026-01-15
  - Authors: n/a
  - Affiliations: not provided
  - Describes the MLIR `gpu` dialect compilation flow for NVIDIA targets: lowering `gpu.launch`/`gpu.module` through passes like `convert-gpu-to-nvvm` (or `gpu-to-llvm`) into NVVM/LLVM IR, then emitting PTX/cubin via `gpu-module-to-binary`, enabling end-to-end kernel codegen from MLIR.
  - Explains target configuration knobs (e.g., GPU architecture/SM, PTX version, fast-math/optimization settings, linking) that control NVPTX/NVVM code generation and binary emission for deployment.
  - No matrix-multiplication–specific content; it’s primarily about the general kernel lowering and binary generation pipeline rather than GEMM/tensor-core abstractions.
