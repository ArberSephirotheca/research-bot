# GPU Research Digest — 2025-12-24
Generated: 2025-12-24T19:07:22.934354728+00:00
Sources: 12
New items: 11
Already seen: 14
Window: last 90 days
Sources failed: 3

## arXiv: GPU OR CUDA
- No new items found in the last 90 days.

## arXiv: ML + GPU
- No new items found in the last 90 days.

## arXiv: Matrix Multiplication + GPU
- No new items found in the last 90 days.

## arXiv: AI/ML + GPU (new: 1, showing: 1)
- [Scalable GPU-Accelerated Euler Characteristic Curves: Optimization and Differentiable Learning for PyTorch](https://arxiv.org/abs/2510.20271v1) — 2025-10-23
  - CUDA ECC kernels optimized for Ampere: 128B coalesced global loads + hierarchical shared-memory accumulation, reporting ~16–2000× speedups vs prior GPU ECC on synthetic grids; aimed at scalable batching and potential multi-GPU extension.
  - Provides a differentiable PyTorch layer (sigmoid-relaxed ECT-style) to learn filtration thresholds end-to-end while keeping ECC computation on-GPU.
  - No matrix multiplication focus; performance gains come from memory access patterns, parallel reductions/atomics avoidance, and kernel-level optimization rather than GEMM.

## Search: last 90 days Hopper H100 tensor cores FP8 GEMM cuBLASLt performance whitepaper (error)
- Fetch error: web_search returned no in-window results (cutoff 2025-09-25)

## Search: last 90 days CUTLASS 3.x new GEMM kernels SM90 SM100 warp-specialized epilogue fusion (new: 1, showing: 1)
- [CUTLASS 4.3.4 (GitHub Release)](https://github.com/NVIDIA/cutlass/releases/tag/v4.3.4) — 2025-12-24
  - CUTLASS 4.3.4 ships CuTe DSL updates, improving the expressiveness/maintainability of tensor layouts and kernel composition used in GEMM and other tensor ops.
  - Includes a CUTLASS C++ workaround for a Blackwell (SM100) TMA descriptor driver bug, improving correctness/stability for TMA-based memory movement in high-performance matmul kernels.
  - Practical impact: safer deployment of Blackwell-optimized GEMM paths and smoother iteration on CuTe-based matmul kernel variants.

## Search: last 90 days Triton compiler matmul autotuning persistent kernels FP16 BF16 FP8 (new: 1, showing: 1)
- [Persistent Matmul — Triton documentation](https://triton-lang.org/main/getting-started/tutorials/09-persistent-matmul.html) — 2025-12-20
  - Walks through implementing GEMM in Triton with a baseline tiled matmul and a **persistent-kernel** variant that keeps a program instance resident to improve data reuse and reduce launch/overhead, targeting higher throughput on GPUs.
  - Covers a **TMA-based persistent matmul** approach (Hopper-class) to accelerate global→shared transfers and better overlap memory movement with compute; includes guidance on scheduling/tiling choices relevant to occupancy and bandwidth.
  - Supports **FP16 and FP8** matmul; FP8 path requires **CUDA compute capability 9.0 (Hopper)**, highlighting practical constraints and performance considerations for low-precision GEMM.

## Search: last 90 days FlashAttention-3 CUDA kernel details tensor core MMA pipeline shared memory scheduling (new: 4, showing: 4)
- [Flash Attention from Scratch Part 3: Kernel 1](https://lubits.ch/flash/Part-3) — 2025-12-21
  - Explains how to budget shared memory per CTA for a FlashAttention-style kernel, and how SMEM footprint directly constrains occupancy/throughput on GPUs.
  - Shows reducing SMEM via buffer aliasing (reusing the same SMEM region for temporaries whose lifetimes don’t overlap), which depends on careful ordering and synchronization to avoid hazards.
  - Sets up next-step performance work—double buffering and pipelining—to overlap global-memory loads with compute (GEMM-like QKᵀ / PV matmul phases) while keeping SMEM usage manageable.
- [Flash Attention from Scratch: Appendix](https://lubits.ch/flash/Appendix) — 2025-12-24
  - Provides compute-capability/throughput tables (e.g., FP16/BF16/TF32/FP32, tensor-core vs CUDA-core rates) to quickly estimate whether an attention kernel should be MMA/tensor-core–bound or memory/latency-bound on a given GPU.
  - Includes instruction-ratio and kernel-property analysis (MMA vs non-MMA mix, scheduling/occupancy considerations) to guide tile sizing and pipeline design so tensor-core math stays fed without being dominated by loads/stores or scalar ops.
  - Specifies kernel details relevant to FlashAttention-style matmuls (QKᵀ and PV): how to balance compute vs memory traffic and choose layouts/tiling that maximize tensor-core utilization while controlling register/shared-memory pressure.
- [Aman's AI Journal • Primers • FlashAttention](https://aman.ai/primers/ai/flashattention/) — 2025-12-20
  - FlashAttention-3 on Hopper centers on **WGMMA** (warp-group MMA) for the QKᵀ and PV matmuls, paired with **TMA** (Tensor Memory Accelerator) to move tiles efficiently and keep tensor cores fed.
  - Uses **warp specialization** plus **ping-pong scheduling** so some warps handle async loads/stores while others compute, reducing SM idle time and hiding memory latency.
  - Implements a **2-stage pipeline** that overlaps **WGMMA compute with softmax** work, cutting stalls between matmul and normalization and improving end-to-end attention throughput.
- [Aman's AI Journal • Primers • Model Acceleration](https://aman.ai/primers/ai/model-acceleration/) — 2025-12-20
  - FlashAttention-3 uses producer–consumer asynchrony with a circular shared-memory buffer to keep data movement and compute overlapped, improving SM utilization and reducing stalls.
  - Overlaps GEMM and softmax via ping-pong scheduling (double-buffering), enabling continuous matmul throughput while softmax runs concurrently.
  - FP8 layout/format handling (transpose/permutation) is a first-class pipeline concern: data layout transforms can dominate or disrupt the matmul/attention kernel schedule if not fused/hidden.

## Search: last 90 days NVIDIA cuBLASLt grouped GEMM batched matmul heuristics kernel selection blog (error)
- Fetch error: OpenAI response error for web_search: 500 Internal Server Error {
  "error": {
    "message": "Internal server error",
    "type": "internal_error",
    "param": null,
    "code": "internal_error"
  }
}

## Search: last 90 days GPU architecture research systolic array mapping tensor core utilization matrix multiplication (error)
- Fetch error: web_search returned no in-window results (cutoff 2025-09-25)

## Search: last 90 days GPU compiler research kernel fusion matmul epilogue fusion ML training throughput (new: 3, showing: 3)
- [FlashFuser: Expanding the Scale of Kernel Fusion for Compute-Intensive Operators via Inter-Core Connection](https://arxiv.org/abs/2512.12949) — 2025-12-15
  - Uses H100’s inter-core connectivity / Distributed Shared Memory to fuse compute-intensive kernels beyond a single SM’s shared-memory limits, cutting global memory traffic and kernel launch overhead.
  - Compiler-driven “large-scale fusion” targets operator chains where intermediate tensors would otherwise spill to HBM; reports reduced memory accesses and notable kernel + end-to-end speedups on NVIDIA H100.
  - Matrix-multiplication relevance: enables fusing GEMM-adjacent epilogues/consumer ops (e.g., bias/activation/normalization-style post-ops) at larger scales by sharing intermediates across SMs via DSM instead of round-tripping to global memory.
- [cuPilot: A Strategy-Coordinated Multi-agent Framework for CUDA Kernel Evolution](https://arxiv.org/abs/2512.16465) — 2025-12-18
  - Proposes **cuPilot**, a **strategy-coordinated multi-agent** system that evolves CUDA kernels using a **strategy-level representation** (e.g., tiling, memory layout, vectorization, pipelining) rather than editing low-level code directly—aimed at producing performant GPU kernels with fewer trial-and-error iterations.
  - Uses **roofline-guided prompting/feedback** to steer agents toward higher arithmetic intensity and better utilization of GPU memory hierarchy (shared memory, registers, coalescing), targeting common performance bottlenecks in CUDA.
  - Reports **speedups over PyTorch** on a benchmark suite that includes **GEMM/matrix multiplication** workloads, indicating the approach can discover competitive matmul kernel variants via automated kernel evolution.
- [tritonBLAS: Triton-based Analytical Approach for GEMM Kernel Parameter Selection](https://arxiv.org/abs/2512.04226) — 2025-12-03
  - Proposes an analytical performance model to pick Triton GEMM (matmul) kernel parameters (e.g., tile/block sizes, warps/stages) ahead of time, avoiding expensive runtime autotuning while targeting near-optimal throughput.
  - Evaluates across a wide range of GEMM shapes and multiple modern GPU architectures, showing the model can reliably choose high-performing Triton kernels close to autotuned baselines.
  - Practical takeaway for GPU practitioners: enables faster deployment/compilation and more predictable performance for matmul-heavy workloads by replacing search-based tuning with model-driven parameter selection.

## Search: last 90 days AI accelerator research matrix multiplication dataflow tensor core alternatives training inference (new: 1, showing: 1)
- [NeuronMM: High-Performance Matrix Multiplicationfor LLM Inference on AWS Trainium (Version 1)](https://www.preprints.org/manuscript/202511.1093/v1) — 2025-11-14
  - Optimizes LLM GEMM under tight on-chip SRAM/bandwidth constraints by enforcing data-layout/tiling choices that maximize reuse and minimize off-chip traffic—conceptually similar to GPU tensor-core GEMM where layout (e.g., K-major/packed) and tile shapes dominate performance.
  - Uses kernel fusion (e.g., matmul + bias/activation/quant/dequant patterns) and caching of weights/activations to cut memory movement and improve effective SRAM bandwidth, paralleling GPU best practices like epilogue fusion and persistent/stream-K style kernels.
  - Highlights that inference performance is often memory-movement bound rather than compute bound; careful scheduling, prefetching, and reuse across tokens/batches can yield large gains—transferable to GPU LLM inference tuning (KV/cache-aware tiling, minimizing HBM reads/writes).
