# GPU Research Digest — 2026-02-09
Generated: 2026-02-09T14:29:06.612589518+00:00
Sources: 12
New items: 6
Already seen: 15
Window: last 90 days
Sources failed: 5

## arXiv: GPU OR CUDA
- No new items found in the last 90 days.

## arXiv: ML + GPU
- No new items found in the last 90 days.

## arXiv: Matrix Multiplication + GPU
- No new items found in the last 90 days.

## arXiv: AI/ML + GPU
- No new items found in the last 90 days.

## Search: last 90 days Hopper H100 tensor cores FP8 GEMM microarchitecture paper (error)
- Fetch error: web_search returned no in-window results (cutoff 2025-11-11)

## Search: last 90 days Blackwell B200 tensor core architecture GEMM throughput analysis (error)
- Fetch error: OpenAI response missing output text for web_search: {
  "id": "resp_0d281f2f8785931c006989eeba6b4c819ca8939263f4824696",
  "object": "response",
  "created_at": 1770647226,
  "status": "incomplete",
  "background": false,
  "billing": {
    "payer": "developer"
  },
  "completed_at": null,
  "error": null,
  "frequency_penalty": 0.0,
  "incomplete_details": {
    "reason": "max_output_tokens"
  },
  "instructions": null,
  "max_output_tokens": 500,
  "max_tool_calls": null,
  "model": "gpt-5.2-2025-12-11",
  "output": [
    {
      "id": "ws_0d281f2f8785931c006989eebae560819c812ce154b83aea14",
      "type": "web_search_call",
      "status": "completed",
      "action": {
        "type": "search",
        "queries": [
          "Blackwell B200 tensor core architecture GEMM throughput analysis"
        ],
        "query": "Blackwell B200 tensor core architecture GEMM throughput analysis"
      }
    },
    {
      "id": "ws_0d281f2f8785931c006989eebbf8f0819cb12f8da983b8c9db",
      "type": "web_search_call",
      "status": "completed",
...

## Search: last 90 days CUTLASS 3.x new kernels FP8 BF16 GEMM performance blog (error)
- Fetch error: web_search returned no in-window results (cutoff 2025-11-11)

## Search: last 90 days cuBLASLt FP8 GEMM autotuning heuristics release notes (new: 4, showing: 4)
- [CUDA Toolkit 13.1 Update 1 - Release Notes — Release Notes 13.1 documentation](https://docs.nvidia.cn/cuda/cuda-toolkit-release-notes/index.html) — 2026-02-04
  - Authors: n/a
  - Affiliations: not provided
  - Adds an experimental cuBLAS GEMM autotuning mode (`CUBLAS_GEMM_AUTOTUNE`) to help select faster GEMM kernels/configurations automatically; intended for performance exploration and may change behavior across releases.
  - Recommends using the cuBLASLt heuristics API for production-grade GEMM selection/tuning (more control and visibility into algorithm choices), including guidance relevant to newer datatypes like FP8.
  - Practical takeaway: for high-performance matrix multiplication, prefer cuBLASLt + heuristics for stable, inspectable tuning; use `CUBLAS_GEMM_AUTOTUNE` as a quick way to probe performance wins.
- [cuBLAS | NVIDIA Developer](https://developer.nvidia.com/cublas) — 2026-02-08
  - Authors: n/a
  - Affiliations: not provided
  - cuBLAS is NVIDIA’s highly optimized GPU BLAS library; its core value for practitioners is fast dense linear algebra, especially GEMM (matrix multiplication), with tuned kernels and Tensor Core acceleration where available.
  - cuBLASLt is the “lightweight”/next-gen GEMM interface: more control over matmul (algorithm selection, epilogues like bias/activation, layout/stride flexibility) to better hit peak performance across shapes and data types.
  - Highlights support for modern low-precision Tensor Core paths (including FP8 where supported) aimed at maximizing throughput for deep learning GEMMs while retaining configurable accuracy/performance tradeoffs.
- [Release Notes v1.12.0 — Gaudi Documentation 1.23.0 documentation](https://docs.habana.ai/en/latest/Release_Notes/Release%20Notes%20v1.12.0.html) — 2026-02-05
  - Authors: n/a
  - Affiliations: not provided
  - Adds FP8 support in SynapseAI 1.12.0 (Gaudi), relevant to GPU practitioners tracking FP8 GEMM/MatMul ecosystems and mixed‑precision training/inference workflows.
  - Signals continued maturation of low‑precision (FP8) matrix‑multiplication paths (e.g., GEMM/linear layers) and associated kernel/library support for higher throughput at reduced memory bandwidth.
  - Useful as a reference point for FP8 feature parity comparisons vs. GPU stacks (CUDA/cuBLASLt/TensorRT), especially around enabling and validating FP8 MatMul performance/accuracy.
- [Releases · NVIDIA/TransformerEngine · GitHub](https://github.com/NVIDIA/TransformerEngine/releases) — 2026-01-09
  - Authors: n/a
  - Affiliations: not provided
  - Adds/updates FP8 support in TransformerEngine (including GEMM/matmul paths), targeting higher throughput and lower memory bandwidth for Transformer workloads on NVIDIA GPUs.
  - Improves FP8 numerics/recipes and kernel behavior (scaling/amax handling, stability fixes), which can affect matmul accuracy/performance trade-offs in training and inference.
  - Introduces/expands prebuilt wheel support for newer CUDA toolchains (e.g., CUDA 13), easing deployment on up-to-date GPU software stacks.

## Search: last 90 days Triton compiler matmul kernel fusion shared memory pipelining research
- No new items found in the last 90 days.

## Search: last 90 days FlashAttention-3 CUDA kernel implementation details tensor cores (error)
- Fetch error: web_search returned no in-window results (cutoff 2025-11-11)

## Search: last 90 days GPU systolic array matmul mapping tensor core MMA instruction scheduling (new: 2, showing: 2)
- [How to Write High-Performance Matrix Multiply in NVIDIA CUDA Tile | NVIDIA Technical Blog](https://developer.nvidia.com/blog/how-to-write-high-performance-matrix-multiply-in-nvidia-cuda-tile/) — 2026-01-19
  - Authors: n/a
  - Affiliations: not provided
  - Explains cuTile’s tiled matmul pipeline: cooperative tile loads into shared/register tiles, followed by a `ct.mma` compute stage, emphasizing how tiling maps work to warps/threads for reuse and bandwidth efficiency.
  - Shows that when operand layouts/dtypes/shapes satisfy Tensor Core constraints, cuTile automatically routes `ct.mma` to Tensor Cores, letting practitioners get HMMA/WGMMA acceleration without hand-writing MMA intrinsics.
  - Provides high-level guidance on matmul scheduling concepts (tile sizes, operand staging, and warp-level mapping) that affect occupancy, memory traffic, and Tensor Core utilization.
- [Hopper/Blackwell Tensor Core MMA layouts | vj-krish](https://vjkrish.com/2026/01/19/Mma_Layouts.html) — 2026-01-19
  - Authors: n/a
  - Affiliations: not provided
  - Explains Hopper/Blackwell `tcgen05` Tensor Core MMA shared-memory layouts/descriptors, including how “swizzle atoms” and core matrix shapes map fragments to SMEM to satisfy Tensor Core operand requirements.
  - Shows how choosing the right SMEM layout/swizzle impacts tiled GEMM kernel design: avoiding bank conflicts, enabling efficient `ldmatrix`/TMA-style feeding, and matching warpgroup MMA consumption patterns.
  - Discusses pipelining multiple in-flight MMA tiles (warpgroup-level scheduling), highlighting layout/descriptor choices that improve overlap of SMEM movement with compute for higher GEMM throughput.

## Search: last 90 days MLIR LLVM NVPTX GPU compiler improvements for GEMM codegen tensor cores (error)
- Fetch error: web_search returned no in-window results (cutoff 2025-11-11)
