# GPU Research Digest — 2026-01-11
Generated: 2026-01-11T14:07:45.753545084+00:00
Sources: 12
New items: 1
Already seen: 16
Window: last 90 days
Sources failed: 6

## arXiv: GPU OR CUDA
- No new items found in the last 90 days.

## arXiv: ML + GPU
- No new items found in the last 90 days.

## arXiv: Matrix Multiplication + GPU
- No new items found in the last 90 days.

## arXiv: AI/ML + GPU
- No new items found in the last 90 days.

## Search: last 90 days Hopper H100 tensor cores FP8 GEMM microarchitecture paper (error)
- Fetch error: OpenAI response error for web_search: 403 Forbidden 

## Search: last 90 days Blackwell B200 tensor core GEMM performance analysis CUDA (error)
- Fetch error: web_search returned no in-window results (cutoff 2025-10-13)

## Search: last 90 days CUTLASS 3.x epilogue fusion GEMM tutorial blog (error)
- Fetch error: web_search returned no in-window results (cutoff 2025-10-13)

## Search: last 90 days cuBLASLt new features grouped GEMM FP8 BF16 release notes (error)
- Fetch error: OpenAI response missing output text for web_search: {
  "id": "resp_0cf96d076c9237b9006963ae7a6bc8819680e0d64a7d013ad1",
  "object": "response",
  "created_at": 1768140410,
  "status": "incomplete",
  "background": false,
  "billing": {
    "payer": "developer"
  },
  "completed_at": null,
  "error": null,
  "frequency_penalty": 0.0,
  "incomplete_details": {
    "reason": "max_output_tokens"
  },
  "instructions": null,
  "max_output_tokens": 500,
  "max_tool_calls": null,
  "model": "gpt-5.2-2025-12-11",
  "output": [
    {
      "id": "ws_0cf96d076c9237b9006963ae7b317c819681a31c440fa7065b",
      "type": "web_search_call",
      "status": "completed",
      "action": {
        "type": "search",
        "queries": [
          "cuBLASLt new features grouped GEMM FP8 BF16 release notes",
          "cuBLASLt grouped GEMM FP8 BF16 release notes",
          "NVIDIA cuBLASLt release notes FP8 BF16 grouped GEMM",
          "cuBLASLt grouped GEMM FP8 BF16 \"Release Notes\""
        ],
        "query": "cuBLASLt new features grouped GEMM FP8 B...

## Search: last 90 days Triton compiler matmul autotuning FP8 tensor cores research
- No new items found in the last 90 days.

## Search: last 90 days FlashAttention 3 CUDA kernel implementation tensor cores matmul (new: 1, showing: 1)
- [FlashAttention 2.8.3 fails during ComfyUI sampling (Torch 2.9.1 + cu130, Windows 11, RTX 5090)](https://www.reddit.com/r/comfyui/comments/1q29i5h/flashattention_283_fails_during_comfyui_sampling/) — 2026-01-02
  - Authors: n/a
  - Affiliations: not provided
  - FlashAttention 2.8.3 crashes during ComfyUI sampling on a very new stack (PyTorch 2.9.1 + CUDA 13.0 on Windows 11, RTX 5090), suggesting a kernel/ABI mismatch or unsupported SM path rather than a model-level issue.
  - GPU practitioners should treat this as an attention-kernel dispatch problem (FlashAttention fused QKᵀ/softmax/V matmul pipeline) where the build may not include the right architecture targets or relies on CUDA features not yet stable in this combo.
  - Practical implication: for stable matmul/attention throughput, pin to known-good CUDA/PyTorch/FlashAttention versions or rebuild FlashAttention with explicit SM support for the 5090; otherwise fall back to PyTorch SDPA/xFormers to avoid fused-kernel failures.

## Search: last 90 days GPU kernel fusion compiler pass GEMM epilogue fusion research (error)
- Fetch error: web_search returned no in-window results (cutoff 2025-10-13)

## Search: last 90 days systolic array vs tensor core matrix multiplication accelerator comparison paper (error)
- Fetch error: web_search returned no in-window results (cutoff 2025-10-13)
