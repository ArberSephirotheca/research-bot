# GPU Research Digest — 2025-12-25
Generated: 2025-12-25T14:08:05.567161850+00:00
Sources: 12
New items: 4
Already seen: 22
Window: last 90 days
Sources failed: 3

## arXiv: GPU OR CUDA
- No new items found in the last 90 days.

## arXiv: ML + GPU
- No new items found in the last 90 days.

## arXiv: Matrix Multiplication + GPU
- No new items found in the last 90 days.

## arXiv: AI/ML + GPU
- No new items found in the last 90 days.

## Search: last 90 days Hopper H100 tensor cores FP8 GEMM microarchitecture paper
- No new items found in the last 90 days.

## Search: last 90 days Blackwell B200 tensor core architecture matmul throughput research (new: 1, showing: 1)
- [WaveSpeedAI X DataCrunch: FLUX Real-Time Image Inference on B200 - WaveSpeedAI Blog](https://wavespeed.ai/blogs/index.php/2025/12/10/wavespeedai-x-datacrunch-flux-real-time-image-inference-on-b200/) — 2025-12-10
  - Authors: n/a
  - Affiliations: not provided
  - Blackwell B200 tensor-core execution details relevant to GEMM: CTA-pair scheduling and shared TMEM (tensor memory) are highlighted as key mechanisms to raise math throughput and improve data reuse for matrix-heavy kernels.
  - Operator-level microbenchmarks break down GEMM and attention performance, emphasizing how kernel choices map to B200’s tensor-core modes and memory hierarchy for real-time FLUX inference.
  - Throughput tables compare H100 vs HGX B200 across BF16/FP8/FP4, showing the largest gains at lower precisions (FP8/FP4) where tensor-core GEMM becomes the dominant speedup driver.

## Search: last 90 days CUTLASS 3.x new GEMM kernels FP8 BF16 SM90 SM100 blog (new: 2, showing: 2)
- [CUTLASS 4.3.3](https://github.com/NVIDIA/cutlass/releases/tag/v4.3.3) — 2025-12-12
  - Authors: n/a
  - Affiliations: not provided
  - CUTLASS 4.3.3 updates the CuTe DSL and CUTLASS kernels, improving the building blocks used to generate high-performance GEMM/matrix-multiply and related tensor operations on NVIDIA GPUs.
  - Includes Blackwell-related support updates, helping practitioners target newer NVIDIA architectures for optimized matmul/Tensor Core paths.
  - General maintenance/compatibility release: expect incremental fixes and tuning that can affect GEMM performance, correctness, and integration in custom kernels.
- [CUTLASS 4.3.2](https://github.com/NVIDIA/cutlass/releases/tag/v4.3.2) — 2025-12-05
  - Authors: n/a
  - Affiliations: not provided
  - CUTLASS v4.3.2 release updates the CuTe DSL and CUTLASS kernels, targeting improved GPU GEMM/linear algebra performance and usability (e.g., better kernel building blocks and scheduling primitives for matmul-heavy workloads).
  - Includes incremental fixes and refinements to matmul-related templates/operators (GEMM/epilogue paths), helping practitioners integrate newer CUDA/SM features with fewer workarounds.
  - Primarily a maintenance/iteration release: expect stability and performance tuning improvements rather than new algorithmic matmul primitives.

## Search: last 90 days cuBLASLt matmul autotuning heuristics FP8 BF16 performance analysis (error)
- Fetch error: web_search returned no in-window results (cutoff 2025-09-26)

## Search: last 90 days Triton compiler matmul kernel fusion shared memory pipelining research
- No new items found in the last 90 days.

## Search: last 90 days FlashAttention-3 GPU kernel matmul attention tensor cores performance paper (error)
- Fetch error: web_search returned no in-window results (cutoff 2025-09-26)

## Search: last 90 days GPU systolic array vs tensor core matrix multiplication mapping study (error)
- Fetch error: web_search returned no in-window results (cutoff 2025-09-26)

## Search: last 90 days CUDA 12.5 12.6 WGMMA GMMA matmul instruction scheduling research (new: 1, showing: 1)
- [SAT-MapIt: A SAT-based Modulo Scheduling Mapper for Coarse Grain Reconfigurable Architectures](https://arxiv.org/abs/2512.02875) — 2025-12-02
  - Authors: n/a
  - Affiliations: not provided
  - Uses a SAT formulation to jointly solve modulo scheduling + placement/routing on CGRAs, which is relevant to GPU practitioners as an alternative constraint-based mapper for spatial/ILP-style accelerators (e.g., tensor-core-like dataflows) when heuristic graph methods fail on tight resource/II constraints.
  - Captures complex constraints (resource conflicts, routing, initiation interval) more precisely than prior graph-algorithm mappers, suggesting a path to better utilization/throughput for regular compute kernels that resemble GPU pipelines.
  - Matrix multiplication isn’t explicitly indicated in the abstract; the approach would mainly apply indirectly to mapping GEMM-like systolic/dataflow schedules onto coarse-grain fabrics.
