# GPU Research Digest — 2026-02-05
Generated: 2026-02-05T14:23:22.624774340+00:00
Sources: 12
New items: 6
Already seen: 12
Window: last 90 days
Sources failed: 5

## arXiv: GPU OR CUDA
- No new items found in the last 90 days.

## arXiv: ML + GPU
- No new items found in the last 90 days.

## arXiv: Matrix Multiplication + GPU
- No new items found in the last 90 days.

## arXiv: AI/ML + GPU (new: 1, showing: 1)
- [GPU-Accelerated ANNS: Quantized for Speed, Built for Change](https://arxiv.org/abs/2601.07048v3) — 2026-01-11
  - Authors: Hunter McCoy, Zikun Wang, Prashant Pandey
  - Affiliations: not provided
  - GPU-native Vamana ANNS with **lock-free, batch-parallel CUDA construction** enabling fast streaming/batch insertions (no full rebuild); builds **~2.4× faster than CAGRA** while adding updatability.
  - **GPU-efficient RaBitQ quantization** cuts vector memory footprint/data movement by **up to 8×** without introducing costly random accesses, improving bandwidth efficiency.
  - Optimized greedy-search kernel improves **latency hiding/compute utilization (up to ~80% roofline peak)** and boosts throughput (**up to 1.93× vs CAGRA; 19–131× vs BANG**); no matrix-multiplication focus (dominant work is distance computations + graph traversal).

## Search: last 90 days Hopper H100 tensor cores FP8 GEMM microarchitecture paper (error)
- Fetch error: web_search returned no in-window results (cutoff 2025-11-07)

## Search: last 90 days Blackwell B200 tensor core GEMM performance analysis CUDA (new: 2, showing: 2)
- [Delivering Massive Performance Leaps for Mixture of Experts Inference on NVIDIA Blackwell | NVIDIA Technical Blog](https://developer.nvidia.com/blog/delivering-massive-performance-leaps-for-mixture-of-experts-inference-on-nvidia-blackwell/) — 2026-01-07
  - Authors: n/a
  - Affiliations: not provided
  - Blackwell (GB200 NVL72 / HGX B200) MoE inference is optimized around GEMM-heavy expert MLPs, leveraging TensorRT-LLM kernel/graph scheduling improvements plus NVFP4 low-precision GEMMs to raise effective throughput while keeping accuracy targets.
  - End-to-end MoE speedups come from reducing non-GEMM overheads: faster expert routing/top‑k + fused epilogue patterns, and improved communication primitives (e.g., all-to-all / reduce-scatter) to better overlap comms with compute across GPUs.
  - Practical takeaway for practitioners: maximize time in large, well-shaped GEMMs (expert FFNs), use NVFP4 where supported, and rely on updated TensorRT‑LLM + comms stack on B200/GB200 to mitigate MoE’s dispatch/communication bottlenecks.
- [NVIDIA B200 GPU Guide: Use Cases, Models, Benchmarks & AI Scale](https://www.clarifai.com/blog/nvidia-b200-gpu-guide/) — 2026-01-22
  - Authors: n/a
  - Affiliations: not provided
  - Summarizes NVIDIA Blackwell B200 positioning for AI/HPC, emphasizing Tensor Core support for low-precision formats (FP4/FP6/FP8) and how these map to practical training/inference use cases.
  - Highlights dense matrix multiplication (GEMM) as the core performance driver, discussing claimed/benchmark throughput for Tensor Core–accelerated dense matmul and the implications for scaling large models.
  - Provides references/context for interpreting GEMM performance claims (e.g., precision mode, Tensor Core utilization, and dense vs. other operation mixes) to help practitioners compare B200 against prior GPUs.

## Search: last 90 days CUTLASS 3.x new GEMM kernels SM90 SM100 tutorial (error)
- Fetch error: web_search returned no in-window results (cutoff 2025-11-07)

## Search: last 90 days cuBLASLt GEMM autotuning heuristics FP8 BF16 Hopper (error)
- Fetch error: web_search returned no in-window results (cutoff 2025-11-07)

## Search: last 90 days Triton compiler matmul kernel fusion performance Hopper H100 (error)
- Fetch error: web_search returned no in-window results (cutoff 2025-11-07)

## Search: last 90 days FlashAttention-3 CUDA kernel matmul attention tensor cores (new: 2, showing: 2)
- [Attention — NVIDIA cuDNN Frontend](https://docs.nvidia.com/deeplearning/cudnn/frontend/latest/operations/Attention.html) — 2026-02-03
  - Authors: n/a
  - Affiliations: not provided
  - cuDNN Frontend exposes Scaled Dot Product Attention (SDPA) as a fused GPU primitive (QKᵀ matmul → softmax → PV matmul) with a support matrix covering data types/layouts, masking (causal/padding), dropout, and bias—aimed at minimizing HBM traffic and kernel launches versus composing separate GEMMs.
  - Implements/aligns with FlashAttention-2–style tiling: blockwise QKᵀ and PV matrix multiplications with online softmax to avoid materializing the attention matrix, improving throughput and memory efficiency on modern NVIDIA GPUs.
  - Practical GPU constraints are explicit: best performance/availability depends on recent architectures and Tensor Core–friendly dtypes (e.g., FP16/BF16/FP8) and specific tensor layouts/strides, so practitioners must match shapes and alignment to
- [Portable Paged Attention in Helion](https://pytorch.org/blog/portable-paged-attention-in-helion/) — 2026-02-03
  - Authors: n/a
  - Affiliations: not provided
  - Implements **paged attention** in Helion to make LLM KV-cache access **page/block-based**, improving GPU memory efficiency and reducing fragmentation while keeping attention throughput high during inference.
  - Emphasizes **portability of the attention kernel** across GPU backends: careful choices around tiling, memory layout, and synchronization to map well to different architectures without rewriting the algorithm per vendor.
  - Attention compute is structured around **GEMM-like tiled dot products (Q·Kᵀ and P·V)** with performance hinging on SRAM/shared-memory reuse, coalesced loads from paged KV, and minimizing bandwidth/latency from irregular cache access.

## Search: last 90 days GPU systolic array matmul mapping tensor cores research (error)
- Fetch error: web_search returned no in-window results (cutoff 2025-11-07)

## Search: last 90 days MLIR LLVM GPU compiler GEMM codegen tensor cores (new: 1, showing: 1)
- [Intel Proposing XeGPU Dialect For LLVM MLIR - Phoronix](https://www.phoronix.com/news/Intel-XeGPU-Dialect-MLIR-LLVM) — 2026-01-31
  - Authors: n/a
  - Affiliations: not provided
  - Intel is proposing an **XeGPU dialect for LLVM MLIR** to enable **high-performance GEMM/matrix-multiply code generation** on Intel GPUs by representing GPU-specific operations directly in MLIR.
  - The dialect would model Intel Xe target instructions such as **DPAS (matrix/MMA-like ops)** and **2D block load**, improving mapping of tiled GEMM kernels to hardware.
  - Positioned similarly to **NVGPU/AMDGPU bridge dialects**, aiming to provide a cleaner lowering path from MLIR to Intel GPU backends for performance-critical linear algebra.
