# GPU Research Digest — 2026-01-08
Generated: 2026-01-08T14:11:38.560638925+00:00
Sources: 12
New items: 4
Already seen: 7
Window: last 90 days
Sources failed: 7

## arXiv: GPU OR CUDA (error)
- Fetch error: bad status for http://export.arxiv.org/api/query?search_query=all:GPU+OR+all:CUDA&start=0&max_results=50

## arXiv: ML + GPU (error)
- Fetch error: fetch http://export.arxiv.org/api/query?search_query=cat:cs.LG+AND+all:GPU&start=0&max_results=50

## arXiv: Matrix Multiplication + GPU
- No new items found in the last 90 days.

## arXiv: AI/ML + GPU (error)
- Fetch error: fetch http://export.arxiv.org/api/query?search_query=(cat:cs.AI+OR+cat:cs.LG)+AND+(all:GPU+OR+all:CUDA+OR+all:accelerator)&start=0&max_results=50

## Search: last 90 days Hopper H100 tensor core GEMM microarchitecture latency throughput analysis (error)
- Fetch error: web_search returned no in-window results (cutoff 2025-10-10)

## Search: last 90 days Blackwell B200 GB200 tensor cores FP8 GEMM performance whitepaper (error)
- Fetch error: web_search returned no in-window results (cutoff 2025-10-10)

## Search: last 90 days CUTLASS 3.x new GEMM kernels SM90 SM100 FP8 BF16 release notes (error)
- Fetch error: OpenAI response missing output text for web_search: {
  "id": "resp_0db9b97df629639c00695fbaba6b38819c94e9140fef53ead0",
  "object": "response",
  "created_at": 1767881403,
  "status": "incomplete",
  "background": false,
  "billing": {
    "payer": "developer"
  },
  "completed_at": null,
  "error": null,
  "incomplete_details": {
    "reason": "max_output_tokens"
  },
  "instructions": null,
  "max_output_tokens": 500,
  "max_tool_calls": null,
  "model": "gpt-5.2-2025-12-11",
  "output": [
    {
      "id": "ws_0db9b97df629639c00695fbabc5814819ca96edc0e008cca08",
      "type": "web_search_call",
      "status": "completed",
      "action": {
        "type": "search",
        "queries": [
          "CUTLASS 3.x new GEMM kernels SM90 SM100 FP8 BF16 release notes",
          "site:github.com NVIDIA CUTLASS 3. x release notes SM100 FP8 BF16",
          "CUTLASS 3. x release notes SM90 FP8 BF16",
          "CUTLASS SM100 GEMM kernel FP8 BF16 3.x release"
        ],
        "query": "CUTLASS 3.x new GEMM kernels SM90 SM100 FP8 BF16 release...

## Search: last 90 days cuBLASLt GEMM autotuning heuristics FP8 BF16 SM90 SM100 update
- No new items found in the last 90 days.

## Search: last 90 days Triton compiler matmul kernel fusion epilogue scheduling performance regression (error)
- Fetch error: web_search returned no in-window results (cutoff 2025-10-10)

## Search: last 90 days FlashAttention-3 CUDA kernel matmul attention tensor cores SM90 benchmarks (new: 1, showing: 1)
- [FlashAttention compatibility | [“LLM Laboratory”]](https://llmlaba.com/articles/flashattn-cuda-compatibility.html) — 2025-12-25
  - Authors: n/a
  - Affiliations: not provided
  - FlashAttention version compatibility hinges on both CUDA toolkit version and GPU SM architecture; Hopper (SM90) has distinct support requirements compared to Ampere (SM80) and earlier, so match FA release + CUDA + SM to avoid build/runtime issues.
  - FA3 has stricter prerequisites (newer CUDA/compiler/toolchain and specific GPU capabilities), and is primarily aimed at newer architectures (notably Hopper/SM90), so older GPUs may need FA2/FA1.
  - Practical takeaway for GEMM-heavy LLM workloads: choose the newest FlashAttention your CUDA+GPU supports to maximize fused attention throughput and reduce memory bandwidth pressure versus unfused matmul+softmax pipelines.

## Search: last 90 days GPU systolic array vs SIMT tensor core matmul mapping research paper
- No new items found in the last 90 days.

## Search: last 90 days MLIR LLVM NVPTX CUDA matmul codegen tensor core MMA lowering research (new: 3, showing: 3)
- ['nvgpu' Dialect - MLIR](https://mlir.llvm.org/docs/Dialects/NVGPU/) — 2026-01-04
  - Authors: n/a
  - Affiliations: not provided
  - Introduces the MLIR **NVGPU dialect** as a GPU-focused intermediate layer for NVIDIA hardware, providing ops that map cleanly to NVVM/PTX and enabling more controlled GPU codegen.
  - Highlights **`nvgpu.mma.sync`** as the key tensor-core primitive: a lowering target from **`vector.contract`** to **`nvvm.mma.sync`**, preserving MMA semantics for efficient matrix multiply-accumulate on NVIDIA Tensor Cores.
  - Enables compiler pipelines to express and optimize **warp-level tiled GEMM** (matrix multiplication) before final lowering, improving portability of high-level matmul IR while still reaching Tensor Core instructions.
- [MLIR: mlir::nvgpu Namespace Reference](https://mlir.llvm.org/doxygen/namespacemlir_1_1nvgpu.html) — 2025-12-30
  - Authors: n/a
  - Affiliations: not provided
  - Documents the `mlir::nvgpu` dialect utilities/enums used to lower NVIDIA GPU matrix-multiply-accumulate (MMA) operations (e.g., `MmaSyncF32Lowering`), mapping high-level MLIR ops to warp-level `mma.sync`-style instructions.
  - Defines matmul/MMA operand roles and related metadata to correctly route A/B operands and accumulators through lowering, enabling correct codegen for tensor-core GEMM patterns.
  - Useful as a practitioner reference for controlling/understanding MMA lowering choices and constraints when targeting NVIDIA GPUs via MLIR.
- [mlir.dialects.transform.nvgpu - MLIR Python bindings documentation](https://mlir.llvm.org/python-bindings/autoapi/mlir/dialects/transform/nvgpu/index.html) — 2025-12-25
  - Authors: n/a
  - Affiliations: not provided
  - Documents MLIR Python bindings for the `transform.nvgpu` dialect, enabling scripted GPU-targeted IR transformations (e.g., lowering/rewriting patterns specific to NVIDIA GPUs).
  - Highlights `transform.nvgpu.rewrite_matmul_as_mma_sync`, which rewrites `memref`-based matmul into `mma.sync` on vector types to leverage Tensor Cores for faster matrix multiplication.
  - Useful for practitioners building custom matmul lowering pipelines: exposes knobs to control when/how matmul is converted to MMA, aligning IR with warp-level Tensor Core execution.
