# GPU Research Digest — 2025-12-28
Generated: 2025-12-28T14:08:51.552048438+00:00
Sources: 12
New items: 4
Already seen: 25
Window: last 90 days
Sources failed: 1

## arXiv: GPU OR CUDA
- No new items found in the last 90 days.

## arXiv: ML + GPU
- No new items found in the last 90 days.

## arXiv: Matrix Multiplication + GPU
- No new items found in the last 90 days.

## arXiv: AI/ML + GPU
- No new items found in the last 90 days.

## Search: last 90 days Hopper H100 tensor cores FP8 GEMM microarchitecture paper
- No new items found in the last 90 days.

## Search: last 90 days Blackwell B200 tensor core matmul performance analysis CUDA
- No new items found in the last 90 days.

## Search: last 90 days CUTLASS 3.x new GEMM kernels SM90 SM100 release notes
- No new items found in the last 90 days.

## Search: last 90 days cuBLASLt GEMM autotuning heuristics FP8 BF16 performance update (new: 2, showing: 2)
- [Release Notes — cuBLASMp (cuBLASMp v0.7.0 released November 24, 2025)](https://docs.nvidia.com/cuda/cublasmp/release_notes) — 2025-11-24
  - Authors: n/a
  - Affiliations: not provided
  - Adds/updates GEMM/matmul support and tuning for newer FP formats (e.g., FP8/FP16/BF16 pathways) with performance/accuracy behavior changes relevant to mixed‑precision training/inference workloads.
  - Improves multi‑GPU/distributed GEMM behavior (communication/overlap, scaling, and/or algorithm selection) to better utilize NVLink/PCIe and reduce end‑to‑end matmul latency.
  - Fixes correctness/performance bugs in cuBLASMp GEMM (edge sizes, strides/layouts, epilogues/accumulation) that could previously cause slowdowns or incorrect results in certain matrix shapes.
- [FP8-Flow-MoE: A Casting-Free FP8 Recipe without Double Quantization Error](https://arxiv.org/abs/2511.02302) — 2025-11-04
  - Authors: n/a
  - Affiliations: not provided
  - Proposes an FP8-centric MoE training recipe (“FP8-Flow-MoE”) that keeps the main GEMM paths in FP8 end-to-end (casting-free where possible), reducing datatype conversion overhead around matmul/linear layers and improving GPU throughput versus BF16 and naïve FP8 pipelines.
  - Avoids “double quantization error” by restructuring scaling/quantization so activations/weights aren’t repeatedly requantized across MoE routing/expert boundaries; this targets better numerical stability while still using FP8 Tensor Core matmuls.
  - Practical GPU takeaway: fewer casts + fewer extra quant/dequant steps means higher effective Tensor Core utilization and less memory traffic/latency around MoE expert GEMMs, translating to reported throughput gains on FP8-capable hardware.

## Search: last 90 days Triton compiler matmul kernel fusion shared memory pipelining research
- No new items found in the last 90 days.

## Search: last 90 days FlashAttention 3 CUDA kernel matmul attention tensor cores benchmark (new: 2, showing: 2)
- [FlashAttention-3: Fast and Accurate Attention with Asynchrony and Low-precision](https://pytorch.org/blog/flashattention-3/) — 2025-12-23
  - Authors: n/a
  - Affiliations: not provided
  - FlashAttention-3 targets NVIDIA Hopper (H100) by restructuring attention to better exploit SM/Tensor Core throughput and hide latency via asynchrony (e.g., overlapping memory movement with compute), improving end-to-end attention speed over FlashAttention-2/Triton/cuDNN in PyTorch benchmarks.
  - Emphasizes higher effective GEMM/Tensor Core utilization inside attention (QKᵀ and PV matmuls) through improved tiling/scheduling and memory access patterns, reducing bandwidth/launch overheads and increasing achieved FLOPs.
  - Adds/benchmarks low-precision paths (notably FP8 alongside FP16), showing that attention can run faster while maintaining accuracy, leveraging Hopper’s FP8 Tensor Cores for additional throughput gains.
- [GitHub - togethercomputer/flash-attention-3: Fast and memory-efficient exact attention](https://github.com/togethercomputer/flash-attention-3) — 2025-12-07
  - Authors: n/a
  - Affiliations: not provided
  - FlashAttention-3 implements **exact attention** with a fused, IO-aware kernel that avoids materializing the \(N \times N\) attention matrix, yielding **large VRAM savings** and enabling longer sequence lengths on GPUs (benchmarks shown for **A100/H100** across varying \(L\)).
  - Performance plots indicate **substantial speedups vs. baseline attention** by improving memory bandwidth utilization and reducing HBM traffic; gains generally grow with longer sequences where naive attention becomes memory-bound.
  - GPU execution is organized around **blocked/tiled QKᵀ and PV computations** (GEMM-like matrix multiplications) fused with softmax and scaling, improving **tensor core utilization** and reducing kernel launch overhead.

## Search: last 90 days systolic array vs GPU tensor core matrix multiplication comparative study (error)
- Fetch error: web_search returned no in-window results (cutoff 2025-09-29)

## Search: last 90 days GPU compiler MLIR LLVM CUDA matmul optimization tensor core codegen
- No new items found in the last 90 days.
