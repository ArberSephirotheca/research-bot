# GPU Research Digest — 2026-01-12
Generated: 2026-01-12T14:11:13.316659976+00:00
Sources: 12
New items: 3
Already seen: 18
Window: last 90 days
Sources failed: 4

## arXiv: GPU OR CUDA
- No new items found in the last 90 days.

## arXiv: ML + GPU
- No new items found in the last 90 days.

## arXiv: Matrix Multiplication + GPU
- No new items found in the last 90 days.

## arXiv: AI/ML + GPU
- No new items found in the last 90 days.

## Search: last 90 days Hopper H100 tensor cores GEMM FP8 research paper
- No new items found in the last 90 days.

## Search: last 90 days Blackwell B200 GPU architecture tensor cores matrix multiplication performance analysis (error)
- Fetch error: web_search returned no in-window results (cutoff 2025-10-14)

## Search: last 90 days CUTLASS 3.x new GEMM kernels FP8 FP16 BF16 blog paper
- No new items found in the last 90 days.

## Search: last 90 days cuBLASLt GEMM autotuning heuristics kernel selection research (error)
- Fetch error: web_search returned no in-window results (cutoff 2025-10-14)

## Search: last 90 days Triton compiler matmul kernel fusion performance paper
- No new items found in the last 90 days.

## Search: last 90 days FlashAttention-3 GPU kernel matmul attention tensor cores benchmark (error)
- Fetch error: web_search returned no in-window results (cutoff 2025-10-14)

## Search: last 90 days systolic array vs GPU tensor cores matrix multiplication accelerator comparison (new: 3, showing: 3)
- [Introduction to Tensor Cores in NVIDIA GPUs](https://naddod.medium.com/introduction-to-tensor-cores-in-nvidia-gpus-ae2a79642733) — 2026-01-01
  - Authors: n/a
  - Affiliations: not provided
  - NVIDIA Tensor Cores are specialized fixed-shape matrix multiply-accumulate (MMA) units (e.g., 4×4×4) that accelerate GEMM-like workloads by executing small matrix tiles per instruction and accumulating results efficiently.
  - They use mixed precision (e.g., FP16/BF16/INT8 inputs with FP32 accumulation) to boost throughput while maintaining acceptable numerical stability, making them ideal for deep learning and tiled matrix multiplication kernels.
  - Execution is integrated into the GPU’s warp-level scheduling model: warps issue MMA operations that map onto Tensor Core pipelines, with performance hinging on tile layout, data movement (shared memory/registers), and keeping Tensor Cores fed.
- [GPU/TPU Architectures](https://www.linkedin.com/pulse/gputpu-architectures-sharada-yeluri-p5hwc) — 2025-12-30
  - Authors: n/a
  - Affiliations: not provided
  - Frames NVIDIA GPU Tensor Cores as **warp/SIMT-controlled matrix-math units**: matrix multiply is issued/managed through the GPU’s existing thread/warp execution model, leveraging the broader GPU memory hierarchy and scheduling.
  - Contrasts this with TPU-style **systolic arrays** as a **dedicated dataflow architecture** for GEMM, where data movement is orchestrated by the array’s fixed pipeline rather than by SIMT threads.
  - Practical takeaway for GPU practitioners: performance hinges on **feeding Tensor Cores efficiently** (tiling, shared-memory/register reuse, and occupancy) since compute is embedded in the general-purpose GPU execution/memory system rather than a standalone systolic dataflow.
- [Tensor core GPU’s impact on performance improvements in AI](https://medium.com/@21338787/tensor-core-gpus-impact-on-performance-improvements-in-ai-2ca1597beccd) — 2025-12-28
  - Authors: n/a
  - Affiliations: not provided
  - Tensor Cores accelerate matrix multiply–accumulate (MMA/GEMM) by executing small matrix tiles per cycle, making dense linear algebra (training/inference matmuls, convolutions via GEMM) the primary source of speedups on NVIDIA GPUs.
  - Mixed-precision (e.g., FP16/BF16/TF32 inputs with FP32 accumulation) is the key tradeoff: large throughput gains with manageable accuracy loss when using scaling/accumulation strategies, shifting performance bottlenecks toward memory/layout and kernel fusion.
  - Architectural evolution increases supported datatypes and tile shapes and improves scheduling/throughput, so peak gains depend on mapping workloads to Tensor Core-friendly GEMMs (proper alignment, dimensions, and use of WMMA/cuBLASLt).

## Search: last 90 days CUDA 12.4 12.5 warp-group MMA WGMMA GEMM research (error)
- Fetch error: web_search returned no in-window results (cutoff 2025-10-14)
