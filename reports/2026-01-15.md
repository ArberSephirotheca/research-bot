# GPU Research Digest — 2026-01-15
Generated: 2026-01-15T14:09:49.910620955+00:00
Sources: 12
New items: 2
Already seen: 16
Window: last 90 days
Sources failed: 5

## arXiv: GPU OR CUDA
- No new items found in the last 90 days.

## arXiv: ML + GPU
- No new items found in the last 90 days.

## arXiv: Matrix Multiplication + GPU
- No new items found in the last 90 days.

## arXiv: AI/ML + GPU
- No new items found in the last 90 days.

## Search: last 90 days Hopper H100 tensor cores FP8 GEMM microarchitecture paper (new: 1, showing: 1)
- [Hopper GPU Architecture | NVIDIA](https://www.nvidia.com/en-us/data-center/technologies/hopper-architecture/) — 2026-01-15
  - Authors: n/a
  - Affiliations: not provided
  - Hopper introduces 4th‑gen Tensor Cores with native FP8 (alongside FP16/BF16/TF32/INT8) to increase GEMM/attention throughput while maintaining accuracy via mixed‑precision accumulation.
  - Transformer Engine dynamically selects FP8/FP16 per layer/tensor (with scaling) to accelerate transformer matrix multiplies (QKV, attention, MLP) without manual precision tuning.
  - Architectural updates target higher utilization for large matrix workloads (improved Tensor Core pipelines and data movement), making Hopper especially effective for training/inference dominated by GEMMs.

## Search: last 90 days Blackwell B200 tensor core GEMM throughput architecture analysis (error)
- Fetch error: web_search returned no in-window results (cutoff 2025-10-17)

## Search: last 90 days CUTLASS 3.x grouped GEMM kernel fusion epilogue research
- No new items found in the last 90 days.

## Search: last 90 days cuBLASLt new heuristics autotuning matmul FP8 BF16 performance study (error)
- Fetch error: web_search returned no in-window results (cutoff 2025-10-17)

## Search: last 90 days Triton compiler matmul scheduling shared memory swizzle research (new: 1, showing: 1)
- [General Matrix-Matrix Multiplication with Tile Library - TileLang 0.1.7.post2 documentation](https://tilelang.com/deeplearning_operators/matmul.html) — 2026-01-11
  - Authors: n/a
  - Affiliations: not provided
  - Walks through a TileLang GEMM kernel design: shared-memory tiling, pipelined K-loop (load/compute overlap), parallel global→shared copies, and tile-level MMA/GEMM dispatch to map work efficiently onto GPU warps/SMs.
  - Highlights practical implementation details for GPU practitioners (tile shapes, memory layout, synchronization, double-buffering) aimed at improving bandwidth utilization and hiding latency in matmul.
  - Includes performance comparisons against cuBLAS and Triton to contextualize achieved throughput and overheads of the TileLang approach.

## Search: last 90 days FlashAttention-3 GPU kernel matmul softmax fusion tensor cores benchmark (error)
- Fetch error: web_search returned no in-window results (cutoff 2025-10-17)

## Search: last 90 days systolic array vs GPU tensor cores matrix multiplication mapping research (error)
- Fetch error: web_search returned no in-window results (cutoff 2025-10-17)

## Search: last 90 days GPU compiler MLIR LLVM CUDA matmul tiling vectorization tensor core codegen paper (error)
- Fetch error: web_search returned no in-window results (cutoff 2025-10-17)
