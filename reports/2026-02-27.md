# GPU Research Digest — 2026-02-27
Generated: 2026-02-27T14:19:36.214794306+00:00
Sources: 12
New items: 4
Already seen: 25
Window: last 90 days
Sources failed: 2

## arXiv: GPU OR CUDA
- No new items found in the last 90 days.

## arXiv: ML + GPU
- No new items found in the last 90 days.

## arXiv: Matrix Multiplication + GPU
- No new items found in the last 90 days.

## arXiv: AI/ML + GPU
- No new items found in the last 90 days.

## Search: last 90 days Hopper H100 tensor cores FP8 GEMM research paper (error)
- Fetch error: web_search returned no in-window results (cutoff 2025-11-29)

## Search: last 90 days Blackwell B200 GPU architecture tensor cores matrix multiplication performance analysis (new: 1, showing: 1)
- [NVIDIA Blackwell Architecture (Emergent Mind)](https://www.emergentmind.com/topics/blackwell-architecture) — 2026-02-22
  - Authors: n/a
  - Affiliations: not provided
  - Blackwell (B200) introduces 5th‑gen Tensor Cores with new `tcgen05.mma`-class matrix-multiply instructions, emphasizing higher MMA throughput and improved scheduling/issue behavior for GEMM-heavy workloads.
  - Expanded low-precision support is a core theme: FP4/FP6 alongside FP8 (and related mixed-precision accumulation paths), targeting better perf/W and higher effective FLOP/s for training/inference GEMMs.
  - Reports highlight practical GEMM characteristics (throughput vs. latency tradeoffs, instruction-level behavior of `tcgen05.mma`, and precision-dependent performance), useful for kernel writers tuning tile shapes, pipelining, and operand formats.

## Search: last 90 days CUTLASS 3.x new features GEMM epilogue fusion SM90 SM100
- No new items found in the last 90 days.

## Search: last 90 days cuBLASLt matmul autotuning heuristics FP8 BF16 performance study (new: 1, showing: 1)
- [Using NVFP4 Low-Precision Model Training for Higher Throughput Without Losing Accuracy](https://developer.nvidia.com/blog/using-nvfp4-low-precision-model-training-for-higher-throughput-without-losing-accuracy/) — 2026-02-23
  - Authors: n/a
  - Affiliations: not provided
  - Compares BF16 training to FP8 variants (FP8-CS, MXFP8) and NVFP4 on B200/GB200, showing substantially higher training throughput as precision drops while maintaining comparable final model accuracy (per reported results).
  - GPU relevance: gains are driven primarily by faster GEMM/attention matmul paths on Tensor Cores (lower-precision matrix multiplies + scaling/format handling), improving overall step time and hardware utilization.
  - NVFP4 targets even higher matmul throughput than FP8 by using 4-bit floating formats with appropriate scaling/accumulation strategies, aiming to preserve convergence/accuracy while maximizing Tensor Core efficiency.

## Search: last 90 days Triton compiler matmul kernel fusion persistent kernels research
- No new items found in the last 90 days.

## Search: last 90 days FlashAttention-3 GPU kernel matmul attention tensor cores benchmarks (new: 1, showing: 1)
- [FlashAttention Port: Efficient FastAttention](https://www.emergentmind.com/topics/flashattention-port-fastattention) — 2026-02-06
  - Authors: n/a
  - Affiliations: not provided
  - Compares multiple attention kernels on modern GPUs, emphasizing achieved throughput/utilization (vs. theoretical) and how kernel design choices (tiling, memory traffic, fusion) impact end-to-end performance; includes references to FlashAttention-3 on H100 with FP16/FP8 throughput figures.
  - Highlights numerical accuracy trade-offs across implementations/precisions (FP16 vs FP8), reporting error characteristics and stability considerations relevant when pushing tensor-core-heavy attention to higher throughput.
  - Frames attention as a sequence of GEMM-like operations (QKᵀ and PV) plus softmax, discussing how efficient kernels restructure these matrix multiplications to maximize tensor core use while minimizing HBM reads/writes.

## Search: last 90 days systolic array vs GPU tensor cores matrix multiplication mapping paper (error)
- Fetch error: web_search returned no in-window results (cutoff 2025-11-29)

## Search: last 90 days GPU compiler MLIR LLVM CUDA matmul optimization tensor core codegen (new: 1, showing: 1)
- [OptiML: An End-to-End Framework for Program Synthesis and CUDA Kernel Optimization](https://arxiv.org/abs/2602.12305) — 2026-02-12
  - Authors: n/a
  - Affiliations: not provided
  - End-to-end CUDA kernel optimization loop: an LLM proposes low-level code transformations, a verifier checks correctness, and Nsight Compute profiler feedback guides iterative search toward higher performance kernels.
  - GPU-practitioner focus on tuning choices that typically dominate CUDA performance (e.g., tiling, memory hierarchy usage, unrolling, vectorized loads/stores, occupancy/register pressure trade-offs) using automated exploration rather than manual trial-and-error.
  - Applicable to compute-heavy kernels such as matrix multiplication/GEMM-style patterns by searching over implementation variants and selecting those that best exploit shared memory and data reuse under profiler-driven constraints.
