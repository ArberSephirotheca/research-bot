# GPU Research Digest — 2026-02-01
Generated: 2026-02-01T14:11:38.662013691+00:00
Sources: 12
New items: 6
Already seen: 17
Window: last 90 days
Sources failed: 4

## arXiv: GPU OR CUDA
- No new items found in the last 90 days.

## arXiv: ML + GPU
- No new items found in the last 90 days.

## arXiv: Matrix Multiplication + GPU
- No new items found in the last 90 days.

## arXiv: AI/ML + GPU
- No new items found in the last 90 days.

## Search: last 90 days NVIDIA Hopper H100 tensor cores GEMM research paper (error)
- Fetch error: web_search returned no in-window results (cutoff 2025-11-03)

## Search: last 90 days Blackwell B200 tensor core matmul microarchitecture analysis (error)
- Fetch error: web_search returned no in-window results (cutoff 2025-11-03)

## Search: last 90 days CUTLASS 3.x new GEMM kernels FP8 BF16 performance blog (error)
- Fetch error: web_search returned no in-window results (cutoff 2025-11-03)

## Search: last 90 days cuBLASLt FP8 GEMM autotuning heuristics performance study (error)
- Fetch error: web_search returned no in-window results (cutoff 2025-11-03)

## Search: last 90 days Triton compiler matmul kernel fusion shared memory swizzle research (new: 1, showing: 1)
- [AutoOverlap: Enabling Fine-Grained Overlap of Computation and Communication with Chunk-Based Scheduling](https://arxiv.org/abs/2601.20595) — 2026-01-28
  - Authors: n/a
  - Affiliations: not provided
  - **What it is (GPU/Triton):** AutoOverlap is a Triton source-to-source compiler that rewrites a *single fused GPU kernel* to overlap computation with communication using **chunk-based scheduling**, enabling finer-grained pipelining than kernel-level overlap.
  - **Why it matters (multi-GPU):** Targets end-to-end multi-GPU workloads where comm stalls dominate; reports **~1.3× average** and **up to 4.7×** speedups by hiding communication latency within the kernel execution.
  - **Matmul relevance:** Not explicitly framed as a new GEMM algorithm, but directly applicable to **matmul-heavy fused kernels** (e.g., GEMM + epilogue/collectives) by chunking tiles so compute on one chunk proceeds while another chunk’s data movement/communication is in flight.

## Search: last 90 days FlashAttention 3 CUDA kernel matmul softmax tensor cores benchmark (new: 2, showing: 2)
- [The New Inference Stack (2025): FlashAttention-3, FP8→FP4, and Practical Patterns for Cheap, Fast LLM Serving](https://medium.com/@sananbintahir_87312/the-new-inference-stack-2025-flashattention-3-fp8-fp4-and-practical-patterns-for-cheap-fast-2ad7c99f8524) — 2026-01-18
  - Authors: n/a
  - Affiliations: not provided
  - **FlashAttention-3 as the new “default” attention kernel:** pushes attention toward a fused, kernel-layer implementation that **overlaps HBM traffic with compute** and **interleaves GEMM/softmax** to reduce memory round-trips—shifting the bottleneck back toward **tensor-core matmul throughput** rather than bandwidth.
  - **FP8→FP4 serving path on Hopper-class GPUs:** uses **FP8 tensor-core GEMMs** (and emerging FP4/weight-only schemes) to cut KV/cache + weight bandwidth/footprint while keeping matmul-heavy layers fast; emphasizes **quantization-aware scaling** and layout choices that preserve GEMM efficiency.
  - **Practical deployment/benchmarking patterns:** highlights that real wins depend on **kernel/version pinning**, correct **GEMM/attention shape coverage** (batch, seq, head
- [FlashAttention 3 Boosts Inference Speed 1.5-2x with IO-Aware Algorithms | LinkedIn post](https://www.linkedin.com/posts/parth-dambhare_flashattention-2-faster-attention-with-better-activity-7411662328074694656-1-Oy) — 2026-01-25
  - Authors: n/a
  - Affiliations: not provided
  - FlashAttention-3 targets NVIDIA Hopper (H100) by restructuring attention around IO-aware tiling and new hardware paths (async Tensor Cores + TMA) to better overlap memory movement with compute and reduce HBM traffic.
  - Key GPU takeaway: higher Tensor Core utilization via improved dataflow (more “GEMM-like” scheduling/tiling for the QKᵀ and PV matmul phases), aiming to keep SMs busy while minimizing stalls from loads/stores.
  - Reported inference gains are ~1.5–2× vs prior attention kernels on Hopper, attributed to better bandwidth/latency hiding and closer-to-peak matmul throughput in attention workloads.

## Search: last 90 days GPU systolic array matmul mapping tensor core MMA instruction scheduling (new: 2, showing: 2)
- [Tensor MatMul GPU - Accera](https://microsoft.github.io/Accera/Tutorials/GPU/Tensor_MatMul_GPU/) — 2026-01-31
  - Authors: n/a
  - Affiliations: not provided
  - Walks through building a high-performance GPU GEMM using tensor cores: choose tensorization tile sizes/splits, reorder loops for data reuse, and structure the kernel around warp-level tiles.
  - Covers mapping work to the GPU hierarchy (block/warp/lane binding) and using shared-memory staging to feed tensor cores efficiently while maintaining coalesced global loads.
  - Shows how to emit warp-level MMA (matrix multiply-accumulate) primitives (e.g., wmma/mma.sync-style) from Accera, highlighting the key scheduling decisions that enable tensor-core throughput.
- [Tensor MatMul SchedulingPolicy GPU - Accera](https://microsoft.github.io/Accera/Tutorials/GPU/Tensor_MatMul_SchedulingPolicy_GPU/) — 2026-01-01
  - Authors: n/a
  - Affiliations: not provided
  - Compares scheduling policies for tensor-core GEMM when a threadblock must issue multiple MMA ops per tile (multi-invocation shapes), contrasting **block-order** vs **pass-order** execution.
  - Analyzes the core tradeoff: **keeping fragments in registers** to reduce shared/global memory traffic vs **spilling/reloading** to manage register pressure and occupancy, impacting throughput.
  - Provides guidance for choosing an MMA scheduling order to balance **register usage, memory I/O, and performance** for tensor-core matmul kernels (as in Accera-generated code).

## Search: last 90 days MLIR LLVM GPU compiler matmul codegen tensor cores warp specialization (new: 1, showing: 1)
- [The LLVM Compiler Infrastructure Project — March 2025 Meeting Schedule (Proton Dialect talk)](https://llvm.org/devmtg/2025-03/) — 2026-01-18
  - Authors: n/a
  - Affiliations: not provided
  - Proton Dialect talk (LLVM/MLIR): focuses on GPU kernel profiling for AI operators, implying tooling to analyze/optimize MLIR-generated GPU code paths.
  - Highlights performance-relevant GPU features like tensor cores and warp specialization—directly applicable to accelerating GEMM/matrix-multiplication-heavy workloads.
  - Suggests an MLIR-based workflow for profiling and tuning matmul-like operators on GPUs, potentially guiding kernel selection and scheduling decisions.
