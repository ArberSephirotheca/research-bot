# GPU Research Digest — 2026-01-28
Generated: 2026-01-28T14:14:14.152907932+00:00
Sources: 12
New items: 1
Already seen: 18
Window: last 90 days
Sources failed: 4

## arXiv: GPU OR CUDA
- No new items found in the last 90 days.

## arXiv: ML + GPU
- No new items found in the last 90 days.

## arXiv: Matrix Multiplication + GPU
- No new items found in the last 90 days.

## arXiv: AI/ML + GPU
- No new items found in the last 90 days.

## Search: last 90 days Hopper H100 tensor cores FP8 GEMM microarchitecture paper
- No new items found in the last 90 days.

## Search: last 90 days Blackwell B200 tensor core MMA instruction set changes GEMM performance analysis
- No new items found in the last 90 days.

## Search: last 90 days CUTLASS 3.x new kernels FP8 BF16 GEMM epilogue fusion blog (error)
- Fetch error: web_search returned no in-window results (cutoff 2025-10-30)

## Search: last 90 days cuBLASLt release notes new heuristics autotuning grouped GEMM (error)
- Fetch error: web_search returned no in-window results (cutoff 2025-10-30)

## Search: last 90 days Triton compiler matmul kernel scheduling pipelining shared memory swizzle research (new: 1, showing: 1)
- [Iris: First-Class Multi-GPU Programming Experience in Triton](https://arxiv.org/abs/2511.12500) — 2025-11-16
  - Authors: n/a
  - Affiliations: not provided
  - Introduces **Iris**, a Triton-based **first-class multi-GPU communication layer** that exposes **tile-based symmetric memory** abstractions, letting kernels address remote GPU memory in a structured way rather than dropping to separate comm APIs.
  - Enables **compute/communication overlap** patterns (e.g., pipelined tile fetch + compute) suitable for bandwidth-bound workloads and distributed linear algebra, aiming to make multi-GPU execution feel like a single Triton program.
  - Relevant to **distributed GEMM/attention-style matmul pipelines**: tiles can be staged/streamed across GPUs while local matmul proceeds, improving utilization by hiding interconnect latency behind tensor-core compute.

## Search: last 90 days FlashAttention-3 GPU kernel implementation tensor cores FP8 BF16 benchmarks
- No new items found in the last 90 days.

## Search: last 90 days GPU systolic array mapping matrix multiplication tensor cores research preprint (error)
- Fetch error: web_search returned no in-window results (cutoff 2025-10-30)

## Search: last 90 days CUDA 12.4 12.5 PTX new MMA instructions wgmma GEMM optimization (error)
- Fetch error: OpenAI response missing output text for web_search: {
  "id": "resp_0fbf03f460c96e1700697a19a93e4c819d9c70a534aa67729b",
  "object": "response",
  "created_at": 1769609641,
  "status": "incomplete",
  "background": false,
  "billing": {
    "payer": "developer"
  },
  "completed_at": null,
  "error": null,
  "frequency_penalty": 0.0,
  "incomplete_details": {
    "reason": "max_output_tokens"
  },
  "instructions": null,
  "max_output_tokens": 500,
  "max_tool_calls": null,
  "model": "gpt-5.2-2025-12-11",
  "output": [
    {
      "id": "ws_0fbf03f460c96e1700697a19a9c158819d95e0b24d4933bae3",
      "type": "web_search_call",
      "status": "completed",
      "action": {
        "type": "search",
        "queries": [
          "CUDA 12.4 12.5 PTX new MMA instructions wgmma GEMM optimization",
          "PTX wgmma new instructions CUDA 12.5",
          "CUDA 12.4 PTX mma instruction wgmma gemm optimization blog",
          "CUDA 12.5 release notes PTX wgmma"
        ],
        "query": "CUDA 12.4 12.5 PTX new MMA instructions wgmma GEMM...
