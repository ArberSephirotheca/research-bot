# GPU Research Digest — 2025-12-22
Generated: 2025-12-22T22:08:35.371565311+00:00
Sources: 12
New items: 16
Already seen: 2
Window: last 90 days
Sources failed: 5

## arXiv: GPU OR CUDA (new: 2, showing: 2)
- [CUDA-L2: Surpassing cuBLAS Performance for Matrix Multiplication through Reinforcement Learning](https://arxiv.org/abs/2512.02551v2) — 2025-12-02
  - Uses an LLM-guided reinforcement learning loop with real CUDA runtime as reward to auto-tune half-precision GEMM (HGEMM) kernel configurations at scale (~1,000 configs), targeting practical CUDA kernel parameters/layouts.
  - Reports consistent speedups over common matmul stacks: offline mode averages +22% vs torch.matmul, +19.2% vs cuBLAS (best NN/TN layouts), +16.8% vs cuBLASLt heuristic, and +11.4% vs cuBLASLt autotuning (up to 100 candidates).
  - In “server mode” (random inter-arrival, inference-like), gains increase further: +28.7% vs torch.matmul, +26.0% vs cuBLAS, +22.4% vs cuBLASLt heuristic, +15.9% vs cuBL
- [PyRadiomics-cuda: a GPU-accelerated 3D features extraction from medical images within PyRadiomics](https://arxiv.org/abs/2510.02894v1) — 2025-10-03
  - GPU-accelerated drop-in extension for PyRadiomics that offloads 3D shape/geometry feature computations to CUDA, dramatically reducing feature-extraction time on large volumetric medical images while keeping the same PyRadiomics API (no code changes).
  - Implemented in Python + C/CUDA; validated across cluster, budget, and home GPUs; BSD-licensed with open-source repo, test suite, and example workflows for scalable radiomics pipelines.
  - No matrix-multiplication focus reported; speedups come from GPU-parallel geometric/voxel-based computations rather than GEMM-heavy kernels.

## arXiv: ML + GPU (new: 3, showing: 3)
- [Optimizing Attention on GPUs by Exploiting GPU Architectural NUMA Effects](https://arxiv.org/abs/2511.02132v1) — 2025-11-03
  - Shows that multi-chiplet/disaggregated GPUs (e.g., MI300X) exhibit strong NUMA in latency/bandwidth, breaking the “uniform memory” assumption in standard attention kernel scheduling and hurting locality/L2 reuse.
  - Proposes **Swizzled Head-first Mapping**: a NUMA-aware head-to-compute mapping for MHA that co-locates heads with the same NUMA/L2 domain to maximize intra-chiplet cache reuse, sustaining **80–97% L2 hit rates**.
  - Delivers up to **50% speedup** over state-of-the-art attention implementations with conventional scheduling; key takeaway for GEMM-heavy attention is that performance is increasingly dominated by **data placement/scheduling across NUMA domains**, not just matmul throughput.
- [GPU Memory Prediction for Multimodal Model Training](https://arxiv.org/abs/2512.07853v1) — 2025-11-26
  - Predicts peak GPU memory for **multimodal training** (where unimodal estimators fail) to proactively avoid OoM and wasted GPU time; reports ~**8.7% MAPE** average error.
  - Uses **layer-wise decomposition + factorized per-layer memory estimation** based on architecture and training behavior, enabling practical capacity planning across heterogeneous modality branches.
  - Emphasis is on **activation/optimizer/gradient memory accounting** rather than introducing new GEMM/matmul kernels; matmul relevance is indirect via layer shapes that drive tensor sizes.
- [WarmServe: Enabling One-for-Many GPU Prewarming for Multi-LLM Serving](https://arxiv.org/abs/2512.09472v1) — 2025-12-10
  - **GPU prewarming for multi-LLM serving:** Introduces *universal GPU workers* that can be proactively prewarmed (models loaded ahead of time using predictable workload patterns) to drastically cut cold-start overhead and **TTFT (up to 50.8×)** versus autoscaling approaches.
  - **Cluster-level GPU efficiency without interference:** Uses an **evict-aware model placement** strategy to reduce cross-model prewarm contention on shared GPU clusters, enabling **up to 2.5× higher request throughput** than GPU-sharing baselines.
  - **GPU memory management:** Implements a **zero-overhead memory switching** mechanism to swap/activate models in GPU memory efficiently; no specific matrix-multiplication/kernel innovations are claimed (focus is on scheduling, placement, and memory residency).

## arXiv: Matrix Multiplication + GPU (new: 2, showing: 2)
- [On the Structure of Floating-Point Noise in Batch-Invariant GPU Matrix Multiplication](https://arxiv.org/abs/2511.00025v1) — 2025-10-26
  - Empirically shows GPU matmul non-determinism is **not** well-modeled as i.i.d. Gaussian noise: the i.i.d. model predicts output instability, but comparing single-input vs batched matmuls yields **0.00% prediction flip rate** in practice.
  - Covariance analysis reveals **highly correlated, structured floating-point error**; for **FP16**, ~**50% of error variance is off-diagonal**, implying coordinated/directional perturbations rather than independent per-element noise.
  - Practical takeaway for GPU practitioners: reliability/robustness analyses (and any “noise injection” proxies) should account for **correlated matmul error structure**, especially under batching, instead of assuming independent random noise.
- [tritonBLAS: Triton-based Analytical Approach for GEMM Kernel Parameter Selection](https://arxiv.org/abs/2512.04226v1) — 2025-12-03
  - Proposes **tritonBLAS**, a **deterministic analytical model** for selecting Triton GEMM kernel parameters (tiling/blocking) using **GPU architectural features** (cache hierarchy, topology, code/data placement) instead of runtime autotuning.
  - Implements a **lightweight GEMM framework entirely in Triton** that predicts near-optimal configurations across varied matrix shapes by explicitly modeling how blocking interacts with hardware and memory.
  - Reports **>95% of autotuned performance** on modern GPUs while **eliminating autotuning overhead (zero tuning time)**, targeting production HPC/ML GEMM workloads.

## arXiv: AI/ML + GPU (new: 5, showing: 5)
- [EvoEngineer: Mastering Automated CUDA Kernel Code Evolution with Large Language Models](https://arxiv.org/abs/2510.03760v1) — 2025-10-04
  - Formalizes CUDA kernel optimization as an LLM-driven code optimization problem with explicit objectives/constraints/metrics, then proposes EvoEngineer—a systematic “code evolution” framework tuned for GPU-kernel correctness + performance tradeoffs.
  - Implements the framework and evaluates on 91 real-world CUDA kernels: 2.72× average median speedup over baseline kernels, 69.8% code validity; peak 36.75× speedup vs PyTorch kernels and best speedup on 28/50 ops achieving >2×.
  - Matrix multiplication isn’t explicitly called out in the abstract; results are reported broadly across CUDA ops/kernels rather than GEMM-specific optimization.
- [From Large to Small: Transferring CUDA Optimization Expertise via Reasoning Graph](https://arxiv.org/abs/2510.19873v1) — 2025-10-22
  - Proposes **ReGraphT**, a **training-free retrieval-augmented** framework that transfers **LLM-like CUDA optimization reasoning** to **small local models** by encoding optimization steps as a **reasoning graph** and exploring it with **Monte Carlo Graph Search**—aimed at practical, privacy-preserving CUDA codegen.
  - Shows **SLMs + ReGraphT** (e.g., **DeepSeek-Coder-V2-Lite**, **Qwen2.5-Coder-7B**) can reach near-LLM CUDA optimization quality, outperforming HPC-tuned baselines and other RAG methods, with **~2.33× average speedup** on CUDAEval/ParEval.
  - Introduces a **CUDA benchmark with difficulty tiers by reasoning complexity**; no specific **matrix multiplication/GEMM** focus is indicated in the abstract
- [AutoSAGE: Input-Aware CUDA Scheduling for Sparse GNN Aggregation (SpMM/SDDMM) and CSR Attention](https://arxiv.org/abs/2511.17594v1) — 2025-11-17
  - Input-aware CUDA scheduler for sparse GNN aggregation kernels (CSR SpMM and SDDMM): selects tiling/thread-block mapping per input based on degree skew, feature width, and GPU architecture, using a cheap model plus on-device micro-probes.
  - Practical deployment features: guardrail fallback to vendor kernels when predicted risky, and a persistent cache to replay chosen schedules deterministically across runs.
  - End-to-end relevance: composes SDDMM → row-softmax → SpMM into a CSR attention pipeline; matches vendor performance at bandwidth-bound widths and delivers up to 4.7× kernel speedups on skew/sparsity stress tests (notably at small feature widths).
- [RFX: High-Performance Random Forests with GPU Acceleration and QLORA Compression](https://arxiv.org/abs/2511.19493v1) — 2025-11-23
  - **GPU proximity at scale via QLORA low-rank + INT8 quantization:** stores the Random Forest proximity matrix as quantized low-rank factors on GPU, cutting memory from ~80 GB to **6.4 MB for 100k samples** (~12,500×) while preserving ~99% geometric structure—enabling >200k-sample proximity analysis that was previously memory-bound.
  - **GPU throughput tuning + linear-algebra-centric visualization:** uses **SM-aware batch sizing** to reach ~95% GPU utilization for proximity/importance workloads, and accelerates **3D MDS** by computing embeddings **directly from low-rank factors** (power iteration), avoiding explicit dense \(N\times N\) proximity materialization and shifting work toward factor-based matmul-like operations.
  - **CPU/GPU tradeoffs for practitioners:**
- [CudaForge: An Agent Framework with Hardware Feedback for CUDA Kernel Optimization](https://arxiv.org/abs/2511.01884v2) — 2025-10-23
  - Training-free, two-agent (Coder/Judge) loop that generates CUDA kernels, validates correctness, and iteratively optimizes using real hardware feedback (e.g., Nsight Compute metrics) to guide changes like memory access and occupancy.
  - On KernelBench, reports 97.6% correct kernels and ~1.68× speedup over PyTorch baselines, with strong cross-GPU generalization (A100, RTX 6000, 4090, 3090) and across multiple base LLMs.
  - Practical cost/latency: ~26.5 minutes per optimized kernel on an RTX6000 and ~$0.3 API cost, far cheaper than prior agentic approaches; not specifically focused on GEMM/matmul in the abstract (general CUDA kernel optimization).

## Search: last 90 days Hopper H100 tensor cores GEMM research paper warp-group MMA FP8 (new: 1, showing: 1)
- [SonicMoE: Accelerating MoE with IO and Tile-aware Optimizations](https://arxiv.org/abs/2512.14080) — 2025-12-16
  - Introduces memory-efficient MoE forward/backward that overlaps HBM/DRAM IO with compute on NVIDIA Hopper (e.g., via pipelined async copies), reducing token dispatch/gather overhead and improving end-to-end throughput.
  - Optimizes MoE expert computation with more efficient grouped GEMM (batched small GEMMs) by improving scheduling/packing to raise tensor-core utilization and reduce launch/fragmentation costs.
  - Adds tile-aware padding/packing to better match GEMM tile shapes, cutting wasted FLOPs and memory traffic from padding and improving effective matmul efficiency for irregular per-expert token counts.

## Search: last 90 days Blackwell B200 GB200 tensor core architecture matmul throughput analysis (new: 1, showing: 1)
- [Microbenchmarking NVIDIA's Blackwell Architecture: An in-depth Architectural Analysis](https://arxiv.org/abs/2512.02189) — 2025-12-01
  - Open-source microbenchmark suite for NVIDIA Blackwell B200 that characterizes 5th-gen Tensor Core behavior, including mixed-precision dense and sparse GEMM throughput/latency and how close kernels can get to peak.
  - Provides architectural insights relevant to matmul optimization (e.g., Tensor Core instruction/pipe utilization, data movement limits, and bottlenecks that cap GEMM performance under different precisions).
  - Evaluates new Blackwell memory features like TMEM and their impact on matrix-multiply data staging/reuse, informing tiling and operand placement strategies for high-throughput GEMM.

## Search: last 90 days CUTLASS 3.x new GEMM kernels FP8 BF16 SM90 SM100 blog paper (error)
- Fetch error: web_search returned no in-window results (cutoff 2025-09-23)

## Search: last 90 days cuBLASLt FP8 GEMM autotuning heuristics performance regression study (new: 2, showing: 2)
- [CUDA-L2: Surpassing cuBLAS Performance for Matrix Multiplication through Reinforcement Learning](https://arxiv.org/abs/2512.02551) — 2025-12-02
  - Introduces **CUDA-L2**, an **RL + LLM–guided autotuning system** that generates and optimizes **HGEMM (half-precision GEMM) CUDA kernels**, targeting practical kernel parameters (e.g., tiling, warp-level mapping, shared-memory usage, pipelining) to maximize throughput on NVIDIA GPUs.
  - Reports **end-to-end GEMM performance surpassing cuBLAS** on selected shapes, and compares against **cuBLASLt** using both its **heuristic selection** and **autotuning** baselines, showing that learned search can beat vendor heuristics for nontrivial matrix sizes/layouts.
  - Suggests a workflow where **LLMs propose kernel variants** and **RL efficiently explores/credits performance** via benchmarking feedback, offering a practitioner-oriented path to **shape-specialized matmul kernels** beyond standard library
- [Low-Rank GEMM: Efficient Matrix Multiplication via Low-Rank Approximation with FP8 Acceleration](https://arxiv.org/abs/2511.18674) — 2025-11-24
  - Proposes a low-rank GEMM pipeline that approximates matrix products with low-rank factors and executes the core compute using FP8 tensor-core acceleration to boost throughput and reduce memory traffic.
  - Uses automatic kernel selection (rank/shape/layout–aware) to choose efficient GPU kernels for the low-rank factors, targeting practical performance across varying matrix sizes and ranks.
  - Benchmarks against cuBLAS GEMM baselines, reporting speedups/efficiency gains when approximation error is acceptable, highlighting when low-rank + FP8 outperforms standard dense GEMM on GPUs.

## Search: last 90 days Triton compiler matmul kernel fusion persistent kernels FP8 research (error)
- Fetch error: web_search returned no in-window results (cutoff 2025-09-23)

## Search: last 90 days FlashAttention-3 GPU kernel implementation tensor cores matmul softmax optimization (error)
- Fetch error: web_search returned no in-window results (cutoff 2025-09-23)

## Search: last 90 days systolic array vs GPU tensor cores matrix multiplication roofline analysis paper (error)
- Fetch error: web_search returned no in-window results (cutoff 2025-09-23)

## Search: last 90 days GPU compiler research MLIR LLVM CUDA matmul scheduling shared memory swizzle (error)
- Fetch error: web_search returned no in-window results (cutoff 2025-09-23)
