# GPU Research Digest — 2026-01-05
Generated: 2026-01-05T14:10:35.577928653+00:00
Sources: 12
New items: 2
Already seen: 17
Window: last 90 days
Sources failed: 5

## arXiv: GPU OR CUDA
- No new items found in the last 90 days.

## arXiv: ML + GPU
- No new items found in the last 90 days.

## arXiv: Matrix Multiplication + GPU
- No new items found in the last 90 days.

## arXiv: AI/ML + GPU
- No new items found in the last 90 days.

## Search: last 90 days Hopper H100 tensor cores FP8 GEMM performance analysis CUTLASS cuBLASLt
- No new items found in the last 90 days.

## Search: last 90 days Blackwell B200 tensor core architecture matmul throughput FP8 FP4 research paper (error)
- Fetch error: web_search returned no in-window results (cutoff 2025-10-07)

## Search: last 90 days CUDA 12.4 12.5 cuBLASLt GEMM autotuning heuristics epilogue fusion (error)
- Fetch error: web_search returned no in-window results (cutoff 2025-10-07)

## Search: last 90 days CUTLASS 3.x grouped GEMM persistent kernels SM90 SM100 matmul research (error)
- Fetch error: web_search returned no in-window results (cutoff 2025-10-07)

## Search: last 90 days Triton compiler matmul kernel fusion shared memory swizzle pipelining performance (error)
- Fetch error: web_search returned no in-window results (cutoff 2025-10-07)

## Search: last 90 days FlashAttention-3 GPU kernel matmul attention tiling tensor cores H100 B200 (error)
- Fetch error: web_search returned no in-window results (cutoff 2025-10-07)

## Search: last 90 days GPU systolic array mapping GEMM tensor core MMA instruction scheduling research (new: 2, showing: 2)
- [Blackwell SM100 GEMMs — NVIDIA CUTLASS Documentation](https://docs.nvidia.com/cutlass/latest/media/docs/cpp/blackwell_functionality.html) — 2026-01-02
  - Authors: n/a
  - Affiliations: not provided
  - Details SM100/SM120 (Blackwell) GEMM support in CUTLASS, mapping high-level GEMM tiles to new PTX `mma` instruction variants (data types/layouts) to efficiently use Blackwell Tensor Cores.
  - Explains kernel scheduling choices for GEMM mainloops—**pingpong** vs **cooperative**—and how they trade off latency hiding, shared-memory usage, and warp/CTA coordination to maximize throughput.
  - Provides practitioner-facing guidance on selecting MMA variants and scheduling modes to tune Blackwell GEMM performance (tile shapes, pipeline stages, and memory movement).
- [NVIDIA Tensor Core TN Layout MMA Instruction - Lei Mao's Log Book](https://leimao.github.io/blog/NVIDIA-Tensor-Core-MMA-Instruction-TN-Layout/) — 2025-12-06
  - Authors: n/a
  - Affiliations: not provided
  - Clarifies how MMA/GEMM operand layouts (NN/NT/TN/TT) map to Tensor Core instructions, with “TN” meaning A is transposed and B is non‑transposed in the MMA op, affecting how fragments are loaded and arranged in registers/shared memory.
  - Argues why newer NVIDIA GPUs emphasize TN-layout MMA: it better matches hardware data paths and memory/coalescing patterns for common row/col-major storage, reducing permutes/transposes and improving Tensor Core utilization.
  - Provides benchmark-driven guidance: choosing the right MMA layout (often TN) can materially impact throughput/latency, so kernel implementations should align shared-memory tiling and fragment layout to the preferred instruction form.

## Search: last 90 days ML training acceleration fused GEMM+activation kernels CUDA graph capture cuDNN cuBLAS research
- No new items found in the last 90 days.
