# GPU Research Digest — 2025-12-27
Generated: 2025-12-27T14:08:54.061777595+00:00
Sources: 12
New items: 3
Already seen: 5
Window: last 90 days
Sources failed: 8

## arXiv: GPU OR CUDA (error)
- Fetch error: bad status for http://export.arxiv.org/api/query?search_query=all:GPU+OR+all:CUDA&start=0&max_results=50

## arXiv: ML + GPU (error)
- Fetch error: fetch http://export.arxiv.org/api/query?search_query=cat:cs.LG+AND+all:GPU&start=0&max_results=50

## arXiv: Matrix Multiplication + GPU (error)
- Fetch error: bad status for http://export.arxiv.org/api/query?search_query=(all:GPU+OR+all:CUDA)+AND+(all:matrix+multiplication+OR+all:GEMM+OR+all:matmul)&start=0&max_results=50

## arXiv: AI/ML + GPU (error)
- Fetch error: bad status for http://export.arxiv.org/api/query?search_query=(cat:cs.AI+OR+cat:cs.LG)+AND+(all:GPU+OR+all:CUDA+OR+all:accelerator)&start=0&max_results=50

## Search: last 90 days Hopper H100 tensor cores FP8 GEMM microarchitecture paper (error)
- Fetch error: web_search returned no in-window results (cutoff 2025-09-28)

## Search: last 90 days Blackwell B200 tensor core architecture matmul throughput research
- No new items found in the last 90 days.

## Search: last 90 days CUTLASS 3.x epilogue fusion GEMM kernel research blog (error)
- Fetch error: web_search returned no in-window results (cutoff 2025-09-28)

## Search: last 90 days cuBLASLt new features FP8 BF16 GEMM autotuning release notes (error)
- Fetch error: OpenAI response missing output text for web_search: {
  "id": "resp_0d8dbe10c156c96400694fe83883ec819f8ebb4f2a399f1539",
  "object": "response",
  "created_at": 1766844472,
  "status": "completed",
  "background": false,
  "billing": {
    "payer": "developer"
  },
  "completed_at": 1766844481,
  "error": null,
  "incomplete_details": null,
  "instructions": null,
  "max_output_tokens": 500,
  "max_tool_calls": null,
  "model": "gpt-5.2-2025-12-11",
  "output": [
    {
      "id": "ws_0d8dbe10c156c96400694fe8392798819fa7206795d778b55a",
      "type": "web_search_call",
      "status": "completed",
      "action": {
        "type": "search",
        "query": "cuBLASLt new features FP8 BF16 GEMM autotuning release notes"
      }
    },
    {
      "id": "ws_0d8dbe10c156c96400694fe83af0cc819fb5736967590b2060",
      "type": "web_search_call",
      "status": "completed",
      "action": {
        "type": "search",
        "query": "site:docs.nvidia.com cuBLASLt release notes 2025"
      }
    },
    {
      "id": "ws_0d8dbe10c156c96400694f...

## Search: last 90 days Triton compiler matmul kernel fusion performance regression study (error)
- Fetch error: web_search returned no in-window results (cutoff 2025-09-28)

## Search: last 90 days FlashAttention-3 GPU kernel implementation tensor cores matmul research (new: 1, showing: 1)
- [Pushing Tensor Accelerators Beyond MatMul in a User-Schedulable Language](https://arxiv.org/abs/2512.02371) — 2025-12-02
  - Authors: n/a
  - Affiliations: not provided
  - Extends Tensor Core-style acceleration beyond plain GEMM by using a user-schedulable Halide workflow plus equality-saturation instruction selection to rewrite/choose tensor-intrinsic implementations for more general tensor computations.
  - GPU-relevant compiler pipeline targets NVIDIA Tensor Cores (e.g., RTX 4070), showing how scheduling + instruction selection can map non-MatMul patterns onto MMA/Tensor Core ops while retaining performance portability.
  - Provides performance results demonstrating that carefully selected rewrites and schedules can approach MatMul-like efficiency on Tensor Cores for broader kernels than standard matrix multiplication.

## Search: last 90 days GPU systolic array mapping GEMM scheduling tensor core research (new: 1, showing: 1)
- [Optimal Software Pipelining and Warp Specialization for Tensor Core GPUs](https://arxiv.org/abs/2512.18134) — 2025-12-19
  - Authors: n/a
  - Affiliations: not provided
  - Presents **Twill**, a **constraint-solver** that automatically derives **provably optimal software-pipelined and warp-specialized schedules** for iterative kernels on **Tensor Core GPUs**, targeting high utilization of compute/memory pipelines.
  - Shows it can **rediscover and certify optimal FlashAttention schedules** on **NVIDIA Hopper and Blackwell**, implying strong performance for **Tensor Core–backed GEMM-like (matmul) fragments** and attention’s tiled matrix multiplications.
  - Practical takeaway for GPU practitioners: replaces manual schedule tuning with an automated method to choose **warp roles, stage counts, and overlap** to maximize Tensor Core throughput while hiding memory latency.

## Search: last 90 days MLIR LLVM CUDA GPU compiler matmul codegen tensor core research (new: 1, showing: 1)
- [Small Language Models as Compiler Experts: Auto-Parallelization for Heterogeneous Systems](https://arxiv.org/abs/2512.19250) — 2025-12-22
  - Authors: n/a
  - Affiliations: not provided
  - Uses small language models to guide compiler auto-parallelization decisions for heterogeneous targets (incl. GPUs), and benchmarks performance/quality against established GPU/compiler stacks like TVM and Triton (and LLVM Polly as a baseline).
  - GPU relevance: positions SLMs as “compiler experts” that can propose parallelization/mapping strategies (e.g., tiling, fusion, scheduling) to better utilize GPU parallelism and memory hierarchy versus traditional heuristic/analytic approaches.
  - Matrix multiplication: not explicitly mentioned in the provided abstract; if included in the paper’s benchmarks, it would likely be used to evaluate GPU tiling/scheduling quality against Triton/TVM-generated GEMM kernels.
