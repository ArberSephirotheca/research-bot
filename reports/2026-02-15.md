# GPU Research Digest — 2026-02-15
Generated: 2026-02-15T14:12:35.503282818+00:00
Sources: 12
New items: 3
Already seen: 19
Window: last 90 days
Sources failed: 2

## arXiv: GPU OR CUDA
- No new items found in the last 90 days.

## arXiv: ML + GPU
- No new items found in the last 90 days.

## arXiv: Matrix Multiplication + GPU
- No new items found in the last 90 days.

## arXiv: AI/ML + GPU
- No new items found in the last 90 days.

## Search: last 90 days Hopper H100 tensor cores FP8 GEMM microarchitecture paper
- No new items found in the last 90 days.

## Search: last 90 days Blackwell B200 tensor core MMA instruction set GEMM performance analysis (error)
- Fetch error: OpenAI response missing output text for web_search: {
  "id": "resp_0e244a1d84aae91a006991d4086fd48196a16ad208da862a34",
  "object": "response",
  "created_at": 1771164680,
  "status": "incomplete",
  "background": false,
  "billing": {
    "payer": "developer"
  },
  "completed_at": null,
  "error": null,
  "frequency_penalty": 0.0,
  "incomplete_details": {
    "reason": "max_output_tokens"
  },
  "instructions": null,
  "max_output_tokens": 500,
  "max_tool_calls": null,
  "model": "gpt-5.2-2025-12-11",
  "output": [
    {
      "id": "ws_0e244a1d84aae91a006991d409078c81969f21a6ca2fb8fded",
      "type": "web_search_call",
      "status": "completed",
      "action": {
        "type": "search",
        "queries": [
          "Blackwell B200 tensor core MMA instruction set GEMM performance analysis"
        ],
        "query": "Blackwell B200 tensor core MMA instruction set GEMM performance analysis"
      }
    },
    {
      "id": "ws_0e244a1d84aae91a006991d40a3f288196804164fe2f6102df",
      "type": "web_search_call",
      "status...

## Search: last 90 days CUTLASS 3.x grouped GEMM epilogue fusion SM90 SM100 blog
- No new items found in the last 90 days.

## Search: last 90 days cuBLASLt new features FP8 BF16 GEMM autotuning release notes
- No new items found in the last 90 days.

## Search: last 90 days Triton compiler matmul kernel fusion FP8 tensor cores performance (new: 1, showing: 1)
- [From hand-tuned to generated: A reproducible Triton GPU kernel benchmark across different vendors - Red Hat Emerging Technologies](https://next.redhat.com/2026/02/12/from-hand-tuned-to-generated-a-reproducible-triton-gpu-kernel-benchmark-across-different-vendors/) — 2026-02-12
  - Authors: n/a
  - Affiliations: not provided
  - Reproducible, cross-vendor benchmark suite for matrix multiplication (GEMM) and common fused variants (e.g., GEMM+GELU), reporting throughput in TFLOPS across multiple GPUs/backends.
  - Compares hand-tuned/autotuned implementations against Triton-generated kernels, highlighting where autotuning closes gaps or where Triton baselines lag/lead on different architectures.
  - Publishes methodology and machine-readable JSON results to enable apples-to-apples kernel performance comparisons and regression tracking for GPU practitioners.

## Search: last 90 days FlashAttention-3 implementation details Hopper tensor cores GEMM scheduling (new: 1, showing: 1)
- [FlashAttention: Efficient Tiled Self-Attention](https://www.emergentmind.com/topics/flashattention-fa) — 2026-01-25
  - Authors: n/a
  - Affiliations: not provided
  - Recasts attention as a tiled, IO-aware kernel that fuses QKᵀ GEMM + softmax + PV GEMM, keeping tiles in SRAM/registers to minimize HBM traffic and make the two matmuls the dominant work.
  - FlashAttention-3 (Hopper/H100) uses warp specialization and pipelining to overlap GEMM with softmax/normalization and memory movement, improving SM occupancy and approaching peak Tensor Core utilization.
  - Adds FP8 block-wise quantization (with appropriate scaling) to run attention matmuls on FP8 Tensor Cores while controlling error, further boosting throughput and utilization on H100.

## Search: last 90 days GPU systolic array matmul mapping tensor core warp-level MMA research (error)
- Fetch error: web_search returned no in-window results (cutoff 2025-11-17)

## Search: last 90 days CUDA 12.4 12.5 PTX wgmma GMMA matmul instruction documentation (new: 1, showing: 1)
- [High throughput injected PTX parallel compilation](https://www.reddit.com/r/CUDA/comments/1qesi1t/high_throughput_injected_ptx_parallel_compilation/) — 2026-01-16
  - Authors: n/a
  - Affiliations: not provided
  - Provides a standalone stress-test harness for NVIDIA GPU toolchains that compiles CUDA→PTX via NVRTC, injects randomized PTX stubs, then compiles PTX→CUBIN with nvPTXcompiler using high parallelism to measure throughput/scalability.
  - Useful for GPU practitioners building JIT/graph-capture/codegen systems (e.g., kernel fusion, autotuners) to benchmark and harden PTX compilation pipelines under heavy concurrent load.
  - No matrix multiplication–specific content indicated; focus is on compilation throughput rather than GEMM kernels.
