# GPU Research Digest — 2026-02-18
Generated: 2026-02-18T14:23:36.939991165+00:00
Sources: 12
New items: 5
Already seen: 7
Window: last 90 days
Sources failed: 6

## arXiv: GPU OR CUDA (error)
- Fetch error: bad status for http://export.arxiv.org/api/query?search_query=all:GPU+OR+all:CUDA&start=0&max_results=50

## arXiv: ML + GPU (error)
- Fetch error: bad status for http://export.arxiv.org/api/query?search_query=cat:cs.LG+AND+all:GPU&start=0&max_results=50

## arXiv: Matrix Multiplication + GPU (error)
- Fetch error: bad status for http://export.arxiv.org/api/query?search_query=(all:GPU+OR+all:CUDA)+AND+(all:matrix+multiplication+OR+all:GEMM+OR+all:matmul)&start=0&max_results=50

## arXiv: AI/ML + GPU (error)
- Fetch error: bad status for http://export.arxiv.org/api/query?search_query=(cat:cs.AI+OR+cat:cs.LG)+AND+(all:GPU+OR+all:CUDA+OR+all:accelerator)&start=0&max_results=50

## Search: last 90 days Hopper H100 tensor cores FP8 GEMM microarchitecture paper
- No new items found in the last 90 days.

## Search: last 90 days Blackwell B200 tensor core architecture GEMM throughput research (error)
- Fetch error: web_search returned no in-window results (cutoff 2025-11-20)

## Search: last 90 days CUTLASS 3.x grouped GEMM kernel fusion epilogue research (new: 1, showing: 1)
- [CUTLASS 3.0 GEMM Backwards Compatibility — NVIDIA CUTLASS Documentation](https://docs.nvidia.com/cutlass/4.3.5/media/docs/cpp/cutlass_3x_backwards_compatibility.html) — 2026-01-09
  - Authors: n/a
  - Affiliations: not provided
  - CUTLASS 3.x keeps the kernel/device API largely compatible with 2.x while shifting GEMM implementation to a composable model: **collective mainloop** (tile-level MMA + async memory movement) + **collective epilogue** (post-processing/writeback).
  - **GemmUniversal** is the primary entry point for GEMM, letting practitioners swap/compose mainloop and epilogue components to target specific GPUs (e.g., Tensor Cores, shared-memory pipelines) without rewriting the whole kernel.
  - The compatibility guidance helps migrate existing GEMM code to CUTLASS 3.x while preserving performance-critical tuning knobs (tile shapes, layouts, epilogue fusion) relevant to high-throughput matrix multiplication.

## Search: last 90 days cuBLASLt FP8 GEMM autotuning heuristics performance analysis
- No new items found in the last 90 days.

## Search: last 90 days Triton compiler matmul scheduling shared memory swizzle research
- No new items found in the last 90 days.

## Search: last 90 days FlashAttention-3 CUDA kernel implementation tensor cores FP8 research (new: 3, showing: 3)
- [SnapMLA: Efficient Long-Context MLA Decoding via Hardware-Aware FP8 Quantized Pipelining](https://arxiv.org/abs/2602.10718) — 2026-02-11
  - Authors: n/a
  - Affiliations: not provided
  - **FP8-quantized MLA decode pipeline:** Introduces a hardware-aware FP8 path for multi-head latent attention (MLA) decoding, emphasizing GEMM-friendly layouts and pipelined execution to keep Tensor Cores busy during long-context decode (where KV bandwidth/latency typically dominates).
  - **RoPE-aware per-token KV quantization:** Quantizes KV with awareness of RoPE rotation so accuracy holds under long contexts while enabling FP8 matmul/accumulation in the attention compute; reduces KV-cache footprint and memory traffic, improving decode throughput.
  - **Kernel + dataflow co-optimization:** Co-designs kernels and data movement (tiling, fusion, staging) to overlap KV dequant/transform with attention matmuls, minimizing HBM round-trips and improving end-to-end decode efficiency on modern NVIDIA GPUs.
- [PackInfer: Compute- and I/O-Efficient Attention for Batched LLM Inference](https://arxiv.org/abs/2602.06072) — 2026-02-03
  - Authors: n/a
  - Affiliations: not provided
  - Proposes a kernel-level “packed” attention framework for heterogeneous batched LLM inference (varying sequence lengths / KV-cache states), grouping compatible requests to improve GPU occupancy and reduce wasted work compared to per-request or poorly padded batching.
  - Improves compute and I/O efficiency by minimizing redundant loads/stores and avoiding unnecessary attention computation on padding/empty regions; targets better utilization than FlashAttention-style kernels under irregular batch shapes.
  - Emphasizes attention as tiled GEMM-like work (Q·Kᵀ and P·V) with packing strategies that increase effective matmul tile utilization and reduce memory traffic in KV-cache reads/writes.
- [FlashMLA: DeepSeek's CUDA Kernels for Lightning-Fast LLM Inference](https://yuv.ai/blog/flashmla) — 2026-01-29
  - Authors: n/a
  - Affiliations: not provided
  - Introduces FlashMLA, a set of specialized CUDA kernels for Multi-Head Latent Attention (MLA) inference on modern NVIDIA GPUs, emphasizing low-latency, high-throughput attention execution via architecture-aware tiling, memory coalescing, and warp-/SM-level scheduling.
  - Highlights FP8 KV-cache support to cut bandwidth and cache footprint, improving effective attention throughput; kernels are tuned to minimize HBM traffic and maximize on-chip reuse (shared memory/registers) during KV reads.
  - Uses GEMM-like (matrix-multiply-style) formulations and fused epilogues where possible to keep Tensor Cores busy, reduce kernel launches, and improve end-to-end LLM decode performance.

## Search: last 90 days GPU systolic array mapping GEMM warp-specialization research (error)
- Fetch error: web_search returned no in-window results (cutoff 2025-11-20)

## Search: last 90 days MLIR LLVM NVPTX GPU compiler matmul fusion tensor core lowering research (new: 1, showing: 1)
- [如何基于 MLIR 实现自动调优 (GPU & Ascend NPU) - 稳住·能赢 - 博客园](https://www.cnblogs.com/notlate-cn/p/19526556) — 2026-01-28
  - Authors: n/a
  - Affiliations: not provided
  - Uses MLIR Transform dialect to decouple **compute IR** (e.g., Linalg `matmul`) from **schedule IR** (tiling, vectorization, bufferization, mapping to GPU/NPU), enabling systematic exploration of matmul schedules without rewriting the core kernel.
  - Implements **auto-tuning as a search over Transform parameters** (tile sizes, vector widths, memory/layout choices, parallel mapping), compiling and benchmarking candidates to pick the fastest configuration for GPU (and Ascend NPU).
  - Highlights a practical workflow for GPU practitioners: generate multiple matmul variants via Transform passes, evaluate performance, and keep the best schedule—mirroring TVM-style tuning but within the MLIR compilation pipeline.
