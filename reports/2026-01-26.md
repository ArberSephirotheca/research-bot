# GPU Research Digest — 2026-01-26
Generated: 2026-01-26T14:13:24.344213244+00:00
Sources: 12
New items: 1
Already seen: 13
Window: last 90 days
Sources failed: 6

## arXiv: GPU OR CUDA
- No new items found in the last 90 days.

## arXiv: ML + GPU
- No new items found in the last 90 days.

## arXiv: Matrix Multiplication + GPU
- No new items found in the last 90 days.

## arXiv: AI/ML + GPU
- No new items found in the last 90 days.

## Search: last 90 days Hopper H100 tensor cores GEMM microarchitecture paper (error)
- Fetch error: web_search returned no in-window results (cutoff 2025-10-28)

## Search: last 90 days Blackwell B200 tensor core MMA instruction details GEMM performance analysis (error)
- Fetch error: web_search returned no in-window results (cutoff 2025-10-28)

## Search: last 90 days CUTLASS 3.x new kernels FP8 GEMM SM90 SM100 blog
- No new items found in the last 90 days.

## Search: last 90 days cuBLASLt FP8 GEMM autotuning heuristics release notes performance (error)
- Fetch error: web_search returned no in-window results (cutoff 2025-10-28)

## Search: last 90 days Triton compiler matmul kernel fusion FP8 BF16 improvements (error)
- Fetch error: web_search returned no in-window results (cutoff 2025-10-28)

## Search: last 90 days FlashAttention 3 GPU kernel matmul attention tensor cores benchmark (new: 1, showing: 1)
- [FlashAttention-3 Baseline Benchmarks](https://www.emergentmind.com/topics/flashattention-3-baseline) — 2025-12-20
  - Authors: n/a
  - Affiliations: not provided
  - Provides reference attention kernels and a benchmarking harness aimed at GPU practitioners, with empirical H100 results to compare throughput, latency, and hardware utilization across attention implementations.
  - Breaks down performance in terms of GPU-critical factors (memory bandwidth vs compute limits, occupancy, and IO patterns), enabling apples-to-apples baselines for optimized attention kernels.
  - Includes utilization/accuracy comparisons to validate that faster kernels preserve numerical behavior while improving effective GEMM/matmul efficiency within attention (QKᵀ and PV paths).

## Search: last 90 days GPU systolic array vs tensor cores matrix multiplication research preprint (error)
- Fetch error: web_search returned no in-window results (cutoff 2025-10-28)

## Search: last 90 days CUDA 12.4 12.5 PTX WGMMA GMMA matmul instruction documentation update (error)
- Fetch error: web_search returned no in-window results (cutoff 2025-10-28)
