# GPU Research Digest — 2026-02-12
Generated: 2026-02-12T14:26:45.733957101+00:00
Sources: 12
New items: 5
Already seen: 17
Window: last 90 days
Sources failed: 4

## arXiv: GPU OR CUDA
- No new items found in the last 90 days.

## arXiv: ML + GPU (new: 1, showing: 1)
- [GPU-Fuzz: Finding Memory Errors in Deep Learning Frameworks](https://arxiv.org/abs/2602.10478v1) — 2026-02-11
  - Authors: Zihao Li, Hongyi Lu, Yanan Guo, Zhenkai Zhang, Shuai Wang, Fengwei Zhang
  - Affiliations: not provided
  - Introduces **GPU-Fuzz**, a constraint-based fuzzer that models DL operator parameters as **formal constraints** and uses a solver to generate boundary-condition test cases that trigger **GPU kernel memory errors** (e.g., OOB, invalid accesses) more systematically than random fuzzing.
  - Demonstrates practical GPU impact across **PyTorch, TensorFlow, and PaddlePaddle**, finding **13 previously unknown GPU memory bugs**, highlighting risk for crashes and potential security issues in production GPU workloads.
  - No specific emphasis on **matrix multiplication/GEMM** in the abstract; the approach targets GPU operators broadly by stressing parameter edge cases that commonly break kernel indexing and memory bounds.

## arXiv: Matrix Multiplication + GPU
- No new items found in the last 90 days.

## arXiv: AI/ML + GPU
- No new items found in the last 90 days.

## Search: last 90 days Hopper H100 tensor cores FP8 GEMM performance analysis CUTLASS cuBLASLt (new: 2, showing: 2)
- [Overview — NVIDIA CUTLASS Documentation](https://docs.nvidia.com/cutlass/latest/overview.html) — 2026-02-05
  - Authors: n/a
  - Affiliations: not provided
  - CUTLASS is NVIDIA’s CUDA C++ template library for high-performance GEMM and related tensor operations, exposing tunable building blocks (tiling, warp-level MMA, epilogues) that map closely to Tensor Cores for practitioner-grade matmul performance.
  - Recent docs highlight Hopper/H100-oriented improvements (new Tensor Core instructions and scheduling/epilogue capabilities) and expanded low-precision support, notably FP8, to maximize throughput for modern training/inference GEMMs.
  - Includes performance guidance and references for selecting/benchmarking kernels (e.g., alignment/layout choices, tile shapes, and epilogue fusion) to approach peak matmul efficiency on current NVIDIA GPUs.
- [Worklog: Optimising GEMM on NVIDIA H100 for cuBLAS-like Performance (WIP) | Hamza's Blog](https://hamzaelshafie.bearblog.dev/worklog-optimising-gemm-on-nvidia-h100-for-cublas-like-performance-wip/) — 2026-01-12
  - Authors: n/a
  - Affiliations: not provided
  - Practical, step-by-step GEMM tuning on NVIDIA H100 (Hopper) aimed at approaching cuBLAS performance, with emphasis on mapping work to Tensor Cores and understanding Hopper-specific execution details.
  - Discusses Hopper features relevant to high-performance matmul—especially FP8 Tensor Core usage and TMA (Tensor Memory Accelerator) for efficient global→shared movement and better overlap of compute/memory.
  - Provides a practitioner-oriented workflow for diagnosing GEMM bottlenecks (tiling, pipeline stages, memory layout/traffic) and iterating toward cuBLAS-like throughput on H100.

## Search: last 90 days Blackwell B200 GB200 tensor core architecture changes GEMM matmul throughput research (error)
- Fetch error: web_search returned no in-window results (cutoff 2025-11-14)

## Search: last 90 days CUDA 12.4 12.5 cuBLASLt new features grouped GEMM epilogue fusion release notes (error)
- Fetch error: web_search returned no in-window results (cutoff 2025-11-14)

## Search: last 90 days CUTLASS 3.x new kernels SM90 SM100 GMMA warp-specialized GEMM blog paper
- No new items found in the last 90 days.

## Search: last 90 days Triton compiler matmul autotuning persistent kernels FP8 BF16 Hopper research
- No new items found in the last 90 days.

## Search: last 90 days FlashAttention-3 paper implementation Hopper tensor cores FP8 attention matmul kernel (error)
- Fetch error: web_search returned no in-window results (cutoff 2025-11-14)

## Search: last 90 days GPU kernel fusion compiler research GEMM epilogue fusion ML training performance (error)
- Fetch error: web_search returned no in-window results (cutoff 2025-11-14)

## Search: last 90 days systolic array vs GPU tensor cores matmul mapping research AI accelerator comparison (new: 2, showing: 2)
- [The Silicon Triad: Why CPUs, GPUs and TPUs Are Reshaping the Future of AI](https://medium.com/programmed-iq/the-silicon-triad-why-cpus-gpus-and-tpus-are-reshaping-the-future-of-ai-34212282ae67) — 2026-01-20
  - Authors: n/a
  - Affiliations: not provided
  - GPUs remain the most flexible high-throughput option for AI matrix multiplication, leveraging massive parallelism (SIMT) and mature software stacks to accelerate GEMM/conv workloads across training and inference.
  - TPUs emphasize peak matmul efficiency via systolic arrays (high data reuse, reduced memory traffic), often outperforming GPUs on dense, regular matrix-multiply-heavy models when shapes map cleanly to the array.
  - CPUs are positioned mainly for orchestration, preprocessing, and latency-sensitive control paths; they generally lag GPUs/TPUs on large-scale matmul throughput due to lower parallel compute density and bandwidth.
- [The Silicon Divergence: A Comprehensive Analysis of Heterogeneous Computing Architectures and Workload Placement Strategies](https://uplatz.com/blog/the-silicon-divergence-a-comprehensive-analysis-of-heterogeneous-computing-architectures-and-workload-placement-strategies/) — 2026-01-12
  - Authors: n/a
  - Affiliations: not provided
  - Surveys heterogeneous CPU/GPU/accelerator architectures with an AI-centric lens, highlighting GPU-relevant specs (e.g., SM/CU organization, memory hierarchy/bandwidth, interconnects like PCIe/NVLink, and tensor/matrix engines) and how these drive throughput vs. latency trade-offs.
  - Discusses workload placement strategies for deciding when to run on GPU vs. CPU/other accelerators, emphasizing data-movement costs, batching/parallelism requirements, and memory-footprint constraints—key determinants for end-to-end performance.
  - Includes a hardware comparison table useful for GPU practitioners to map workloads (especially dense linear algebra) to devices based on peak FLOPs (FP16/BF16/FP32), tensor-core availability, and memory bandwidth; matrix multiplication is treated as a canonical GPU-suited kernel when present.
