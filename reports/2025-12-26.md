# GPU Research Digest — 2025-12-26
Generated: 2025-12-26T14:08:11.737148304+00:00
Sources: 12
New items: 3
Already seen: 21
Window: last 90 days
Sources failed: 3

## arXiv: GPU OR CUDA
- No new items found in the last 90 days.

## arXiv: ML + GPU
- No new items found in the last 90 days.

## arXiv: Matrix Multiplication + GPU
- No new items found in the last 90 days.

## arXiv: AI/ML + GPU
- No new items found in the last 90 days.

## Search: last 90 days Hopper H100 tensor cores GEMM microarchitecture latency throughput whitepaper
- No new items found in the last 90 days.

## Search: last 90 days Blackwell B200 GB200 tensor core FP8 FP4 GEMM performance analysis
- No new items found in the last 90 days.

## Search: last 90 days CUTLASS 3.x new GEMM kernels SM90 SM100 TMA warp-specialized tutorial (new: 3, showing: 3)
- [CUTLASS 3.0 GEMM API — NVIDIA CUTLASS Documentation](https://docs.nvidia.com/cutlass/4.3.0/media/docs/cpp/gemm_api_3x.html) — 2025-12-23
  - Authors: n/a
  - Affiliations: not provided
  - CUTLASS 3.x introduces a revamped GEMM API that composes GEMM kernels from policy-selected building blocks (mainloop, epilogue, scheduling/dispatch), letting practitioners tune performance/portability across architectures without rewriting kernels.
  - For Hopper (SM90), it documents warp-specialized GEMM mainloops that pair TMA (Tensor Memory Accelerator) for async global→shared movement with GMMA (Hopper Tensor Core) for compute (e.g., `MainloopSm90TmaGmmaWarpSpecialized`), targeting higher throughput and better overlap of memory/compute.
  - Describes dispatch/scheduling variants (different “kernel schedules” and “epilogue schedules”) that trade off occupancy, latency hiding, and pipeline depth—useful for selecting the right GEMM configuration for specific matrix shapes and data types.
- [Deep Dive on CUTLASS Ping-Pong GEMM Kernel – PyTorch](https://pytorch.org/blog/cutlass-ping-pong-gemm-kernel/) — 2025-12-22
  - Authors: n/a
  - Affiliations: not provided
  - Explains the CUTLASS 3.x SM90 “ping-pong” GEMM: warp-specialized roles (producer/consumer) overlap TMA-based global→shared transfers with Tensor Core MMA to hide memory latency and keep compute saturated.
  - Details the SM90 data movement pipeline (TMA multicast/async, shared-memory staging, double-buffering) and how tile shapes, cluster/warp scheduling, and epilogue choices impact throughput and occupancy for matrix multiplication.
  - Provides PyTorch-oriented guidance for mapping GEMM workloads to this kernel (problem sizes, layouts, alignment) and interpreting performance limits (memory vs compute bound) on Hopper GPUs.
- [Changelog — NVIDIA CUTLASS Documentation](https://docs.nvidia.com/cutlass/4.3.1/CHANGELOG.html) — 2025-12-23
  - Authors: n/a
  - Affiliations: not provided
  - CUTLASS 3.x adds full support for NVIDIA Blackwell (SM100) kernels, expanding GEMM/matrix-multiply coverage and performance portability to the newest architecture.
  - Introduces Blackwell-specific warp-specialization and scheduling features to better utilize SM100 execution resources, improving throughput/latency for GEMM-heavy workloads.
  - Release notes highlight ongoing kernel/API updates relevant to high-performance linear algebra on GPUs (notably matmul/GEMM paths).

## Search: last 90 days cuBLASLt matmul epilogue fusion FP8 BF16 performance regression benchmark (error)
- Fetch error: web_search returned no in-window results (cutoff 2025-09-27)

## Search: last 90 days Triton compiler matmul autotuning persistent kernels Hopper SM90 research
- No new items found in the last 90 days.

## Search: last 90 days FlashAttention-3 CUDA kernel tensor cores FP8 matmul fused attention benchmark (error)
- Fetch error: web_search returned no in-window results (cutoff 2025-09-27)

## Search: last 90 days GPU systolic array vs SIMT tensor core matmul mapping research paper (error)
- Fetch error: web_search returned no in-window results (cutoff 2025-09-27)

## Search: last 90 days CUDA kernel fusion compiler passes matmul epilogue fusion ML training performance
- No new items found in the last 90 days.
