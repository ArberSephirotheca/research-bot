# GPU Research Digest — 2026-01-31
Generated: 2026-01-31T14:10:10.726798623+00:00
Sources: 12
New items: 4
Already seen: 14
Window: last 90 days
Sources failed: 6

## arXiv: GPU OR CUDA
- No new items found in the last 90 days.

## arXiv: ML + GPU
- No new items found in the last 90 days.

## arXiv: Matrix Multiplication + GPU
- No new items found in the last 90 days.

## arXiv: AI/ML + GPU
- No new items found in the last 90 days.

## Search: last 90 days Hopper H100 tensor cores FP8 GEMM microarchitecture paper (error)
- Fetch error: web_search returned no in-window results (cutoff 2025-11-02)

## Search: last 90 days CUTLASS 3.x new GEMM kernels SM90 TMA warp-specialization blog (new: 3, showing: 3)
- [Changelog — NVIDIA CUTLASS Documentation](https://docs.nvidia.com/cutlass/4.3.5/CHANGELOG.html) — 2026-01-09
  - Authors: n/a
  - Affiliations: not provided
  - CUTLASS 4.3.5 fixes multiple CuTe DSL issues, including a CPU overhead regression introduced in 4.3.4—relevant for reducing host-side bottlenecks when launching/tuning GEMM kernels.
  - CUTLASS C++ updates switch to CUDA Driver “Get Version” runtime APIs, improving version detection/compatibility logic for GPU deployments and build/runtime integration.
- [Changelog — NVIDIA CUTLASS Documentation](https://docs.nvidia.com/cutlass/4.3.5/CHANGELOG.html#id3) — 2025-12-22
  - Authors: n/a
  - Affiliations: not provided
  - CUTLASS 4.3.4 adds CuTe DSL support for Programmatic Dependent Launch (PDL), enabling more flexible GPU kernel launch/control patterns that can benefit fused GEMM and other tiled linear algebra pipelines.
  - Includes CUTLASS C++ workarounds for an NVIDIA driver bug that could cause intermittent kernel execution errors, improving reliability for GEMM-heavy workloads.
- [Changelog — NVIDIA CUTLASS Documentation](https://docs.nvidia.com/cutlass/4.3.5/CHANGELOG.html#id4) — 2025-12-12
  - Authors: n/a
  - Affiliations: not provided
  - CUTLASS 4.3.3 adds C++ support for a **blockscaled variant of ragged, contiguous grouped GEMM**, targeting efficient GEMM over variable-sized (ragged) matrices common in batched workloads.
  - Introduces a **simplified Mixture-of-Experts (MoE) API** for this grouped GEMM path (example 92), easing integration of MoE-style routed GEMMs on NVIDIA GPUs.

## Search: last 90 days cuBLASLt FP8 GEMM performance tuning SM90 release notes (error)
- Fetch error: web_search returned no in-window results (cutoff 2025-11-02)

## Search: last 90 days Triton compiler matmul autotuning FP8 BF16 Hopper tutorial (error)
- Fetch error: OpenAI response missing output text for web_search: {
  "id": "resp_067373f6f309af5400697e0d0971f881a2a0ef1d4e6240aecd",
  "object": "response",
  "created_at": 1769868553,
  "status": "incomplete",
  "background": false,
  "billing": {
    "payer": "developer"
  },
  "completed_at": null,
  "error": null,
  "frequency_penalty": 0.0,
  "incomplete_details": {
    "reason": "max_output_tokens"
  },
  "instructions": null,
  "max_output_tokens": 500,
  "max_tool_calls": null,
  "model": "gpt-5.2-2025-12-11",
  "output": [
    {
      "id": "ws_067373f6f309af5400697e0d09d05081a2a91cddbe649d6099",
      "type": "web_search_call",
      "status": "completed",
      "action": {
        "type": "search",
        "queries": [
          "Triton compiler matmul autotuning FP8 BF16 Hopper tutorial"
        ],
        "query": "Triton compiler matmul autotuning FP8 BF16 Hopper tutorial"
      }
    },
    {
      "id": "ws_067373f6f309af5400697e0d0aac3881a29c5a9d69ed1006c2",
      "type": "web_search_call",
      "status": "completed",
      "actio...

## Search: last 90 days FlashAttention-3 paper Hopper H100 tensor cores fused attention kernels (error)
- Fetch error: web_search returned no in-window results (cutoff 2025-11-02)

## Search: last 90 days GPU kernel fusion compiler research matmul epilogue fusion CUDA (error)
- Fetch error: web_search returned no in-window results (cutoff 2025-11-02)

## Search: last 90 days systolic array vs tensor core GEMM mapping GPU architecture research (new: 1, showing: 1)
- [Rohan Yadav — Warp Specialization](https://rohany.github.io/blog/warp-specialization/) — 2026-01-28
  - Authors: n/a
  - Affiliations: not provided
  - Describes **warp specialization**: assign distinct roles to different warps within a thread block (e.g., one set handles **Tensor Core MMA compute**, others handle **async global→shared copies / TMA**, epilogue, etc.) to keep the compute warps continuously issuing HMMA/WGMMA while hiding memory latency.
  - Explains how this mitigates **SIMT divergence and instruction-mix interference** in GEMM kernels: separating load/store/control-heavy code paths from the MMA hot loop improves Tensor Core utilization on **Hopper/Blackwell**.
  - Highlights practical implications for high-performance **matrix multiplication**: structure kernels as producer/consumer pipelines with dedicated warps, using shared-memory staging and asynchronous transfers to sustain throughput.

## Search: last 90 days NVIDIA Blackwell B200 tensor cores FP4 GEMM architecture details (error)
- Fetch error: OpenAI response missing output text for web_search: {
  "id": "resp_0db39afa4bdc1bea00697e0d370fb88190b0bba78a52abbe6a",
  "object": "response",
  "created_at": 1769868599,
  "status": "completed",
  "background": false,
  "billing": {
    "payer": "developer"
  },
  "completed_at": 1769868610,
  "error": null,
  "frequency_penalty": 0.0,
  "incomplete_details": null,
  "instructions": null,
  "max_output_tokens": 500,
  "max_tool_calls": null,
  "model": "gpt-5.2-2025-12-11",
  "output": [
    {
      "id": "ws_0db39afa4bdc1bea00697e0d376de881908efb8c6803143567",
      "type": "web_search_call",
      "status": "completed",
      "action": {
        "type": "search",
        "queries": [
          "NVIDIA Blackwell B200 tensor cores FP4 GEMM architecture details"
        ],
        "query": "NVIDIA Blackwell B200 tensor cores FP4 GEMM architecture details"
      }
    },
    {
      "id": "ws_0db39afa4bdc1bea00697e0d38714c8190b0e2a1cef45c08b2",
      "type": "web_search_call",
      "status": "completed",
      "action": {
        "typ...
