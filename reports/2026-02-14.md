# GPU Research Digest — 2026-02-14
Generated: 2026-02-14T14:12:04.401614194+00:00
Sources: 12
New items: 2
Already seen: 25
Window: last 90 days
Sources failed: 1

## arXiv: GPU OR CUDA
- No new items found in the last 90 days.

## arXiv: ML + GPU
- No new items found in the last 90 days.

## arXiv: Matrix Multiplication + GPU
- No new items found in the last 90 days.

## arXiv: AI/ML + GPU
- No new items found in the last 90 days.

## Search: last 90 days Hopper H100 tensor cores FP8 GEMM microarchitecture paper
- No new items found in the last 90 days.

## Search: last 90 days Blackwell B200 tensor core MMA instruction set GEMM performance analysis (new: 1, showing: 1)
- [Why can the mma instruction only reach 50% peak computing throughput? - Jetson Thor - NVIDIA Developer Forums](https://forums.developer.nvidia.com/t/why-can-the-mma-instruction-only-reach-50-peak-computing-throughput/359745) — 2026-02-05
  - Authors: n/a
  - Affiliations: not provided
  - On Jetson Thor, `tcgen05.mma.cta_group::2.kind::mxf4nvf4.block_scale.scale_vec::4X` can hit near-peak nvfp4 GEMM throughput only when enough independent CTA groups/clusters are active; increasing cluster dimensions reduces the number of concurrently active clusters per SM/GPC, cutting effective SM occupancy.
  - The observed ~50% peak is primarily a scheduling/occupancy limit (active cluster count) rather than the MMA datapath itself: fewer resident clusters means fewer warps feeding Tensor Cores, so MMA issue slots go underutilized.
  - Practical implication for GEMM tuning: choose cluster/CTA shapes that maximize concurrent cluster residency (and thus warp-level parallelism) while meeting shared-memory/register constraints; overly large clusters can halve throughput even with the same MMA instruction.

## Search: last 90 days CUTLASS 3.x new GEMM kernels FP8 BF16 SM90 SM100 blog release notes
- No new items found in the last 90 days.

## Search: last 90 days cuBLASLt GEMM autotuning heuristics FP8 BF16 performance regression study (new: 1, showing: 1)
- [Optimizing DeepSeek R1 Throughput on NVIDIA Blackwell GPUs: A Deep Dive for Developers — TensorRT-LLM](https://nvidia.github.io/TensorRT-LLM/1.1.0rc2.post1/blogs/tech_blog/blog3_Optimizing_DeepSeek_R1_Throughput_on_NVIDIA_Blackwell_GPUs.html) — 2026-01-24
  - Authors: n/a
  - Affiliations: not provided
  - Uses FP8 quantization on NVIDIA Blackwell to boost DeepSeek R1 throughput, with attention to how FP8 changes GEMM shapes/tiling and makes matmul performance more sensitive to algorithm choice.
  - Shows that default cuBLAS/cuBLASLt heuristics can pick suboptimal GEMM kernels for specific (M,N,K), batch, and layout patterns common in LLM inference; recommends manual tuning when matmul dominates.
  - Proposes offline benchmarking/selection of the best `cublasLtMatmul` algorithms per target shape (and caching those choices) to consistently hit higher GEMM throughput than heuristic selection.

## Search: last 90 days Triton compiler matmul kernel fusion pipelining shared memory swizzle research
- No new items found in the last 90 days.

## Search: last 90 days FlashAttention-3 GPU kernel implementation tensor cores FP8 BF16 benchmarks (error)
- Fetch error: web_search returned no in-window results (cutoff 2025-11-16)

## Search: last 90 days systolic array vs GPU tensor cores matrix multiplication roofline analysis 2025
- No new items found in the last 90 days.

## Search: last 90 days GPU compiler research MLIR LLVM CUDA matmul scheduling tensor core codegen
- No new items found in the last 90 days.
