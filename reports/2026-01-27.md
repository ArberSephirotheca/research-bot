# GPU Research Digest — 2026-01-27
Generated: 2026-01-27T14:14:38.203319301+00:00
Sources: 12
New items: 4
Already seen: 19
Window: last 90 days
Sources failed: 3

## arXiv: GPU OR CUDA
- No new items found in the last 90 days.

## arXiv: ML + GPU (new: 1, showing: 1)
- [CUROCKET: Optimizing ROCKET for GPU](https://arxiv.org/abs/2601.17091v1) — 2026-01-23
  - Authors: Ole Stüven, Keno Moenck, Thorsten Schüppstuhl
  - Affiliations: not provided
  - Implements ROCKET’s random 1D convolutions efficiently on GPU despite inhomogeneous (variable) kernels that make standard GPU convolution approaches inefficient; targets high parallelism for time-series feature extraction.
  - Reports up to ~11× higher computational efficiency per watt versus CPU ROCKET, indicating substantial throughput/energy gains from GPU execution.
  - Emphasis is on custom GPU convolution/feature extraction rather than reformulating the workload as GEMM/matrix multiplication (no explicit matmul-centric approach highlighted in the abstract).

## arXiv: Matrix Multiplication + GPU
- No new items found in the last 90 days.

## arXiv: AI/ML + GPU
- No new items found in the last 90 days.

## Search: last 90 days Hopper H100 tensor cores FP8 GEMM microarchitecture paper
- No new items found in the last 90 days.

## Search: last 90 days Blackwell B200 tensor core architecture FP4 FP8 GEMM performance analysis
- No new items found in the last 90 days.

## Search: last 90 days CUTLASS 3.x new GEMM kernels SM90 SM100 tutorial blog (error)
- Fetch error: web_search returned no in-window results (cutoff 2025-10-29)

## Search: last 90 days cuBLASLt GEMM autotuning heuristics SM90 FP8 BF16 release notes (new: 2, showing: 2)
- [Release Notes — cuBLASDx 0.5.0](https://docs.nvidia.com/cuda/cublasdx/0.5.0/release_notes.html) — 2026-01-26
  - Authors: n/a
  - Affiliations: not provided
  - Adds an experimental pipelining API in cuBLASDx 0.5.0, targeting higher-throughput GEMM/matrix-multiply kernels via better overlap of memory movement and compute.
  - Introduces initial Hopper/Blackwell-oriented features for matmul acceleration: TMA (Tensor Memory Accelerator), WGMMA, and 1SM UTCMMA support, plus CUDA 13.1 compatibility.
  - Notes known issues: specific CUDA toolchain versions can miscompile certain fp8/bf16 paths, so GEMM correctness/perf may depend on CUDA version and configuration.
- [Release Notes — cuBLASDx 0.3.1](https://docs.nvidia.com/cuda/cublasdx/0.3.1/release_notes.html) — 2026-01-22
  - Authors: n/a
  - Affiliations: not provided
  - Minor cuBLASDx update focused on quality-of-life improvements for GPU GEMM workflows.
  - Improves GEMM heuristic selection, especially for “non-suggested” execution configurations, to better choose performant kernels.
  - Overall aims at more robust matrix multiplication performance/behavior without major API or feature changes.

## Search: last 90 days Triton compiler matmul kernel fusion pipelining shared memory swizzle research
- No new items found in the last 90 days.

## Search: last 90 days FlashAttention 3 CUDA kernel implementation tensor cores FP8 BF16 benchmarks (new: 1, showing: 1)
- [FlashAttention-3: GPU-Optimized Attention](https://www.emergentmind.com/topics/flashattention-3) — 2026-01-24
  - Authors: n/a
  - Affiliations: not provided
  - FlashAttention-3 is an attention kernel redesign targeting NVIDIA Hopper, maximizing Tensor Core utilization by restructuring attention around GEMM-heavy (QKᵀ and PV) computation with better tiling/scheduling to reduce memory traffic and keep math units busy.
  - Emphasizes low-precision pipelines (notably FP8 on Hopper) to raise end-to-end attention throughput; reports substantially higher achieved TFLOP/s vs prior FlashAttention versions by aligning data movement, accumulation, and Tensor Core-friendly layouts.
  - Notes accuracy considerations for FP8 attention (e.g., scaling/accumulation choices and mixed-precision strategies) to preserve model quality while exploiting FP8 speedups.

## Search: last 90 days GPU systolic array mapping GEMM tiling dataflow tensor core utilization study (error)
- Fetch error: web_search returned no in-window results (cutoff 2025-10-29)

## Search: last 90 days MLIR LLVM NVPTX CUDA matmul codegen tensor core wgmma research (error)
- Fetch error: web_search returned no in-window results (cutoff 2025-10-29)
