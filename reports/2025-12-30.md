# GPU Research Digest — 2025-12-30
Generated: 2025-12-30T14:09:21.289115257+00:00
Sources: 12
New items: 3
Already seen: 18
Window: last 90 days
Sources failed: 5

## arXiv: GPU OR CUDA
- No new items found in the last 90 days.

## arXiv: ML + GPU
- No new items found in the last 90 days.

## arXiv: Matrix Multiplication + GPU
- No new items found in the last 90 days.

## arXiv: AI/ML + GPU
- No new items found in the last 90 days.

## Search: last 90 days Hopper H200 tensor cores FP8 GEMM performance analysis CUTLASS cuBLAS (new: 2, showing: 2)
- [Deep Dive on the Hopper TMA Unit for FP8 GEMMs – PyTorch](https://pytorch.org/blog/hopper-tma-unit/) — 2025-12-28
  - Authors: n/a
  - Affiliations: not provided
  - Benchmarks FP8 GEMM on H100 showing Triton kernels using Hopper’s Tensor Memory Accelerator (TMA) can approach or match cuBLAS FP8/FP16 and CUTLASS “ping‑pong” FP8 performance, with TFLOPs tables across shapes highlighting when each path wins (often driven by tile shape, K size, and memory/launch overheads).
  - Explains how TMA changes the GEMM data-movement bottleneck: bulk 2D/3D async copies into shared memory with less per-thread address math, plus practical details on TMA descriptor creation/passing overhead and why descriptor handling matters for small/medium GEMMs.
  - Discusses scaling behavior and tuning implications for practitioners (e.g., pipeline depth, shared-memory staging, warp-group MMA scheduling) and where FP8 compute becomes compute-bound vs memory/latency
- [Blackwell GPU Architecture](https://www.emergentmind.com/topics/blackwell-gpu-architecture) — 2025-12-25
  - Authors: n/a
  - Affiliations: not provided
  - Summarizes NVIDIA Blackwell GPU architecture with a mixed‑precision Tensor Core throughput table, explicitly comparing Blackwell vs Hopper/H200 (incl. FP8 TFLOPS), which is directly useful for sizing GEMM/attention workloads across generations.
  - Highlights matrix‑multiplication–centric performance via Tensor Core modes (e.g., FP8 and other mixed precisions), providing quick reference numbers for practitioners optimizing matmul-heavy training/inference.
  - Cites a Dec 2025 source for the throughput figures, making it a handy contextual anchor when discussing/validating H200 FP8 capability and expected Blackwell uplift.

## Search: last 90 days Blackwell B200 GPU architecture tensor cores matrix multiplication GEMM whitepaper analysis (error)
- Fetch error: send OpenAI response for web_search

## Search: last 90 days Triton compiler matmul autotuning FP8 BF16 GPU kernel performance blog
- No new items found in the last 90 days.

## Search: last 90 days CUTLASS 3.x grouped GEMM epilogue fusion CUDA 12.5 12.6 release notes (error)
- Fetch error: OpenAI response missing output text for web_search: {
  "id": "resp_05502875f1e7a162006953dcd1aee0819689f60991e3529e36",
  "object": "response",
  "created_at": 1767103697,
  "status": "incomplete",
  "background": false,
  "billing": {
    "payer": "developer"
  },
  "completed_at": null,
  "error": null,
  "incomplete_details": {
    "reason": "max_output_tokens"
  },
  "instructions": null,
  "max_output_tokens": 500,
  "max_tool_calls": null,
  "model": "gpt-5.2-2025-12-11",
  "output": [
    {
      "id": "ws_05502875f1e7a162006953dcd2174081969f703e73c03f08e0",
      "type": "web_search_call",
      "status": "completed",
      "action": {
        "type": "search",
        "query": "CUTLASS 3.x grouped GEMM epilogue fusion CUDA 12.5 release notes"
      }
    },
    {
      "id": "ws_05502875f1e7a162006953dcd4f9688196a31709df0b1b2514",
      "type": "web_search_call",
      "status": "completed",
      "action": {
        "type": "search",
        "query": "CUDA 12.5 release notes CUTLASS"
      }
    },
    {
      "id": "ws_05502...

## Search: last 90 days cuBLASLt new features FP8 GEMM heuristics autotuning CUDA 12.5 12.6 (error)
- Fetch error: OpenAI response missing output text for web_search: {
  "id": "resp_07d4346f8eab80ff006953dcdf1384819184112f339d7c5061",
  "object": "response",
  "created_at": 1767103711,
  "status": "incomplete",
  "background": false,
  "billing": {
    "payer": "developer"
  },
  "completed_at": null,
  "error": null,
  "incomplete_details": {
    "reason": "max_output_tokens"
  },
  "instructions": null,
  "max_output_tokens": 500,
  "max_tool_calls": null,
  "model": "gpt-5.2-2025-12-11",
  "output": [
    {
      "id": "ws_07d4346f8eab80ff006953dcdfdb148191a667512ef232eaf1",
      "type": "web_search_call",
      "status": "completed",
      "action": {
        "type": "search",
        "query": "cuBLASLt new features FP8 GEMM heuristics autotuning CUDA 12.5 12.6"
      }
    },
    {
      "id": "ws_07d4346f8eab80ff006953dce1f6188191846c438213a91727",
      "type": "web_search_call",
      "status": "completed",
      "action": {
        "type": "search",
        "query": "site:docs.nvidia.com \"12.6\" \"cuBLAS\" \"Release\" \"New Features\" FP...

## Search: last 90 days FlashAttention-3 GPU kernel implementation tensor cores FP8 BF16 performance (error)
- Fetch error: web_search returned no in-window results (cutoff 2025-10-01)

## Search: last 90 days GPU kernel fusion compiler MLIR CUDA matmul epilogue fusion research paper (new: 1, showing: 1)
- [Mirage Persistent Kernel: A Compiler and Runtime for Mega-Kernelizing Tensor Programs](https://arxiv.org/abs/2512.22219) — 2025-12-22
  - Authors: n/a
  - Affiliations: not provided
  - Compiles multi-GPU inference graphs into a single **persistent “mega-kernel”** using an **SM-level graph IR**, reducing kernel-launch overhead and improving on-GPU producer/consumer locality (fewer round-trips through global memory).
  - Runtime schedules work across SMs inside the persistent kernel, enabling **fine-grained fusion** of tensor ops (including GEMM-adjacent patterns like matmul + epilogue/normalization) and better overlap of compute/communication for inference.
  - Generates optimized CUDA code tailored to the fused graph, aiming for higher **SM occupancy and data reuse** than conventional per-op kernels, especially when matmul-heavy pipelines are bottlenecked by launch latency and memory traffic.

## Search: last 90 days systolic array vs tensor core matrix multiplication AI accelerator research comparison (error)
- Fetch error: web_search returned no in-window results (cutoff 2025-10-01)
