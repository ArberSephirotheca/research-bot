# GPU Research Digest — 2026-02-13
Generated: 2026-02-13T14:21:57.053005218+00:00
Sources: 12
New items: 5
Already seen: 20
Window: last 90 days
Sources failed: 2

## arXiv: GPU OR CUDA
- No new items found in the last 90 days.

## arXiv: ML + GPU
- No new items found in the last 90 days.

## arXiv: Matrix Multiplication + GPU
- No new items found in the last 90 days.

## arXiv: AI/ML + GPU (new: 1, showing: 1)
- [DICE: Diffusion Large Language Models Excel at Generating CUDA Kernels](https://arxiv.org/abs/2602.11715v1) — 2026-02-12
  - Authors: Haolei Bai, Lingcheng Kong, Xueyi Chen, Jianmian Wang, Zhiqiang Tao, Huan Wang
  - Affiliations: not provided
  - Builds **CuKe**, a curated/augmented SFT dataset targeting **high-performance CUDA kernels**, addressing the scarcity of quality CUDA training data for codegen models.
  - Proposes **BiC-RL** (two-stage RL): **kernel infilling** (structure/holes first) then **end-to-end kernel generation**, leveraging diffusion LLMs’ non-autoregressive refinement to improve correctness/performance.
  - **DICE (1.7B/4B/8B)** sets new **KernelBench** SOTA, outperforming similarly sized AR and diffusion baselines for CUDA kernel generation; abstract doesn’t specifically call out **GEMM/matrix multiplication**.

## Search: last 90 days Hopper H100 tensor cores FP8 GEMM microarchitecture paper
- No new items found in the last 90 days.

## Search: last 90 days Blackwell B200 tensor core architecture FP4 FP8 matrix multiplication research (new: 3, showing: 3)
- [Chasing 6+ TB/s: an MXFP8 quantizer on Blackwell](https://blog.fal.ai/chasing-6-tb-s-an-mxfp8-quantizer-on-blackwell/) — 2026-01-27
  - Authors: n/a
  - Affiliations: not provided
  - Implements an MXFP8 quantizer in CuTeDSL that sustains 6+ TB/s on NVIDIA Blackwell B200, targeting high-throughput activation/weight quantization on GPU.
  - Writes UE8M0 scale exponents directly into Blackwell’s `tcgen05` packed scale layout, so downstream block-scaled GEMMs can consume scales natively without an extra repacking kernel.
  - Practical implication for matmul pipelines: reduces preprocessing overhead and improves end-to-end throughput for FP8 block-scaled GEMM by aligning quantizer output with Tensor Core scale expectations.
- [D-Legion: A Scalable Many-Core Architecture for Accelerating Matrix Multiplication in Quantized LLMs](https://arxiv.org/abs/2602.06252) — 2026-02-05
  - Authors: n/a
  - Affiliations: not provided
  - Proposes a scalable many-core design built around **adaptive-precision systolic arrays** to accelerate LLM **GEMM/matmul** under quantization, targeting higher throughput/efficiency than fixed-precision datapaths.
  - Supports **multiple matmul modes**—including **quantized dense** and **quantized sparse**—suggesting hardware-level flexibility akin to GPU tensor-core mixed-precision + sparsity acceleration.
  - Emphasizes **scalability across many cores** for large matmul workloads, relevant to GPU practitioners thinking about tiling, dataflow, and inter-core communication for LLM inference/training kernels.
- [Bare-Metal Tensor Virtualization: Overcoming the Memory Wall in Edge-AI Inference on ARM64](https://arxiv.org/abs/2601.03324) — 2026-01-06
  - Authors: n/a
  - Affiliations: not provided
  - Proposes a “Virtual Tensor Core” on ARM64: hand-tuned SIMD microkernels plus a tensor/matrix layout that increases data reuse and reduces memory traffic—conceptually similar to GPU Tensor Core tiling/packing for GEMM under bandwidth limits.
  - Emphasizes memory-wall–aware matrix/tensor ops (notably GEMM-like workloads) via explicit blocking, packing, and contiguous layouts to keep operands in fast local storage (registers/cache), mirroring GPU shared-memory tiling strategies.
  - Takeaway for GPU practitioners: performance gains come primarily from layout + kernel co-design (tile shapes, packing formats, and compute/memory overlap), reinforcing that bandwidth—not FLOPs—dominates edge inference unless GEMM is structured for reuse.

## Search: last 90 days CUTLASS 3.x new kernels SM90 SM100 GEMM performance blog paper
- No new items found in the last 90 days.

## Search: last 90 days cuBLASLt GEMM autotuning heuristics epilogue fusion SM90 SM100
- No new items found in the last 90 days.

## Search: last 90 days Triton compiler matmul kernel fusion pipelining shared memory swizzle research
- No new items found in the last 90 days.

## Search: last 90 days FlashAttention-3 CUDA kernel tensor cores FP8 matmul tiling paper (error)
- Fetch error: web_search returned no in-window results (cutoff 2025-11-15)

## Search: last 90 days GPU systolic array mapping matrix multiplication tensor core MMA scheduling research (new: 1, showing: 1)
- [U.S. Patent Application: System and Method for Optimizing Machine Learning Inference Systems and Processes for Operating a Compiler Therefor (Application #20260003590)](https://patents.justia.com/patent/20260003590) — 2026-01-01
  - Authors: n/a
  - Affiliations: not provided
  - Compiler technique for mapping high-level MMA (matrix multiply-accumulate) ops onto specific NVIDIA Tensor Core instruction variants (e.g., `mma.sync.aligned.m16n8k16.row.col`) by selecting an instruction tile layout that matches the target hardware.
  - Uses composite mapping functions to translate between tensor memory/layout representations and the Tensor Core instruction’s expected fragment/tile layout, aiming to minimize data swizzles/reformats and improve Tensor Core utilization.
  - Emphasizes layout-aware lowering of GEMM-like inference kernels so the compiler can choose/transform operand layouts (row/col, alignment, tiling) to hit optimal Tensor Core paths.

## Search: last 90 days GPU compiler MLIR LLVM CUDA matmul codegen tensor core MMA optimization paper (error)
- Fetch error: web_search returned no in-window results (cutoff 2025-11-15)
