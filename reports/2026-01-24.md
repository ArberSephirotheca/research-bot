# GPU Research Digest — 2026-01-24
Generated: 2026-01-24T14:08:27.653964269+00:00
Sources: 12
New items: 3
Already seen: 14
Window: last 90 days
Sources failed: 5

## arXiv: GPU OR CUDA
- No new items found in the last 90 days.

## arXiv: ML + GPU
- No new items found in the last 90 days.

## arXiv: Matrix Multiplication + GPU
- No new items found in the last 90 days.

## arXiv: AI/ML + GPU
- No new items found in the last 90 days.

## Search: last 90 days Hopper H100 tensor cores GEMM research paper (error)
- Fetch error: web_search returned no in-window results (cutoff 2025-10-26)

## Search: last 90 days Blackwell B200 tensor core MMA instruction details matmul performance analysis (new: 1, showing: 1)
- [Matrix Multiplication on NVIDIA's Blackwell: Part 1 - Introduction](https://www.modular.com/blog/matrix-multiplication-on-nvidias-blackwell-part-1-introduction) — 2026-01-22
  - Authors: n/a
  - Affiliations: not provided
  - Introduces Blackwell’s 5th‑gen Tensor Cores (`tcgen05`) for GEMM, positioning them as the successor path to Hopper’s WGMMA and framing what changes for warp-/warpgroup-level matmul on NVIDIA GPUs.
  - Highlights support for larger MMA tile shapes on Blackwell, implying different optimal blocking/tiling strategies and potentially higher per-instruction math throughput for GEMM kernels.
  - Previews “tensor memory” concepts relevant to feeding Tensor Cores efficiently, focusing on data movement/layout considerations that impact practical matmul performance.

## Search: last 90 days CUTLASS 3.x new kernels FP8 GEMM blog post (error)
- Fetch error: web_search returned no in-window results (cutoff 2025-10-26)

## Search: last 90 days cuBLASLt FP8 GEMM autotuning heuristics performance study (new: 2, showing: 2)
- [NVIDIA Matmul Heuristics — nvMatmulHeuristics](https://docs.nvidia.com/cuda/nvidia-matmul-heuristics/) — 2026-01-22
  - Authors: n/a
  - Affiliations: not provided
  - Describes **nvMatmulHeuristics**, NVIDIA’s **analytic GEMM kernel-selection module** used to pick high-performance matmul configurations (including **FP8**) and underpinning parts of **cuBLAS** heuristics.
  - Focuses on **practitioner-facing guidance** for choosing/tuning matmul parameters (e.g., tile shapes, pipeline/stage choices, layouts) to maximize **throughput and utilization** on NVIDIA GPUs.
  - Highlights how heuristic-driven selection can reduce autotuning effort while still achieving near-optimal performance across **problem sizes, datatypes, and hardware generations**.
- [GEMM Heuristics — NVIDIA CUTLASS Documentation](https://docs.nvidia.com/cutlass/latest/media/docs/cpp/heuristics.html) — 2026-01-24
  - Authors: n/a
  - Affiliations: not provided
  - Describes CUTLASS GEMM “heuristics” that shrink or eliminate expensive runtime autotuning by selecting a small set of high-probability kernel configs (tile shapes, pipeline stages, layouts, epilogues) for given GEMM problem sizes and datatypes.
  - Uses NVIDIA’s `nvidia-matmul-heuristics` to map GEMM descriptors to recommended CUTLASS kernels, improving time-to-first-matmul and reducing tuning overhead while keeping near-optimal performance.
  - Notes coverage for modern GPU features, including FP8 (f8) GEMM heuristics on Hopper/Blackwell, aligning kernel choices with Tensor Core capabilities and architecture-specific constraints.

## Search: last 90 days Triton compiler matmul kernel fusion research preprint (error)
- Fetch error: web_search returned no in-window results (cutoff 2025-10-26)

## Search: last 90 days FlashAttention-3 GPU kernel implementation tensor cores matmul (error)
- Fetch error: web_search returned no in-window results (cutoff 2025-10-26)

## Search: last 90 days systolic array vs GPU tensor cores matrix multiplication comparison study (error)
- Fetch error: web_search returned no in-window results (cutoff 2025-10-26)

## Search: last 90 days GPU compiler MLIR LLVM CUDA matmul codegen tensor core scheduling research
- No new items found in the last 90 days.
