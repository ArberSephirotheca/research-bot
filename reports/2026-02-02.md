# GPU Research Digest — 2026-02-02
Generated: 2026-02-02T14:20:43.241191654+00:00
Sources: 12
New items: 2
Already seen: 15
Window: last 90 days
Sources failed: 4

## arXiv: GPU OR CUDA
- No new items found in the last 90 days.

## arXiv: ML + GPU (new: 1, showing: 1)
- [HetCCL: Accelerating LLM Training with Heterogeneous GPUs](https://arxiv.org/abs/2601.22585v1) — 2026-01-30
  - Authors: Heehoon Kim, Jaehwan Lee, Taejeoung Kim, Jongwon Park, Jinpyo Kim, Pyongwon Suh, Ryan H. Choi, Sangwoo Lee, Jaejin Lee
  - Affiliations: not provided
  - Enables high-performance collective communication (e.g., all-reduce) across heterogeneous GPU clusters (NVIDIA + AMD) by unifying NCCL/RCCL backends and adding RDMA-based cross-vendor paths without driver changes—critical for distributed LLM training scalability.
  - Matches native NCCL/RCCL performance on homogeneous clusters while uniquely providing efficient scaling when mixing vendors, reducing idle time/inefficiency in multi-GPU training runs.
  - No matrix multiplication/GEMM kernel contributions; impact is on communication efficiency that directly affects end-to-end throughput for matmul-heavy LLM training.

## arXiv: Matrix Multiplication + GPU
- No new items found in the last 90 days.

## arXiv: AI/ML + GPU
- No new items found in the last 90 days.

## Search: last 90 days Hopper H100 tensor cores GEMM microarchitecture paper (error)
- Fetch error: web_search returned no in-window results (cutoff 2025-11-04)

## Search: last 90 days Blackwell B200 tensor core MMA instruction set changes GEMM performance analysis
- No new items found in the last 90 days.

## Search: last 90 days CUTLASS 3.x new GEMM kernels SM90 SM100 tutorial blog
- No new items found in the last 90 days.

## Search: last 90 days cuBLASLt GEMM autotuning heuristics epilogue fusion performance regression (error)
- Fetch error: web_search returned no in-window results (cutoff 2025-11-04)

## Search: last 90 days Triton compiler matmul kernel fusion shared memory pipelining research
- No new items found in the last 90 days.

## Search: last 90 days FlashAttention-3 GPU kernel matmul attention tensor cores SM90 benchmark (error)
- Fetch error: web_search returned no in-window results (cutoff 2025-11-04)

## Search: last 90 days systolic array vs GPU tensor cores matrix multiplication roofline analysis (new: 1, showing: 1)
- [What is the roofline model? | GPU Glossary](https://modal.com/gpu-glossary/perf/roofline-model) — 2026-02-02
  - Authors: n/a
  - Affiliations: not provided
  - Roofline model: a quick way to bound GPU kernel performance by the minimum of **compute throughput** (FLOP/s “roof”) and **memory bandwidth** (B/s “roof”), using **arithmetic intensity** (FLOPs per byte) to see whether you’re compute- or memory-limited.
  - On GPUs you often consider multiple compute roofs (e.g., **FP32/FP16/Tensor Core** peak) plus a memory roof (HBM/global memory), helping diagnose whether optimizations should target **more reuse/tiling** (raise intensity) or **better math utilization** (raise achieved FLOP/s).
  - For **matrix multiplication (GEMM)**, arithmetic intensity is typically high due to data reuse, so well-tiled GEMMs tend to sit under the **Tensor Core/compute roof**; poor tiling or

## Search: last 90 days CUDA 12.4 12.5 GMMA WGMMA matmul programming guide examples (error)
- Fetch error: OpenAI response missing output text for web_search: {
  "id": "resp_03360cf73bdfbae0006980b2b00a24819c928b79d2925f4943",
  "object": "response",
  "created_at": 1770042032,
  "status": "incomplete",
  "background": false,
  "billing": {
    "payer": "developer"
  },
  "completed_at": null,
  "error": null,
  "frequency_penalty": 0.0,
  "incomplete_details": {
    "reason": "max_output_tokens"
  },
  "instructions": null,
  "max_output_tokens": 500,
  "max_tool_calls": null,
  "model": "gpt-5.2-2025-12-11",
  "output": [
    {
      "id": "ws_03360cf73bdfbae0006980b2b05b54819c81deddbaffbaa9bd",
      "type": "web_search_call",
      "status": "completed",
      "action": {
        "type": "search",
        "queries": [
          "CUDA 12.4 12.5 GMMA WGMMA matmul programming guide examples"
        ],
        "query": "CUDA 12.4 12.5 GMMA WGMMA matmul programming guide examples"
      }
    },
    {
      "id": "ws_03360cf73bdfbae0006980b2b14d90819c88b9940226f5cb87",
      "type": "web_search_call",
      "status": "completed",
      "act...
