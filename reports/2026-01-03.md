# GPU Research Digest — 2026-01-03
Generated: 2026-01-03T14:08:07.219940244+00:00
Sources: 12
New items: 5
Already seen: 18
Window: last 90 days
Sources failed: 2

## arXiv: GPU OR CUDA
- No new items found in the last 90 days.

## arXiv: ML + GPU
- No new items found in the last 90 days.

## arXiv: Matrix Multiplication + GPU
- No new items found in the last 90 days.

## arXiv: AI/ML + GPU
- No new items found in the last 90 days.

## Search: last 90 days Hopper H100 tensor cores FP8 GEMM research paper (new: 1, showing: 1)
- [Design in Tiles: Automating GEMM Deployment on Tile-Based Many-PE Accelerators](https://arxiv.org/abs/2512.13638) — 2025-12-15
  - Authors: n/a
  - Affiliations: not provided
  - Proposes an automated deployment framework that maps GEMM onto tile-based many-PE accelerators, emphasizing tiling/partitioning, data movement, and scheduling decisions that mirror GPU GEMM optimization concerns (tile sizes, reuse, and memory hierarchy fit).
  - Targets high-throughput low-precision GEMM (mentions FP8-scale performance), positioning the approach as competitive with GH200-class GPU systems—useful for practitioners thinking about how to automate kernel selection/tuning beyond hand-crafted CUTLASS/cuBLAS-style pipelines.
  - Highlights end-to-end automation for GEMM mapping rather than manual kernel engineering, suggesting a path toward compiler/runtime-driven GEMM performance portability across non-traditional, tile-centric accelerator architectures.

## Search: last 90 days Blackwell B200 GB200 tensor core architecture matmul throughput analysis (error)
- Fetch error: web_search returned no in-window results (cutoff 2025-10-05)

## Search: last 90 days CUTLASS 3.x new GEMM kernels SM90 SM100 performance blog (new: 2, showing: 2)
- [CUTLASS 4.x Changelog — NVIDIA CUTLASS Documentation](https://docs.nvidia.com/cutlass/latest/CHANGELOG.html) — 2025-12-12
  - Authors: n/a
  - Affiliations: not provided
  - Adds/updates CUTLASS 4.x kernels for newer NVIDIA architectures (Hopper SM90 and Blackwell SM100/SM120), including attention and GEMM-related kernel improvements relevant to high-throughput matmul workloads.
  - Improves GEMM variants such as grouped and blockwise GEMM (better performance/coverage and usability), targeting common batched/irregular matrix multiplication patterns in ML/HPC.
  - Enhances SM90 TF32 GEMM kernels and tooling (profiler improvements) to aid performance tuning and benchmarking of matmul/attention kernels on modern GPUs.
- [Releases · NVIDIA/cutlass · GitHub](https://github.com/NVIDIA/cutlass/releases) — 2025-12-24
  - Authors: n/a
  - Affiliations: not provided
  - CUTLASS 4.x releases (e.g., v4.3.4) continue to evolve NVIDIA’s CUDA C++ template library for high-performance GEMM and related linear algebra kernels, targeting modern GPU tensor core pipelines and mixed-precision workloads.
  - Release notes typically highlight GEMM/kernel performance tuning, expanded datatype/layout support, and fixes that improve correctness/stability for matrix multiplication and fused epilogues (common in DL inference/training).
  - Assets and examples in each release help practitioners integrate updated GEMM building blocks (including grouped/batched variants) into custom CUDA code or frameworks while tracking API/behavior changes across versions.

## Search: last 90 days cuBLASLt GEMM autotuning heuristics FP8 BF16 matmul update
- No new items found in the last 90 days.

## Search: last 90 days Triton compiler matmul kernel fusion shared memory pipelining research
- No new items found in the last 90 days.

## Search: last 90 days FlashAttention-3 GPU kernel matmul softmax fused attention performance study (error)
- Fetch error: web_search returned no in-window results (cutoff 2025-10-05)

## Search: last 90 days GPU systolic array vs SIMT tensor core matmul mapping research (new: 1, showing: 1)
- [Optimal Software Pipelining and Warp Specialization for Tensor Core GPUs](https://www.aimodels.fyi/papers/arxiv/optimal-software-pipelining-warp-specialization-tensor-core) — 2025-12-23
  - Authors: n/a
  - Affiliations: not provided
  - Proposes an automatic code-generation approach that applies software pipelining to overlap global-memory loads, shared-memory staging, and Tensor Core MMA execution, improving utilization for GEMM-like kernels.
  - Introduces warp specialization (assigning distinct roles such as load/compute/store to different warps) to reduce contention and hide latency, targeting higher throughput on Tensor Core GPUs.
  - Demonstrates that combining pipelining + specialization yields faster matrix multiplication/tensor workloads than conventional monolithic warp designs by better balancing memory bandwidth and Tensor Core compute.

## Search: last 90 days GPU compiler MLIR LLVM CUDA matmul scheduling tensor core codegen paper (new: 1, showing: 1)
- [Library Liberation: Competitive Performance Matmul Through Compiler-composed Nanokernels](https://arxiv.org/abs/2511.13764) — 2025-11-14
  - Authors: n/a
  - Affiliations: not provided
  - Uses an MLIR-based pipeline to *compose “nanokernels”* (small, reusable GPU codelets) into full matmul microkernels, targeting library-level performance without hand-written assembly/CUDA kernels.
  - Emphasizes compiler-driven selection/fusion of tiling, vectorization, shared-memory staging, and register blocking to generate competitive GEMM kernels across shapes/datatypes while improving portability.
  - Reduces dependence on vendor BLAS by enabling auto-generated matmul kernels that can be specialized per GPU architecture and workload at compile time.
