# GPU Research Digest — 2026-01-04
Generated: 2026-01-04T14:08:18.653791806+00:00
Sources: 12
New items: 1
Already seen: 18
Window: last 90 days
Sources failed: 4

## arXiv: GPU OR CUDA
- No new items found in the last 90 days.

## arXiv: ML + GPU
- No new items found in the last 90 days.

## arXiv: Matrix Multiplication + GPU
- No new items found in the last 90 days.

## arXiv: AI/ML + GPU
- No new items found in the last 90 days.

## Search: last 90 days Hopper H100 tensor cores FP8 GEMM microarchitecture paper
- No new items found in the last 90 days.

## Search: last 90 days Blackwell B200 tensor core GEMM performance analysis CUDA
- No new items found in the last 90 days.

## Search: last 90 days CUTLASS 3.x new GEMM kernels SM90 SM100 tutorial (error)
- Fetch error: web_search returned no in-window results (cutoff 2025-10-06)

## Search: last 90 days cuBLASLt matmul autotuning heuristics FP8 BF16 performance
- No new items found in the last 90 days.

## Search: last 90 days Triton compiler matmul kernel fusion shared memory pipelining (error)
- Fetch error: web_search returned no in-window results (cutoff 2025-10-06)

## Search: last 90 days FlashAttention-3 CUDA kernel implementation tensor cores matmul (error)
- Fetch error: web_search returned no in-window results (cutoff 2025-10-06)

## Search: last 90 days GPU systolic array vs tensor cores matrix multiplication research (new: 1, showing: 1)
- [Modular: Matrix Multiplication on Blackwell: Part 2 - Using Hardware Features to Optimize Matmul](https://www.modular.com/blog/matrix-multiplication-on-nvidias-blackwell-part-2-using-hardware-features-to-optimize-matmul) — 2026-01-04
  - Authors: n/a
  - Affiliations: not provided
  - Explains how to map matmul onto Blackwell Tensor Cores (WGMMA / `tcgen05.mma`) by choosing tile shapes and data layouts that satisfy hardware constraints, emphasizing warp-/warpgroup-level tiling and operand swizzles for peak throughput.
  - Shows how to use TMA (Tensor Memory Accelerator) to pipeline global→shared transfers (multicast/async) and overlap memory movement with compute, reducing SMEM staging overhead and improving sustained GEMM performance.
  - Highlights practical optimization levers for practitioners: alignment/stride requirements, shared-memory layout to avoid bank conflicts, and scheduling (double-buffering, warpgroup synchronization) to keep Tensor Cores fed.

## Search: last 90 days MLIR LLVM GPU compiler matmul lowering tensor core MMA (error)
- Fetch error: web_search returned no in-window results (cutoff 2025-10-06)
