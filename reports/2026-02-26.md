# GPU Research Digest — 2026-02-26
Generated: 2026-02-26T14:26:48.770595123+00:00
Sources: 12
New items: 7
Already seen: 18
Window: last 90 days
Sources failed: 3

## arXiv: GPU OR CUDA
- No new items found in the last 90 days.

## arXiv: ML + GPU
- No new items found in the last 90 days.

## arXiv: Matrix Multiplication + GPU
- No new items found in the last 90 days.

## arXiv: AI/ML + GPU
- No new items found in the last 90 days.

## Search: last 90 days GPU tensor core matrix multiplication research paper 2025 (error)
- Fetch error: web_search returned no in-window results (cutoff 2025-11-28)

## Search: last 90 days CUTLASS 3.x GEMM kernel fusion Hopper Blackwell performance blog (new: 1, showing: 1)
- [Crafting Efficient Kernels with Epilogue Fusion](https://blog.fal.ai/crafting-efficient-kernels-with-epilogue-fusion/) — 2026-02-03
  - Authors: n/a
  - Affiliations: not provided
  - GEMM epilogue fusion folds post-matmul ops (e.g., bias, activation, scaling) into the GEMM writeback path to eliminate extra global memory reads/writes, improving bandwidth efficiency and end-to-end throughput.
  - Highlights Hopper/Blackwell-relevant epilogue considerations (e.g., scheduling/latency hiding and memory traffic around the accumulator→output stage) and how to structure fused epilogues to keep the mainloop compute-bound.
  - Demonstrates CUTLASS EVT (Epilogue Visitor Tree) patterns for composing fused epilogues, including a custom visitor implementing a fused gated-SiLU, as a template for adding bespoke post-GEMM logic without separate kernels.

## Search: last 90 days cuBLASLt new features FP8 BF16 GEMM Hopper Blackwell release notes (new: 2, showing: 2)
- [Release Notes — cuBLASMp](https://docs.nvidia.com/cuda/cublasmp/release_notes/index.html) — 2026-02-09
  - Authors: n/a
  - Affiliations: not provided
  - Replaced NVSHMEM with NCCL Symmetric Memory in cuBLASMp v0.8.0, impacting multi-GPU communication/memory semantics for distributed GEMM workflows.
  - Added buffer registration APIs plus convenience alloc/free to better manage/optimize device memory used by cuBLASMp operations.
  - Introduced new matmul descriptor attributes and shipped bug fixes, improving configurability and stability of matrix multiplication.
- [libcublaslt12 binary package in Ubuntu Resolute ppc64el](https://launchpad.net/ubuntu/resolute/ppc64el/libcublaslt12) — 2026-01-29
  - Authors: n/a
  - Affiliations: not provided
  - Ubuntu Resolute (ppc64el) has a published binary package for `libcublaslt12` (cuBLASLt runtime), version `12.4.5.8~12.4.1-6`, in the `proposed` pocket—relevant for deploying NVIDIA GPU linear algebra on POWER systems.
  - cuBLASLt provides the runtime support for high-performance GEMM/matrix multiplication and related fused operations; this package availability affects which matmul kernels and optimizations are usable on that distro/arch combo.
  - Being in `proposed` indicates it’s not yet fully released/stable, so practitioners may need to opt-in to get newer cuBLASLt matmul behavior/perf fixes on ppc64el.

## Search: last 90 days Triton compiler matmul autotuning FP8 tensor cores research (new: 1, showing: 1)
- [Some Matrix Multiplication Engines Are Not As Accurate As We Thought](https://pytorch.org/blog/some-matrix-multiplication-engines-are-not-as-accurate-as-we-thought/) — 2026-02-12
  - Authors: n/a
  - Affiliations: not provided
  - Matmul “engine” choice (cuBLAS/cuBLASLt, CUTLASS, Triton, etc.) can materially change numerical accuracy for the same GEMM due to different accumulation paths, rounding, and internal kernel variants—especially pronounced with FP8/low-precision workflows.
  - Profiling shows Triton FP8 `tl.dot()` can lower to FP8 Tensor Core instructions (e.g., Hopper QGMMA), so Triton kernels may be using true hardware FP8 TC pipelines rather than scalar emulation; accuracy/perf characteristics follow the selected TC variant.
  - Seemingly “performance-only” compiler/autotune knobs (e.g., `num_warps`) can flip which TC instruction/kernel strategy is generated, changing both throughput and numerical error—so practitioners should validate accuracy across tuning configurations, not just across libraries.

## Search: last 90 days FlashAttention 3 GPU kernel matmul attention tensor cores Hopper (error)
- Fetch error: web_search returned no in-window results (cutoff 2025-11-28)

## Search: last 90 days GPU architecture research systolic array tensor core MMA instruction scheduling (error)
- Fetch error: web_search returned no in-window results (cutoff 2025-11-28)

## Search: last 90 days CUDA 12.4 12.5 matmul performance improvements wgmma Hopper Blackwell (new: 1, showing: 1)
- [NVIDIA cuTile Python Guide Shows 90% cuBLAS Performance for Matrix Ops](https://blockchain.news/news/nvidia-cutile-python-matrix-multiply-blackwell-tutorial) — 2026-01-14
  - Authors: n/a
  - Affiliations: not provided
  - NVIDIA’s cuTile Python guide (CUDA 13.1, Blackwell) demonstrates custom matrix multiplication kernels achieving >90% of cuBLAS throughput, suggesting Python-level development can get close to vendor-tuned GEMM performance.
  - Highlights cuTile as a tile-centric programming model for composing matmul/matrix ops with explicit control over tiling and data movement—useful for practitioners optimizing GEMM-like workloads on new Blackwell GPUs.
  - Performance claim is based on Blackwell testing; takeaway is that well-tuned tiling strategies in cuTile can approach cuBLAS efficiency for core matrix operations.

## Search: last 90 days AI accelerator research matrix multiplication dataflow SRAM bandwidth roofline 2025 (new: 2, showing: 2)
- [A Compute and Communication Runtime Model for Loihi 2](https://arxiv.org/abs/2601.10035) — 2026-01-15
  - Authors: n/a
  - Affiliations: not provided
  - Proposes a max-affine, multi-dimensional roofline-style *lower-bound* runtime model that separates compute vs. communication limits; conceptually similar to GPU roofline analysis but extended to include on-chip network/communication constraints.
  - Validates the model on a linear layer (matrix–vector multiply), showing when performance is compute-bound vs. communication-bound—useful for GPU practitioners thinking about GEMV/linear inference where bandwidth and interconnect dominate.
  - Highlights scaling regimes where communication becomes the bottleneck, paralleling multi-GPU strong-scaling limits (e.g., collective/NoC overheads overtaking FLOPs), and suggests modeling communication explicitly when projecting speedups.
- [A Novel SRAM In-Memory Computing Accelerator Design Approach with R2R-Ladder for AI Sensors and Eddy Current Testing](https://www.mdpi.com/3042-5999/2/1/2) — 2026-01-15
  - Authors: n/a
  - Affiliations: not provided
  - Proposes a 6T-SRAM in-memory computing (IMC) macro in 180 nm CMOS using an R2R-ladder to perform analog compute near/within SRAM, targeting AI-sensor and eddy-current-testing inference workloads—an alternative to GPU offload for edge deployments where power/latency dominate.
  - GPU relevance: IMC can reduce DRAM traffic and energy for MAC-heavy kernels, but it trades off precision, programmability, and scaling; best viewed as a fixed-function/near-sensor complement rather than a general GPU replacement.
  - Matrix multiplication: the paper frames acceleration around MAC-style operations typical of GEMM/conv, but the compute is implemented as SRAM-resident analog accumulation (not a standard GPU-style tiled GEMM pipeline).
