# GPU Research Digest — 2025-12-31
Generated: 2025-12-31T14:08:29.532280510+00:00
Sources: 12
New items: 2
Already seen: 20
Window: last 90 days
Sources failed: 4

## arXiv: GPU OR CUDA
- No new items found in the last 90 days.

## arXiv: ML + GPU
- No new items found in the last 90 days.

## arXiv: Matrix Multiplication + GPU
- No new items found in the last 90 days.

## arXiv: AI/ML + GPU
- No new items found in the last 90 days.

## Search: last 90 days Hopper H100 tensor cores FP8 GEMM microarchitecture paper
- No new items found in the last 90 days.

## Search: last 90 days Blackwell B200 tensor core architecture GEMM throughput research
- No new items found in the last 90 days.

## Search: last 90 days CUTLASS 3.x FP8 GEMM kernel fusion epilogue visitor research (error)
- Fetch error: web_search returned no in-window results (cutoff 2025-10-02)

## Search: last 90 days cuBLASLt new features FP8 BF16 GEMM autotuning release notes (error)
- Fetch error: web_search returned no in-window results (cutoff 2025-10-02)

## Search: last 90 days Triton compiler matmul performance improvements warp-specialization research (new: 1, showing: 1)
- [Enabling advanced GPU features in PyTorch – Warp Specialization](https://pytorch.org/blog/warp-specialization/) — 2025-12-28
  - Authors: n/a
  - Affiliations: not provided
  - Introduces Triton “warp specialization” in PyTorch: partitions a kernel into producer/consumer warps with pipelined execution, using TTGIR communication ops to pass data between warp groups and reduce stalls (e.g., overlap global-memory loads with compute).
  - Details the compiler path from Triton IR → TTGIR transforms (partitioning + comm ops) → LLVM/PTX materialization, enabling warp-group coordination without hand-written CUDA.
  - Reports ~10–15% speedups on real kernels, including FlashAttention and FP8 row-wise GEMM (matrix multiplication), by improving load/compute overlap and utilization.

## Search: last 90 days FlashAttention-3 paper GPU kernel matmul attention tensor cores (error)
- Fetch error: web_search returned no in-window results (cutoff 2025-10-02)

## Search: last 90 days systolic array vs GPU tensor cores matrix multiplication comparative study (error)
- Fetch error: web_search returned no in-window results (cutoff 2025-10-02)

## Search: last 90 days CUDA 12.6 PTX wgmma mma instruction GEMM optimization research (new: 1, showing: 1)
- [About wgmma.mma_async.sync.aligned.m64n256k16.f16.f16.f16 instruction's descriptors and byte offsets.](https://www.reddit.com/r/CUDA/comments/1px5pj6/about_wgmmamma/) — 2025-12-27
  - Authors: n/a
  - Affiliations: not provided
  - **WGMMA uses shared-memory “descriptors” to describe layout, not just shape:** even with a fixed compute tile (e.g., m64n256k16), the instruction must know where the tile starts in shared memory (leading byte offset) and how to advance between rows/segments (stride byte offset) to support arbitrary placements, padding, and swizzled/packed layouts.
  - **Offsets/strides enable flexible tiling and pipelining:** they let kernels double-buffer, stage multiple tiles, and handle non-contiguous or padded shared-memory tiles without extra address arithmetic in the inner loop—critical for high-throughput MMA pipelines.
  - **Practical implication for GEMM kernels:** correct descriptor byte offsets/strides are essential to map global→shared→WGMMA loads efficiently (avoid bank conflicts/misalignment) and to support edge
