# GPU Research Digest — 2026-02-25
Generated: 2026-02-25T14:26:01.290859444+00:00
Sources: 12
New items: 4
Already seen: 22
Window: last 90 days
Sources failed: 2

## arXiv: GPU OR CUDA
- No new items found in the last 90 days.

## arXiv: ML + GPU
- No new items found in the last 90 days.

## arXiv: Matrix Multiplication + GPU
- No new items found in the last 90 days.

## arXiv: AI/ML + GPU
- No new items found in the last 90 days.

## Search: last 90 days Hopper H100 tensor cores GEMM microarchitecture paper (error)
- Fetch error: web_search returned no in-window results (cutoff 2025-11-27)

## Search: last 90 days Blackwell B200 GB200 tensor core matmul performance analysis (new: 1, showing: 1)
- [Optimizing DeepSeek-V3.2 on NVIDIA Blackwell GPUs — TensorRT LLM](https://nvidia.github.io/TensorRT-LLM/1.2.0rc8/blogs/tech_blog/blog15_Optimizing_DeepSeek_V32_on_NVIDIA_Blackwell_GPUs.html) — 2026-01-11
  - Authors: n/a
  - Affiliations: not provided
  - Details TensorRT‑LLM inference optimizations for DeepSeek‑V3.2 on NVIDIA Blackwell (B200/GB200), with practitioner-focused performance analysis and a reproducible benchmark methodology.
  - Emphasizes GPU-side throughput/latency tuning via TensorRT‑LLM kernel selection and scheduling for transformer workloads dominated by GEMM/matrix multiplications (attention + MLP), highlighting how Blackwell features are leveraged.
  - Includes page metadata (last updated date) alongside results, useful for tracking changes in kernels/benchmarks over time.

## Search: last 90 days CUTLASS 3.x new GEMM kernels SM90 SM100 tutorial (error)
- Fetch error: OpenAI response missing output text for web_search: {
  "id": "resp_06b1cb23d847835300699f062d30f081939367fe68a3ec5195",
  "object": "response",
  "created_at": 1772029485,
  "status": "incomplete",
  "background": false,
  "billing": {
    "payer": "developer"
  },
  "completed_at": null,
  "error": null,
  "frequency_penalty": 0.0,
  "incomplete_details": {
    "reason": "max_output_tokens"
  },
  "instructions": null,
  "max_output_tokens": 500,
  "max_tool_calls": null,
  "model": "gpt-5.2-2025-12-11",
  "output": [
    {
      "id": "ws_06b1cb23d847835300699f062da9288193a905c2fbbc885fc7",
      "type": "web_search_call",
      "status": "completed",
      "action": {
        "type": "search",
        "queries": [
          "CUTLASS 3.x new GEMM kernels SM90 SM100 tutorial"
        ],
        "query": "CUTLASS 3.x new GEMM kernels SM90 SM100 tutorial"
      }
    },
    {
      "id": "ws_06b1cb23d847835300699f062eb56881939ab611f006aeb617",
      "type": "web_search_call",
      "status": "completed",
      "action": {
        "type"...

## Search: last 90 days cuBLASLt GEMM autotuning heuristics SM90 SM100 blog (new: 1, showing: 1)
- [GEMM Heuristics — NVIDIA CUTLASS Documentation](https://docs.nvidia.com/cutlass/4.3.5/media/docs/cpp/heuristics.html) — 2026-01-01
  - Authors: n/a
  - Affiliations: not provided
  - CUTLASS’s `cutlass_library` can shrink GEMM autotuning by querying NVIDIA’s `nvidia-matmul-heuristics`, which predicts a small set of high-performing kernel configs (tile shapes, pipeline stages, etc.) instead of exhaustively benchmarking many candidates at runtime.
  - The heuristics flow targets modern NVIDIA architectures (Hopper sm9x and Blackwell sm10x), helping practitioners pick near-optimal matmul kernels quickly for given problem sizes/datatypes/layouts.
  - Documentation includes an sm90 (Hopper) example showing how to integrate the heuristic recommendations into CUTLASS GEMM selection to reduce tuning overhead while retaining strong performance.

## Search: last 90 days Triton compiler matmul kernel fusion persistent kernels research (new: 1, showing: 1)
- [Hexagon-MLIR: An AI Compilation Stack For Qualcomm's Neural Processing Units (NPUs)](https://arxiv.org/abs/2602.19762) — 2026-02-23
  - Authors: n/a
  - Affiliations: not provided
  - MLIR-based, open-source compilation stack for Qualcomm Hexagon NPUs that unifies lowering of both Triton kernels and PyTorch models—relevant to GPU practitioners as a Triton/MLIR-style path to portable, accelerator-specific codegen.
  - Emphasizes “mega-kernel” generation (fusing ops into large kernels) to improve on-chip locality and cut external memory bandwidth—analogous to GPU fusion strategies for GEMM-adjacent workloads (e.g., GEMM+epilogue/attention blocks).
  - Matrix-multiply relevance is primarily indirect: the stack targets the same performance levers as GPU GEMM (tiling, data movement, fusion), aiming to keep matmul-heavy models bandwidth-efficient via aggressive lowering and fusion.

## Search: last 90 days FlashAttention-3 CUDA kernel matmul attention tensor cores SM90
- No new items found in the last 90 days.

## Search: last 90 days systolic array vs tensor cores matrix multiplication GPU architecture study (new: 1, showing: 1)
- [Hardware Acceleration for Neural Networks: A Comprehensive Survey](https://arxiv.org/abs/2512.23914) — 2025-12-30
  - Authors: n/a
  - Affiliations: not provided
  - Surveys GPU-centric deep learning acceleration, emphasizing how modern GPU SMs plus tensor-core/matrix-multiply units drive throughput for GEMM/conv via mixed precision (e.g., FP16/BF16/INT8) and high data reuse; highlights the roofline tension between compute and HBM bandwidth.
  - Summarizes optimization levers most relevant to GPU matmul: tiling/blocking into shared memory, warp-level MMA scheduling, operand layout/transforms to maximize tensor-core utilization, fusion to reduce memory traffic, and quantization/precision scaling to increase effective FLOP/s.
  - Contrasts GPUs with systolic arrays/ASIC accelerators mainly through dataflow (output-/weight-/row-stationary) and on-chip SRAM sizing; maps these ideas back to GPU kernel design choices (tile shapes, reuse patterns, and minimizing global memory reads/writes).

## Search: last 90 days GPU compiler MLIR LLVM CUDA matmul codegen tensor core research
- No new items found in the last 90 days.
