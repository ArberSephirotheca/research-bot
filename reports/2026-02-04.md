# GPU Research Digest — 2026-02-04
Generated: 2026-02-04T14:22:09.849934080+00:00
Sources: 12
New items: 2
Already seen: 15
Window: last 90 days
Sources failed: 5

## arXiv: GPU OR CUDA
- No new items found in the last 90 days.

## arXiv: ML + GPU
- No new items found in the last 90 days.

## arXiv: Matrix Multiplication + GPU
- No new items found in the last 90 days.

## arXiv: AI/ML + GPU
- No new items found in the last 90 days.

## Search: last 90 days Hopper H100 tensor cores FP8 GEMM microarchitecture paper (error)
- Fetch error: web_search returned no in-window results (cutoff 2025-11-06)

## Search: last 90 days Blackwell B200 tensor core MMA instruction set GEMM performance analysis
- No new items found in the last 90 days.

## Search: last 90 days CUTLASS 3.x new kernels FP8 GEMM epilogue fusion blog (error)
- Fetch error: web_search returned no in-window results (cutoff 2025-11-06)

## Search: last 90 days cuBLASLt FP8 GEMM autotuning heuristics performance regression analysis (error)
- Fetch error: web_search returned no in-window results (cutoff 2025-11-06)

## Search: last 90 days Triton compiler matmul kernel scheduling shared memory swizzle research (new: 1, showing: 1)
- [Advancing GPU Programming with the CUDA Tile IR Backend for OpenAI Triton](https://developer.nvidia.com/blog/advancing-gpu-programming-with-the-cuda-tile-ir-backend-for-openai-triton/) — 2026-01-30
  - Authors: n/a
  - Affiliations: not provided
  - Introduces a new Triton backend that lowers Triton kernels through an MLIR pipeline into NVIDIA’s CUDA Tile IR, targeting tiled GPU code generation (shared-memory tiling, vectorized loads/stores, and warp-level scheduling) to improve performance portability and compiler-driven optimization.
  - Emphasizes tiled-kernel tuning considerations—tile sizes, pipeline stages, shared-memory usage, and occupancy/latency trade-offs—especially relevant for GEMM/matrix-multiplication-style workloads where tile shapes and memory layout dominate throughput.
  - Discusses current performance characteristics and limitations of the Triton→TileIR path (e.g., feature coverage and cases where hand-tuned CUDA/Triton may still win), providing guidance on when the backend helps and what kernel patterns map best to Tile IR.

## Search: last 90 days FlashAttention-3 GPU kernel matmul softmax fusion tensor cores benchmark (new: 1, showing: 1)
- [The $100 Billion Bottleneck: Why Kernel Fusion is the Secret to Scaling AI](https://medium.com/%40emmanuelalo52/the-100-billion-bottleneck-why-kernel-fusion-is-the-secret-to-scaling-ai-2cd62ae04107) — 2026-01-01
  - Authors: n/a
  - Affiliations: not provided
  - Kernel fusion removes expensive intermediate reads/writes to global memory between ops (e.g., matmul → bias/activation/softmax), increasing arithmetic intensity and improving end-to-end throughput—often more than optimizing the GEMM alone.
  - Fusing “online softmax + matmul” (compute softmax statistics on-the-fly while producing the next matmul output) reduces memory traffic and kernel-launch overhead, which is especially impactful for attention-like patterns where softmax sits between matrix multiplications.
  - Benchmarks on a GTX 1650 show fused implementations outperforming separate kernels, highlighting that on bandwidth/launch-limited GPUs, reducing passes over matrices can dominate performance gains.

## Search: last 90 days systolic array vs tensor core GEMM mapping GPU architecture research (error)
- Fetch error: web_search returned no in-window results (cutoff 2025-11-06)

## Search: last 90 days GPU compiler kernel fusion matmul epilogue fusion ML training throughput paper (error)
- Fetch error: web_search returned no in-window results (cutoff 2025-11-06)
