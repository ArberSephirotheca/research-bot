# GPU Research Digest — 2026-01-07
Generated: 2026-01-07T14:09:28.788007611+00:00
Sources: 12
New items: 2
Already seen: 17
Window: last 90 days
Sources failed: 6

## arXiv: GPU OR CUDA
- No new items found in the last 90 days.

## arXiv: ML + GPU
- No new items found in the last 90 days.

## arXiv: Matrix Multiplication + GPU
- No new items found in the last 90 days.

## arXiv: AI/ML + GPU
- No new items found in the last 90 days.

## Search: last 90 days Hopper H100 tensor cores FP8 GEMM microarchitecture paper (new: 1, showing: 1)
- [H100 has 4.6x A100 Performance in TensorRT LLM, achieving 10,000 tok/s at 100ms to first token — TensorRT LLM](https://nvidia.github.io/TensorRT-LLM/blogs/H100vsA100.html) — 2025-12-17
  - Authors: n/a
  - Affiliations: not provided
  - H100 (Hopper) delivers ~4.6× higher TensorRT-LLM inference performance than A100 (Ampere) on GEMM-dominated LLM workloads, largely driven by faster Tensor Cores and improved utilization of large matrix multiplies.
  - FP8 is a key enabler: using FP8 Tensor Core GEMMs significantly boosts throughput versus higher-precision paths while maintaining practical LLM inference quality, making H100 especially strong for matmul-heavy decode/prefill.
  - Reported end-to-end serving point: up to ~10,000 tokens/s with ~100 ms time-to-first-token, indicating both higher matmul throughput and lower latency in the critical prefill stage compared to A100.

## Search: last 90 days Blackwell B200 tensor core matmul throughput details CUDA (error)
- Fetch error: web_search returned no in-window results (cutoff 2025-10-09)

## Search: last 90 days CUTLASS 3.x new GEMM kernels SM90 SM100 release notes (error)
- Fetch error: web_search returned no in-window results (cutoff 2025-10-09)

## Search: last 90 days cuBLASLt matmul epilogue fusion grouped GEMM performance update (error)
- Fetch error: web_search returned no in-window results (cutoff 2025-10-09)

## Search: last 90 days Triton compiler matmul autotuning persistent kernel research (new: 1, showing: 1)
- [Release Notes :: NVIDIA Deep Learning Triton Inference Server Documentation (Release 25.12)](https://docs.nvidia.com/deeplearning/triton-inference-server/release-notes/rel-25-12.html) — 2025-12-22
  - Authors: n/a
  - Affiliations: not provided
  - Triton Inference Server 25.12 release notes enumerate the GPU-facing container stack (CUDA/cuDNN/NCCL/TensorRT and related libs) and backend/component versions, which directly determine supported GPU architectures, driver/CUDA compatibility, and achievable inference throughput/latency.
  - Practical GPU impact: version bumps in TensorRT/CUDA libraries can change kernel selection, fusion behavior, and memory/communication performance (e.g., NCCL collectives), affecting multi-GPU scaling and end-to-end serving efficiency.
  - No matrix-multiplication–specific (GEMM) algorithm updates are described beyond whatever performance/behavior changes come indirectly from updated CUDA/TensorRT/cuBLAS-family components listed in the container contents.

## Search: last 90 days FlashAttention-3 CUDA kernel tensor core GEMM optimization (error)
- Fetch error: web_search returned no in-window results (cutoff 2025-10-09)

## Search: last 90 days GPU systolic array mapping matrix multiplication tensor cores research (error)
- Fetch error: web_search returned no in-window results (cutoff 2025-10-09)

## Search: last 90 days CUDA 12.4 12.5 WMMA MMA PTX matmul instruction changes (error)
- Fetch error: web_search returned no in-window results (cutoff 2025-10-09)
