# GPU Research Digest — 2026-02-22
Generated: 2026-02-22T14:11:35.048400452+00:00
Sources: 12
New items: 4
Already seen: 24
Window: last 90 days
Sources failed: 3

## arXiv: GPU OR CUDA
- No new items found in the last 90 days.

## arXiv: ML + GPU
- No new items found in the last 90 days.

## arXiv: Matrix Multiplication + GPU
- No new items found in the last 90 days.

## arXiv: AI/ML + GPU
- No new items found in the last 90 days.

## Search: last 90 days Hopper H100 tensor cores FP8 GEMM performance analysis CUTLASS cuBLASLt
- No new items found in the last 90 days.

## Search: last 90 days Blackwell B200 GB200 tensor core architecture matmul GEMM throughput whitepaper analysis (new: 2, showing: 2)
- [GPT-OSS Performance Optimizations on NVIDIA Blackwell: Pushing the Pareto Frontier | vLLM Blog](https://blog.vllm.ai/2026/02/01/gpt-oss-optimizations.html) — 2026-02-01
  - Authors: n/a
  - Affiliations: not provided
  - **Blackwell (B200/GB200) inference tuning in vLLM:** Practical deployment notes to improve **throughput/latency** on Blackwell, emphasizing kernel/backend choices and configuration knobs that move the Pareto frontier for LLM serving.
  - **Low-precision GEMM focus (FP8/FP4, incl. MoE):** Guidance on using **CUTLASS-based** paths for **FP8/FP4 matrix multiplications** (notably MoE), leveraging Blackwell Tensor Cores to increase tokens/s while controlling quality/accuracy tradeoffs.
  - **Kernel/backend selection matters:** Reported gains come largely from picking the right **matmul implementation** (CUTLASS vs alternatives) and precision mode per layer/expert, highlighting that **GEMM efficiency** is the dominant lever on Blackwell for these models.
- [Hierarchical Precision and Recursion for Accelerating Symmetric Linear Solves on MXUs](https://arxiv.org/abs/2601.08082) — 2026-01-12
  - Authors: n/a
  - Affiliations: not provided
  - Proposes a hierarchical mixed-precision scheme for symmetric linear solves that pushes most FLOPs into MXU-friendly GEMM/SYRK/TRSM kernels, using recursion to restructure the solve so Tensor Cores stay saturated while higher precision is applied only where needed for accuracy.
  - Emphasizes throughput-driven design: convert factor/solve steps into large, regular matrix-multiply–like updates (blocked/recursive SYRK/GEMM) to maximize arithmetic intensity and reduce memory-bound behavior—directly applicable to GPU kernel selection and tiling strategies.
  - Shows how “precision hierarchy” (low-precision compute with selective refinement/correction in higher precision) can deliver near-Tensor-Core peak performance for SPD/symmetric solves while maintaining numerical robustness, offering a template for integrating iterative refinement with MXU-accelerated GEMM pipelines.

## Search: last 90 days Triton 3.0 matmul kernel autotuning persistent kernels FP8 BF16 research
- No new items found in the last 90 days.

## Search: last 90 days FlashAttention-3 CUDA kernel fused attention tensor cores H100 benchmark (error)
- Fetch error: web_search returned no in-window results (cutoff 2025-11-24)

## Search: last 90 days CUTLASS 3.x grouped GEMM epilogue fusion SM90 SM100 tutorial research
- No new items found in the last 90 days.

## Search: last 90 days cuBLASLt matmul heuristics algorithm selection workspace tuning FP8 BF16 (new: 2, showing: 2)
- [Jet-RL: Enabling On-Policy FP8 Reinforcement Learning with Unified Training and Rollout Precision Flow](https://arxiv.org/abs/2601.14243) — 2026-01-20
  - Authors: n/a
  - Affiliations: not provided
  - Proposes a unified FP8 precision flow for **both on-policy RL training and rollout**, targeting matmul-dominated workloads (policy/value forward passes, backprop, and inference during environment interaction) to maximize Tensor Core utilization and reduce memory bandwidth/activation footprint versus BF16.
  - Analyzes **FP8 vs BF16 stability tradeoffs** in RL (sensitivity to noisy gradients/advantages and distribution shift during rollouts), and motivates selective higher-precision “guardrails” (e.g., keeping certain reductions/normalizations/accumulators in BF16/FP16) while leaving GEMMs in FP8.
  - Provides practical guidance for GPU practitioners on **where FP8 GEMMs are safe and where precision promotion is needed**, emphasizing matmul-heavy pipeline design (scaling/amax management, accumulation precision, and consistent precision between rollout and training to avoid mismatch
- [FP8-RL: A Practical and Stable Low-Precision Stack for LLM Reinforcement Learning](https://arxiv.org/abs/2601.18150) — 2026-01-26
  - Authors: n/a
  - Affiliations: not provided
  - Proposes an FP8 rollout stack for LLM RL that uses W8A8/FP8 GEMMs plus an FP8 KV-cache to cut memory bandwidth/footprint and increase tensor-core throughput during generation-heavy RL workloads.
  - Identifies and mitigates BF16↔FP8 numerical/scale mismatches that can destabilize RL (e.g., across GEMM boundaries and cache usage), emphasizing practical scaling/quantization handling to keep matmul outputs consistent.
  - Provides tuning/engineering guidance for deploying mixed BF16/FP8 GEMM pipelines in practice (kernel/config choices, where to keep BF16 vs FP8) to achieve stable training while retaining FP8 speedups.

## Search: last 90 days GPU compiler kernel fusion matmul epilogue scheduling MLIR LLVM NVCC research (error)
- Fetch error: web_search returned no in-window results (cutoff 2025-11-24)

## Search: last 90 days systolic array vs GPU tensor cores GEMM mapping dataflow comparison AI accelerator paper (error)
- Fetch error: web_search returned no in-window results (cutoff 2025-11-24)
