# GPU Research Digest — 2026-01-23
Generated: 2026-01-23T14:11:09.601055256+00:00
Sources: 12
New items: 1
Already seen: 19
Window: last 90 days
Sources failed: 5

## arXiv: GPU OR CUDA
- No new items found in the last 90 days.

## arXiv: ML + GPU
- No new items found in the last 90 days.

## arXiv: Matrix Multiplication + GPU
- No new items found in the last 90 days.

## arXiv: AI/ML + GPU
- No new items found in the last 90 days.

## Search: last 90 days Hopper H100 tensor cores FP8 GEMM microarchitecture paper
- No new items found in the last 90 days.

## Search: last 90 days Blackwell B200 tensor core GEMM performance analysis CUDA (error)
- Fetch error: web_search returned no in-window results (cutoff 2025-10-25)

## Search: last 90 days CUTLASS 3.x new GEMM kernels SM90 SM100 tutorial benchmark
- No new items found in the last 90 days.

## Search: last 90 days cuBLASLt GEMM autotuning heuristics FP8 BF16 performance regression (error)
- Fetch error: web_search returned no in-window results (cutoff 2025-10-25)

## Search: last 90 days Triton compiler matmul kernel fusion shared memory pipelining research (error)
- Fetch error: OpenAI response missing output text for web_search: {
  "id": "resp_074e8d0f33fdebdb00697381463e548195b01960a3b9fdcd71",
  "object": "response",
  "created_at": 1769177414,
  "status": "incomplete",
  "background": false,
  "billing": {
    "payer": "developer"
  },
  "completed_at": null,
  "error": null,
  "frequency_penalty": 0.0,
  "incomplete_details": {
    "reason": "max_output_tokens"
  },
  "instructions": null,
  "max_output_tokens": 500,
  "max_tool_calls": null,
  "model": "gpt-5.2-2025-12-11",
  "output": [
    {
      "id": "ws_074e8d0f33fdebdb0069738146f1f88195b76c1c5ba4ef1ee9",
      "type": "web_search_call",
      "status": "completed",
      "action": {
        "type": "search",
        "queries": [
          "Triton compiler matmul kernel fusion shared memory pipelining research"
        ],
        "query": "Triton compiler matmul kernel fusion shared memory pipelining research"
      }
    },
    {
      "id": "ws_074e8d0f33fdebdb0069738148044481958cfc184d4f6ec752",
      "type": "web_search_call",
      "status": "...

## Search: last 90 days FlashAttention 3 GPU kernel matmul softmax fusion tensor cores (error)
- Fetch error: web_search returned no in-window results (cutoff 2025-10-25)

## Search: last 90 days systolic array vs GPU tensor cores matrix multiplication roofline study (new: 1, showing: 1)
- [ML Systems Textbook — Hardware Acceleration (Systolic Arrays section)](https://www.mlsysbook.ai/contents/core/hw_acceleration/hw_acceleration) — 2025-11-24
  - Authors: n/a
  - Affiliations: not provided
  - Systolic arrays accelerate GEMM by mapping it to a 2D grid of MAC units with a fixed dataflow (e.g., output-/weight-/input-stationary), maximizing operand reuse via local neighbor-to-neighbor movement; this reduces expensive SRAM/DRAM traffic and improves energy efficiency—key context for why “dataflow” matters as much as peak FLOPs.
  - Compared to GPU tensor cores, systolic arrays are typically more rigid (compile-time scheduled data movement, predictable reuse), while tensor cores are specialized MMA units embedded in a general GPU that relies on the memory hierarchy (shared memory/L1/registers) and warp scheduling to approximate similar reuse patterns.
  - For practitioners, the takeaway is to reason about GEMM performance in terms of where A/B tiles live and how often they’re reused: systolic arrays bake reuse into the interconnect,

## Search: last 90 days GPU compiler MLIR LLVM CUDA matmul codegen tensor core MMA optimization (error)
- Fetch error: web_search returned no in-window results (cutoff 2025-10-25)
