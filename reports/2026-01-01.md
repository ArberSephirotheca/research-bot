# GPU Research Digest — 2026-01-01
Generated: 2026-01-01T14:08:09.778903577+00:00
Sources: 12
New items: 3
Already seen: 17
Window: last 90 days
Sources failed: 6

## arXiv: GPU OR CUDA
- No new items found in the last 90 days.

## arXiv: ML + GPU
- No new items found in the last 90 days.

## arXiv: Matrix Multiplication + GPU
- No new items found in the last 90 days.

## arXiv: AI/ML + GPU
- No new items found in the last 90 days.

## Search: last 90 days Hopper H100 tensor cores FP8 GEMM cuBLASLt performance paper (error)
- Fetch error: web_search returned no in-window results (cutoff 2025-10-03)

## Search: last 90 days Blackwell B200 tensor core architecture matrix multiplication throughput research (error)
- Fetch error: web_search returned no in-window results (cutoff 2025-10-03)

## Search: last 90 days CUTLASS 3.x new GEMM kernels SM90 SM100 tutorial blog (error)
- Fetch error: web_search returned no in-window results (cutoff 2025-10-03)

## Search: last 90 days Triton compiler matmul kernel fusion autotuning research preprint (new: 1, showing: 1)
- [TritonForge: Profiling-Guided Framework for Automated Triton Kernel Optimization](https://arxiv.org/abs/2512.09196) — 2025-12-09
  - Authors: n/a
  - Affiliations: not provided
  - Profiling-guided auto-optimizer for Triton: iteratively analyzes kernel code, runs GPU profiling, and applies targeted transformations (e.g., tiling/block sizes, memory access/layout, pipelining/unrolling, fusion) to improve throughput without manual tuning.
  - Demonstrates consistent speedups across multiple Triton kernels and GPU architectures, indicating portability of the optimization loop and usefulness for practitioners maintaining kernels across generations.
  - Applicable to GEMM/matmul-style workloads by automatically searching/adjusting matmul-relevant parameters (tile shapes, shared-memory/register usage, vectorization) based on measured bottlenecks rather than heuristics.

## Search: last 90 days FlashAttention-3 GPU kernel matmul attention tensor cores benchmark (error)
- Fetch error: web_search returned no in-window results (cutoff 2025-10-03)

## Search: last 90 days CUDA 12.4 12.5 wgmma warp-group MMA GEMM examples SM90 (error)
- Fetch error: OpenAI response missing output text for web_search: {
  "id": "resp_0cdb485a9a458aef0069567f9b381881a28a7a4823469f1561",
  "object": "response",
  "created_at": 1767276443,
  "status": "completed",
  "background": false,
  "billing": {
    "payer": "developer"
  },
  "completed_at": 1767276455,
  "error": null,
  "incomplete_details": null,
  "instructions": null,
  "max_output_tokens": 500,
  "max_tool_calls": null,
  "model": "gpt-5.2-2025-12-11",
  "output": [
    {
      "id": "ws_0cdb485a9a458aef0069567f9b88e081a28a28d565e686cae2",
      "type": "web_search_call",
      "status": "completed",
      "action": {
        "type": "search",
        "query": "CUDA 12.4 12.5 wgmma warp-group MMA GEMM example SM90"
      }
    },
    {
      "id": "ws_0cdb485a9a458aef0069567f9d66fc81a29abdf356ecab3c46",
      "type": "web_search_call",
      "status": "completed",
      "action": {
        "type": "open_page"
      }
    },
    {
      "id": "ws_0cdb485a9a458aef0069567f9e588881a2a5ec409c1d7664da",
      "type": "web_search_call",
      "st...

## Search: last 90 days systolic array vs GPU tensor cores matrix multiplication roofline analysis (error)
- Fetch error: web_search returned no in-window results (cutoff 2025-10-03)

## Search: last 90 days GPU compiler MLIR LLVM NVPTX matmul codegen tensor core lowering research (new: 2, showing: 2)
- [ML-Triton: Multi-Level GPU Compiler](https://www.emergentmind.com/topics/ml-triton) — 2025-12-25
  - Authors: n/a
  - Affiliations: not provided
  - Presents **ML-Triton**, a **multi-level lowering pipeline** for Triton GPU kernels that progressively refines high-level tensor ops (notably **GEMM/matmul**) into explicit GPU execution structure (tiling, memory layout, and parallel mapping).
  - Describes staged lowering that maps computation through **block → warp → SIMD/lane** granularities, enabling warp-level specialization (e.g., shared-memory staging, vectorization, and coalesced/global-memory access patterns) critical for high-throughput matmul.
  - Final codegen lowers to **LLVM/NVVM intrinsics**, bridging compiler IR decisions (e.g., warp-level ops) to hardware-specific instructions for NVIDIA GPUs.
- [nncase: An End-to-End Compiler for Efficient LLM Deployment on Heterogeneous Storage Architectures](https://arxiv.org/abs/2512.21571) — 2025-12-25
  - Authors: n/a
  - Affiliations: not provided
  - End-to-end LLM compiler pipeline using e-graph rewriting plus buffer-/memory-aware codegen to improve operator fusion, layout choices, and scheduling—relevant for GPU backends where memory traffic and kernel launch overhead dominate.
  - Targets heterogeneous storage hierarchies (e.g., multiple memory tiers), emphasizing explicit buffer planning and data movement minimization; maps well to GPU global/shared/register considerations even if not NVPTX-specific.
  - Matrix multiplication relevance: focuses on optimizing LLM compute graphs where GEMM/attention are central, but contributions are primarily in graph/codegen and buffer management rather than proposing a new GPU GEMM microkernel.
