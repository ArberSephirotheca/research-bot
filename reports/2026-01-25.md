# GPU Research Digest — 2026-01-25
Generated: 2026-01-25T14:08:04.163321517+00:00
Sources: 12
New items: 2
Already seen: 15
Window: last 90 days
Sources failed: 3

## arXiv: GPU OR CUDA
- No new items found in the last 90 days.

## arXiv: ML + GPU
- No new items found in the last 90 days.

## arXiv: Matrix Multiplication + GPU
- No new items found in the last 90 days.

## arXiv: AI/ML + GPU
- No new items found in the last 90 days.

## Search: last 90 days Hopper H100 tensor cores FP8 GEMM microarchitecture paper (error)
- Fetch error: web_search returned no in-window results (cutoff 2025-10-27)

## Search: last 90 days Blackwell B200 GB200 tensor core matmul performance analysis CUDA (error)
- Fetch error: web_search returned no in-window results (cutoff 2025-10-27)

## Search: last 90 days CUTLASS 3.x new GEMM kernels FP8 BF16 SM90 SM100 blog (new: 1, showing: 1)
- [Changelog — NVIDIA CUTLASS Documentation (CUTLASS 4.3.2)](https://docs.nvidia.com/cutlass/4.3.2/CHANGELOG.html) — 2025-12-05
  - Authors: n/a
  - Affiliations: not provided
  - Adds/expands Hopper (SM90) and Blackwell (SM100) CUTLASS support, including new/updated GEMM kernels and epilogue paths aimed at higher throughput and better utilization of Tensor Cores.
  - Improves GEMM kernel selection/coverage (more tile shapes, data types, and layouts) and refines library defaults to reduce performance cliffs across common matrix sizes.
  - Includes assorted kernel/library fixes and performance tuning relevant to matmul-heavy workloads (e.g., better scheduling/pipelining and correctness/edge-case fixes).

## Search: last 90 days cuBLASLt FP8 GEMM autotuning heuristics performance regression study
- No new items found in the last 90 days.

## Search: last 90 days Triton compiler matmul kernel fusion shared memory pipelining research
- No new items found in the last 90 days.

## Search: last 90 days FlashAttention-3 CUDA kernel matmul attention IO-aware tiling tensor cores (new: 1, showing: 1)
- [FlashAttention | LLM Inference Handbook](https://bentoml.com/llm/inference-optimization/flashattention) — 2026-01-22
  - Authors: n/a
  - Affiliations: not provided
  - FlashAttention (v1/v2) makes attention GPU-efficient by **IO-aware tiling**: compute attention in on-chip SRAM (shared memory/registers) in blocks to avoid materializing the full \(N \times N\) attention matrix, drastically reducing **HBM reads/writes** and making performance closer to compute-bound.
  - The core operation is structured around **GEMM-like kernels** (QKᵀ and PV) fused with softmax/rescaling, using tiling and fusion to maximize **data reuse** and minimize intermediate memory traffic.
  - FlashAttention-3 targets **Hopper**: better mapping to **Tensor Cores** (BF16/FP8) and newer GPU features to raise utilization/throughput, improving attention latency/throughput especially at long sequence lengths.

## Search: last 90 days GPU systolic array mapping GEMM tensor core MMA scheduling paper
- No new items found in the last 90 days.

## Search: last 90 days MLIR LLVM NVPTX GPU compiler matmul codegen tensor core MMA research (error)
- Fetch error: web_search returned no in-window results (cutoff 2025-10-27)
