# GPU Research Digest — 2026-02-23
Generated: 2026-02-23T14:24:38.890039619+00:00
Sources: 12
New items: 2
Already seen: 24
Window: last 90 days
Sources failed: 2

## arXiv: GPU OR CUDA
- No new items found in the last 90 days.

## arXiv: ML + GPU
- No new items found in the last 90 days.

## arXiv: Matrix Multiplication + GPU
- No new items found in the last 90 days.

## arXiv: AI/ML + GPU
- No new items found in the last 90 days.

## Search: last 90 days Hopper H100 tensor cores FP8 GEMM microarchitecture paper
- No new items found in the last 90 days.

## Search: last 90 days Blackwell B200 tensor core MMA instruction set GEMM performance analysis
- No new items found in the last 90 days.

## Search: last 90 days CUTLASS 3.x new GEMM kernels SM90 SM100 tutorial blog
- No new items found in the last 90 days.

## Search: last 90 days cuBLASLt GEMM autotuning heuristics FP8 BF16 performance regression study
- No new items found in the last 90 days.

## Search: last 90 days Triton compiler matmul kernel fusion pipelining shared memory swizzle research
- No new items found in the last 90 days.

## Search: last 90 days FlashAttention-3 GPU kernel matmul softmax fused attention SM90 benchmarks (error)
- Fetch error: web_search returned no in-window results (cutoff 2025-11-25)

## Search: last 90 days systolic array vs tensor core GPU matrix multiplication mapping research preprint (new: 2, showing: 2)
- [Tensor CUR Decomposition under the Linear-Map-Based Tensor-Tensor Multiplication](https://arxiv.org/abs/2602.09539) — 2026-02-10
  - Authors: n/a
  - Affiliations: not provided
  - Defines a **tensor CUR** factorization under a **linear-map-based tensor–tensor multiplication** (a generalization of the t-product), which reduces core computation to **batched/blocked matrix multiplications** in a transform domain—well-suited to GPU acceleration via GEMM-heavy kernels.
  - CUR-style **column/row (fiber/slice) sampling** yields a low-rank approximation that can cut compute/memory versus full tensor SVD/t-product methods; GPU implementation benefits from **efficient sampling + gather/scatter** plus large GEMM workloads for reconstruction.
  - Demonstrates applicability (e.g., **video foreground–background separation**) where the pipeline maps naturally to GPUs: transform/linear-map application + **many small-to-medium GEMMs** and parallelizable per-slice operations.
- [Exploiting the Structure in Tensor Decompositions for Matrix Multiplication](https://arxiv.org/abs/2602.11041) — 2026-02-11
  - Authors: n/a
  - Affiliations: not provided
  - Uses structured tensor decompositions to derive a new fast algorithm for \(6\times 6\) matrix multiplication, improving the arithmetic exponent vs prior best—relevant as a building block for theoretical/blocked GEMM variants.
  - GPU impact is indirect: the method reduces scalar multiply/add counts but introduces more complex data movement and scheduling; practical speedups would depend on mapping the decomposition to fused kernels with high reuse and minimal extra memory traffic.
  - Suggests opportunities for specialized small-matrix (microkernel) implementations where fixed-size \(6\times 6\) blocks can exploit the structure, but benefits must be weighed against increased kernel complexity and potential loss of tensor-core-friendly regularity.

## Search: last 90 days GPU compiler scheduling for GEMM warp-specialization TMA cp.async research (error)
- Fetch error: web_search returned no in-window results (cutoff 2025-11-25)
