# GPU Research Digest — 2025-12-29
Generated: 2025-12-29T14:09:23.179027985+00:00
Sources: 12
New items: 1
Already seen: 22
Window: last 90 days
Sources failed: 2

## arXiv: GPU OR CUDA
- No new items found in the last 90 days.

## arXiv: ML + GPU
- No new items found in the last 90 days.

## arXiv: Matrix Multiplication + GPU
- No new items found in the last 90 days.

## arXiv: AI/ML + GPU
- No new items found in the last 90 days.

## Search: last 90 days Hopper H100 tensor cores FP8 GEMM microarchitecture paper
- No new items found in the last 90 days.

## Search: last 90 days Blackwell B200 tensor core GEMM performance analysis CUDA
- No new items found in the last 90 days.

## Search: last 90 days CUTLASS 3.x grouped GEMM epilogue fusion research blog (error)
- Fetch error: web_search returned no in-window results (cutoff 2025-09-30)

## Search: last 90 days cuBLASLt matmul autotuning heuristics tensor core kernels
- No new items found in the last 90 days.

## Search: last 90 days Triton compiler matmul kernel fusion shared memory pipelining
- No new items found in the last 90 days.

## Search: last 90 days FlashAttention-3 GPU kernel matmul softmax tensor cores benchmark (new: 1, showing: 1)
- [HadaCore: Tensor Core Accelerated Hadamard Transform Kernel – PyTorch](https://pytorch.org/blog/hadacore/) — 2025-12-23
  - Authors: n/a
  - Affiliations: not provided
  - Presents **HadaCore**, a custom CUDA kernel for the **Hadamard transform** that maps the operation onto **NVIDIA Tensor Cores** (WMMA-style MMA usage) to outperform conventional CUDA implementations on **A100/H100**, with benchmarks showing substantial speedups for common transform sizes used in attention/inference paths.
  - Emphasizes **Tensor Core–friendly tiling/layout and fusion opportunities** (treating Hadamard as structured matrix ops rather than elementwise), improving throughput and reducing memory traffic—relevant when Hadamard appears alongside **GEMM-heavy** workloads (e.g., attention blocks).
  - Discusses practical deployment notes around **FP8 FlashAttention** in inference (accuracy/usage tradeoffs) and how fast Hadamard kernels can complement FP8/Tensor Core pipelines without becoming a bottleneck.

## Search: last 90 days systolic array vs GPU tensor core matrix multiplication comparative study (error)
- Fetch error: web_search returned no in-window results (cutoff 2025-09-30)

## Search: last 90 days GPU compiler MLIR LLVM CUDA matmul scheduling tensor core codegen
- No new items found in the last 90 days.
