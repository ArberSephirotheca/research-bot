# GPU Research Digest — 2026-01-20
Generated: 2026-01-20T14:14:01.240127885+00:00
Sources: 12
New items: 1
Already seen: 17
Window: last 90 days
Sources failed: 5

## arXiv: GPU OR CUDA
- No new items found in the last 90 days.

## arXiv: ML + GPU
- No new items found in the last 90 days.

## arXiv: Matrix Multiplication + GPU
- No new items found in the last 90 days.

## arXiv: AI/ML + GPU
- No new items found in the last 90 days.

## Search: last 90 days Hopper H100 tensor cores FP8 GEMM microarchitecture paper
- No new items found in the last 90 days.

## Search: last 90 days Blackwell B200 tensor core GEMM performance analysis CUDA (error)
- Fetch error: web_search returned no in-window results (cutoff 2025-10-22)

## Search: last 90 days CUTLASS 3.x new GEMM kernels SM90 SM100 tutorial benchmark
- No new items found in the last 90 days.

## Search: last 90 days cuBLASLt GEMM autotuning heuristics FP8 BF16 release notes (error)
- Fetch error: web_search returned no in-window results (cutoff 2025-10-22)

## Search: last 90 days Triton compiler matmul kernel fusion shared memory pipelining research (new: 1, showing: 1)
- [Generating Fast GPU Kernels without Programming in CUDA/Triton | by Zhihao Jia | Medium](https://zhihaojia.medium.com/generating-fast-gpu-kernels-without-programming-in-cuda-triton-3fdd4900d9bc) — 2026-01-16
  - Authors: n/a
  - Affiliations: not provided
  - Mirage auto-generates fused GPU kernels by applying algebraic transformations to combine ops (e.g., RMSNorm + MatMul) and eliminate intermediate global-memory reads/writes, targeting higher arithmetic intensity and better bandwidth utilization.
  - Emphasizes fusion around matrix multiplication: pushing elementwise/normalization work into the MatMul epilogue (or surrounding tiles) to reduce memory traffic and kernel launch overhead compared to hand-written CUDA/Triton pipelines.
  - Positions Mirage as a higher-level alternative to CUDA/Triton kernel programming, exploring more fusion opportunities automatically while still producing performance-oriented kernels for common LLM blocks dominated by MatMul.

## Search: last 90 days FlashAttention-3 GPU kernel matmul softmax fused attention H100 benchmark (error)
- Fetch error: web_search returned no in-window results (cutoff 2025-10-22)

## Search: last 90 days systolic array vs tensor core GPU matrix multiplication mapping study (error)
- Fetch error: web_search returned no in-window results (cutoff 2025-10-22)

## Search: last 90 days GPU compiler MLIR LLVM CUDA matmul scheduling tensor core codegen research (error)
- Fetch error: web_search returned no in-window results (cutoff 2025-10-22)
