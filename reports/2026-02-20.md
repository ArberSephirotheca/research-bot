# GPU Research Digest — 2026-02-20
Generated: 2026-02-20T14:21:01.876012078+00:00
Sources: 12
New items: 10
Already seen: 19
Window: last 90 days
Sources failed: 4

## arXiv: GPU OR CUDA (new: 1, showing: 1)
- [SoftDTW-CUDA-Torch: Memory-Efficient GPU-Accelerated Soft Dynamic Time Warping for PyTorch](https://arxiv.org/abs/2602.17206v1) — 2026-02-19
  - Authors: Ron Shapira Weber, Oren Freifeld
  - Affiliations: not provided
  - Removes common GPU SoftDTW bottlenecks via tiled anti-diagonal CUDA kernels: supports arbitrary sequence lengths (no 1024 cap) while maintaining efficient parallel wavefront execution.
  - Improves backward-pass robustness with a log-space formulation, avoiding overflow/instability for small smoothing parameters (γ), enabling reliable training on GPU.
  - Cuts GPU memory drastically by fusing pairwise distance computation into the DP kernel (no materialized \(O(BNM)\) distance tensor), reporting up to ~98% memory reduction; not primarily a GEMM/matmul-based approach.

## arXiv: ML + GPU
- No new items found in the last 90 days.

## arXiv: Matrix Multiplication + GPU
- No new items found in the last 90 days.

## arXiv: AI/ML + GPU (new: 1, showing: 1)
- [GPU-Accelerated Algorithms for Graph Vector Search: Taxonomy, Empirical Study, and Research Directions](https://arxiv.org/abs/2602.16719v1) — 2026-02-10
  - Authors: Yaowen Liu, Xuejia Chen, Anxin Tian, Haoyang Li, Qinbin Li, Xin Zhang, Alexander Zhou, Chen Jason Zhang, Qing Li, Lei Chen
  - Affiliations: not provided
  - Taxonomizes GPU optimization strategies for graph-based ANN (e.g., mapping distance eval, neighbor expansion, and pruning to GPU execution units) and empirically benchmarks 6 systems across 8 large datasets for both build and query.
  - Finds distance computation is the main on-GPU bottleneck; performance hinges on optimizing high-throughput similarity/distance kernels (often GEMM-like when batched) plus memory access patterns rather than graph logic alone.
  - Shows end-to-end latency at scale is frequently dominated by CPU↔GPU data transfer, highlighting system-level trade-offs (scalability vs. GPU memory footprint, batching/pipelining to amortize transfers).

## Search: last 90 days Hopper H100 tensor cores FP8 GEMM microarchitecture paper (error)
- Fetch error: web_search returned no in-window results (cutoff 2025-11-22)

## Search: last 90 days Blackwell B200 tensor core architecture FP4 FP8 matrix multiplication research (new: 2, showing: 2)
- [FP4 on DGX Spark — Why It Doesn't Scale Like You'd Expect - DGX Spark / GB10 - NVIDIA Developer Forums](https://forums.developer.nvidia.com/t/fp4-on-dgx-spark-why-it-doesnt-scale-like-youd-expect/360142) — 2026-02-09
  - Authors: n/a
  - Affiliations: not provided
  - DGX Spark (GB10) doesn’t scale FP4 GEMM throughput the way you’d extrapolate from Blackwell datacenter GPUs because it lacks key Blackwell tensor-core features (notably TMEM and some FP4-focused MMA instructions), so the “FP4 should be ~2× FP8” expectation often fails in practice.
  - As a result, FP4 matmul on GB10 is frequently limited by different bottlenecks (instruction availability/issue, data movement, packing/unpacking) rather than pure tensor-core math rate, making FP8 performance comparatively closer than expected.
  - Practical takeaway for practitioners: treat FP4 on DGX Spark as a distinct performance target—benchmark your GEMMs and kernels directly instead of assuming Blackwell FP4 scaling rules apply.
- [Driving vLLM WideEP and Large-Scale Serving Toward Maturity on Blackwell (Part I) | vLLM Blog](https://blog.vllm.ai/2026/02/03/dsr1-gb200-part1.html) — 2026-02-03
  - Authors: n/a
  - Affiliations: not provided
  - Details Blackwell/GB200 serving optimizations in vLLM WideEP centered on Tensor Core GEMMs, leveraging NVFP4 and FP8 matrix multiplication paths to increase throughput for large-scale inference.
  - Explains FP4 packed-weight formats plus per-block/per-tile scaling factors, and how kernels perform on-the-fly dequantization during GEMM to avoid materializing higher-precision weights in memory.
  - Highlights kernel-level considerations for Tensor-Core-optimized GEMM (data layout/packing, scaling application, and accumulation choices) to make low-precision matmul practical and accurate in production serving.

## Search: last 90 days CUTLASS 3.x new GEMM kernels SM90 SM100 performance analysis (error)
- Fetch error: web_search returned no in-window results (cutoff 2025-11-22)

## Search: last 90 days cuBLASLt GEMM autotuning heuristics epilogue fusion research (error)
- Fetch error: web_search returned no in-window results (cutoff 2025-11-22)

## Search: last 90 days Triton compiler matmul kernel scheduling shared memory pipelining research (new: 1, showing: 1)
- [GEMM Triton kernel - Ayoub's Blog](https://blog.ayghri.me/posts/ml/gpu/triton-kernels/) — 2026-02-06
  - Authors: n/a
  - Affiliations: not provided
  - Walks through implementing a GEMM/matmul in Triton, mapping program IDs to M/N tiles and using block sizes (e.g., `BLOCK_M/N/K`) to control tiling, data reuse, and memory coalescing for high throughput.
  - Explains key performance knobs: `num_warps` (parallelism/occupancy vs. register pressure) and `num_stages` (software pipelining to overlap global-memory loads with compute), and how they affect latency hiding.
  - Discusses shared-memory (scratchpad) usage patterns and Triton autotuning to search tile sizes/warps/stages for best performance across shapes and hardware.

## Search: last 90 days FlashAttention-3 CUDA kernel implementation tensor cores matmul attention research (new: 2, showing: 2)
- [From Buffers to Registers: Unlocking Fine-Grained FlashAttention with Hybrid-Bonded 3D NPU Co-Design](https://arxiv.org/abs/2602.11016) — 2026-02-11
  - Authors: n/a
  - Affiliations: not provided
  - Proposes a FlashAttention-style schedule for 3D-stacked accelerators that replaces SRAM-buffered tiles with fine-grained, register-to-register producer/consumer handoff (akin to warp-level pipelining), cutting on-chip SRAM traffic/energy and improving utilization—relevant to GPU kernel design where shared-memory pressure limits attention throughput.
  - Introduces “3D-Flow” scheduling that exploits vertical (die-to-die) bandwidth to stream Q/K/V and partial results at sub-tile granularity, reducing synchronization and enabling tighter fusion of attention steps; conceptually similar to pushing more of the attention pipeline into registers instead of shared memory.
  - Matrix-multiply relevance: attention’s core QKᵀ and PV GEMM-like phases are treated as tiled MMA pipelines, but optimized by minimizing intermediate writes (no SRAM round-trips) and keeping accumulators
- [Flash Attention Issues With ROCm Linux](https://www.reddit.com/r/ROCm/comments/1qt95qe/flash_attention_issues_with_rocm_linux/) — 2026-02-01
  - Authors: n/a
  - Affiliations: not provided
  - FlashAttention on AMD/ROCm Linux is evolving (incl. v3 support landing), with practical issues often tied to kernel selection/autotuning rather than pure correctness—important for attention workloads that are effectively specialized GEMM-like tiled matmul + softmax pipelines.
  - Autotuning behavior can significantly impact performance/compile time on ROCm; practitioners may need to manage tuning/caching to avoid slow first-run or suboptimal kernel picks for their specific GPU/sequence shapes.
  - Enabling Triton’s AMD backend may require specific environment variables/flags; getting these set correctly is key to unlocking optimized matmul/attention kernels on ROCm.

## Search: last 90 days GPU kernel fusion compiler passes for GEMM epilogue activation layernorm research (new: 3, showing: 3)
- [Fast and Fusiest: An Optimal Fusion-Aware Mapper for Accelerator Modeling and Evaluation](https://arxiv.org/abs/2602.15166) — 2026-02-16
  - Authors: n/a
  - Affiliations: not provided
  - Proposes **FFM (fusion-aware mapper)** that explores a **comprehensive fused mapspace** for tensor-algebra kernels, aiming to better model/choose mappings where multiple ops are fused—relevant to GPU kernel fusion and compiler/autotuning workflows.
  - Uses **pruning of suboptimal partial mappings** to keep the search tractable, enabling faster evaluation of fusion choices and tiling/scheduling options that impact GPU occupancy, memory traffic, and data reuse.
  - Applicable to **GEMM-like / matmul-centric workloads** common in DL pipelines, helping identify when and how to fuse surrounding ops (e.g., epilogues) for improved performance modeling and mapping selection.
- [Fusion Guide | Docs | RunMat](https://runmat.org/docs/fusion-guide) — 2026-02-06
  - Authors: n/a
  - Affiliations: not provided
  - Describes GPU fusion patterns to cut kernel launches and global memory traffic, emphasizing when/why to fuse adjacent ops in compute graphs.
  - Highlights matmul epilogue fusion (e.g., scale, bias, activation) so post-GEMM transforms run in the same kernel, improving bandwidth efficiency and latency.
  - Covers additional fusible graph kinds beyond matmul (common elementwise/reduction patterns) and practical constraints for safe/performant fusion on GPUs.
- [Layer Normalization — Triton documentation](https://triton-lang.org/main/getting-started/tutorials/05-layer-norm.html) — 2026-02-14
  - Authors: n/a
  - Affiliations: not provided
  - Walks through implementing LayerNorm forward and backward in Triton using custom GPU kernels, emphasizing fusion (mean/variance, affine transform, and gradient pieces) to reduce memory traffic and kernel launch overhead.
  - Details practical GPU kernel concerns—tiling over feature dimension, vectorized loads/stores, numerically stable reductions, and handling arbitrary shapes/strides—plus benchmarking against baseline implementations.
  - No matrix multiplication focus; the core computation is reduction-heavy normalization rather than GEMM, but the tutorial is directly relevant to designing other fused, bandwidth-bound normalization-style kernels.

## Search: last 90 days systolic array vs GPU tensor cores matrix multiplication throughput energy efficiency study (error)
- Fetch error: web_search returned no in-window results (cutoff 2025-11-22)
