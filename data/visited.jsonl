{"url":"https://arxiv.org/abs/2512.02551v2","title":"CUDA-L2: Surpassing cuBLAS Performance for Matrix Multiplication through Reinforcement Learning","source":"arXiv: GPU OR CUDA","published":"2025-12-02","fetched_at":"2025-12-22T22:05:35.528485173+00:00"}
{"url":"https://arxiv.org/abs/2510.02894v1","title":"PyRadiomics-cuda: a GPU-accelerated 3D features extraction from medical images within PyRadiomics","source":"arXiv: GPU OR CUDA","published":"2025-10-03","fetched_at":"2025-12-22T22:05:39.392452519+00:00"}
{"url":"https://arxiv.org/abs/2511.02132v1","title":"Optimizing Attention on GPUs by Exploiting GPU Architectural NUMA Effects","source":"arXiv: ML + GPU","published":"2025-11-03","fetched_at":"2025-12-22T22:05:44.044068358+00:00"}
{"url":"https://arxiv.org/abs/2512.07853v1","title":"GPU Memory Prediction for Multimodal Model Training","source":"arXiv: ML + GPU","published":"2025-11-26","fetched_at":"2025-12-22T22:05:47.749870378+00:00"}
{"url":"https://arxiv.org/abs/2512.09472v1","title":"WarmServe: Enabling One-for-Many GPU Prewarming for Multi-LLM Serving","source":"arXiv: ML + GPU","published":"2025-12-10","fetched_at":"2025-12-22T22:05:53.592265725+00:00"}
{"url":"https://arxiv.org/abs/2511.00025v1","title":"On the Structure of Floating-Point Noise in Batch-Invariant GPU Matrix Multiplication","source":"arXiv: Matrix Multiplication + GPU","published":"2025-10-26","fetched_at":"2025-12-22T22:05:57.892405132+00:00"}
{"url":"https://arxiv.org/abs/2512.04226v1","title":"tritonBLAS: Triton-based Analytical Approach for GEMM Kernel Parameter Selection","source":"arXiv: Matrix Multiplication + GPU","published":"2025-12-03","fetched_at":"2025-12-22T22:06:01.960172404+00:00"}
{"url":"https://arxiv.org/abs/2510.03760v1","title":"EvoEngineer: Mastering Automated CUDA Kernel Code Evolution with Large Language Models","source":"arXiv: AI/ML + GPU","published":"2025-10-04","fetched_at":"2025-12-22T22:06:06.124690032+00:00"}
{"url":"https://arxiv.org/abs/2510.19873v1","title":"From Large to Small: Transferring CUDA Optimization Expertise via Reasoning Graph","source":"arXiv: AI/ML + GPU","published":"2025-10-22","fetched_at":"2025-12-22T22:06:10.750339230+00:00"}
{"url":"https://arxiv.org/abs/2511.17594v1","title":"AutoSAGE: Input-Aware CUDA Scheduling for Sparse GNN Aggregation (SpMM/SDDMM) and CSR Attention","source":"arXiv: AI/ML + GPU","published":"2025-11-17","fetched_at":"2025-12-22T22:06:15.264630374+00:00"}
{"url":"https://arxiv.org/abs/2511.19493v1","title":"RFX: High-Performance Random Forests with GPU Acceleration and QLORA Compression","source":"arXiv: AI/ML + GPU","published":"2025-11-23","fetched_at":"2025-12-22T22:06:20.145315106+00:00"}
{"url":"https://arxiv.org/abs/2511.01884v2","title":"CudaForge: An Agent Framework with Hardware Feedback for CUDA Kernel Optimization","source":"arXiv: AI/ML + GPU","published":"2025-10-23","fetched_at":"2025-12-22T22:06:24.278273543+00:00"}
{"url":"https://arxiv.org/abs/2512.14080","title":"SonicMoE: Accelerating MoE with IO and Tile-aware Optimizations","source":"Search: last 90 days Hopper H100 tensor cores GEMM research paper warp-group MMA FP8","published":"2025-12-16","fetched_at":"2025-12-22T22:07:08.699785573+00:00"}
{"url":"https://arxiv.org/abs/2512.02189","title":"Microbenchmarking NVIDIA's Blackwell Architecture: An in-depth Architectural Analysis","source":"Search: last 90 days Blackwell B200 GB200 tensor core architecture matmul throughput analysis","published":"2025-12-01","fetched_at":"2025-12-22T22:07:20.836961190+00:00"}
{"url":"https://arxiv.org/abs/2512.02551","title":"CUDA-L2: Surpassing cuBLAS Performance for Matrix Multiplication through Reinforcement Learning","source":"Search: last 90 days cuBLASLt FP8 GEMM autotuning heuristics performance regression study","published":"2025-12-02","fetched_at":"2025-12-22T22:07:55.454977127+00:00"}
{"url":"https://arxiv.org/abs/2511.18674","title":"Low-Rank GEMM: Efficient Matrix Multiplication via Low-Rank Approximation with FP8 Acceleration","source":"Search: last 90 days cuBLASLt FP8 GEMM autotuning heuristics performance regression study","published":"2025-11-24","fetched_at":"2025-12-22T22:07:58.897279888+00:00"}
{"url":"https://arxiv.org/abs/2510.20271v1","title":"Scalable GPU-Accelerated Euler Characteristic Curves: Optimization and Differentiable Learning for PyTorch","source":"arXiv: AI/ML + GPU","published":"2025-10-23","fetched_at":"2025-12-24T19:05:43.034999706+00:00"}
{"url":"https://github.com/NVIDIA/cutlass/releases/tag/v4.3.4","title":"CUTLASS 4.3.4 (GitHub Release)","source":"Search: last 90 days CUTLASS 3.x new GEMM kernels SM90 SM100 warp-specialized epilogue fusion","published":"2025-12-24","fetched_at":"2025-12-24T19:06:05.202135101+00:00"}
{"url":"https://triton-lang.org/main/getting-started/tutorials/09-persistent-matmul.html","title":"Persistent Matmul — Triton documentation","source":"Search: last 90 days Triton compiler matmul autotuning persistent kernels FP16 BF16 FP8","published":"2025-12-20","fetched_at":"2025-12-24T19:06:18.461976927+00:00"}
{"url":"https://lubits.ch/flash/Part-3","title":"Flash Attention from Scratch Part 3: Kernel 1","source":"Search: last 90 days FlashAttention-3 CUDA kernel details tensor core MMA pipeline shared memory scheduling","published":"2025-12-21","fetched_at":"2025-12-24T19:06:31.896526743+00:00"}
{"url":"https://lubits.ch/flash/Appendix","title":"Flash Attention from Scratch: Appendix","source":"Search: last 90 days FlashAttention-3 CUDA kernel details tensor core MMA pipeline shared memory scheduling","published":"2025-12-24","fetched_at":"2025-12-24T19:06:35.876246634+00:00"}
{"url":"https://aman.ai/primers/ai/flashattention/","title":"Aman's AI Journal • Primers • FlashAttention","source":"Search: last 90 days FlashAttention-3 CUDA kernel details tensor core MMA pipeline shared memory scheduling","published":"2025-12-20","fetched_at":"2025-12-24T19:06:38.730732064+00:00"}
{"url":"https://aman.ai/primers/ai/model-acceleration/","title":"Aman's AI Journal • Primers • Model Acceleration","source":"Search: last 90 days FlashAttention-3 CUDA kernel details tensor core MMA pipeline shared memory scheduling","published":"2025-12-20","fetched_at":"2025-12-24T19:06:41.989098601+00:00"}
{"url":"https://arxiv.org/abs/2512.12949","title":"FlashFuser: Expanding the Scale of Kernel Fusion for Compute-Intensive Operators via Inter-Core Connection","source":"Search: last 90 days GPU compiler research kernel fusion matmul epilogue fusion ML training throughput","published":"2025-12-15","fetched_at":"2025-12-24T19:07:07.168003932+00:00"}
{"url":"https://arxiv.org/abs/2512.16465","title":"cuPilot: A Strategy-Coordinated Multi-agent Framework for CUDA Kernel Evolution","source":"Search: last 90 days GPU compiler research kernel fusion matmul epilogue fusion ML training throughput","published":"2025-12-18","fetched_at":"2025-12-24T19:07:10.142445174+00:00"}
{"url":"https://arxiv.org/abs/2512.04226","title":"tritonBLAS: Triton-based Analytical Approach for GEMM Kernel Parameter Selection","source":"Search: last 90 days GPU compiler research kernel fusion matmul epilogue fusion ML training throughput","published":"2025-12-03","fetched_at":"2025-12-24T19:07:12.713762080+00:00"}
{"url":"https://www.preprints.org/manuscript/202511.1093/v1","title":"NeuronMM: High-Performance Matrix Multiplicationfor LLM Inference on AWS Trainium (Version 1)","source":"Search: last 90 days AI accelerator research matrix multiplication dataflow tensor core alternatives training inference","published":"2025-11-14","fetched_at":"2025-12-24T19:07:22.934337986+00:00"}
{"url":"https://wavespeed.ai/blogs/index.php/2025/12/10/wavespeedai-x-datacrunch-flux-real-time-image-inference-on-b200/","title":"WaveSpeedAI X DataCrunch: FLUX Real-Time Image Inference on B200 - WaveSpeedAI Blog","source":"Search: last 90 days Blackwell B200 tensor core architecture matmul throughput research","published":"2025-12-10","fetched_at":"2025-12-25T14:06:55.345130868+00:00"}
{"url":"https://github.com/NVIDIA/cutlass/releases/tag/v4.3.3","title":"CUTLASS 4.3.3","source":"Search: last 90 days CUTLASS 3.x new GEMM kernels FP8 BF16 SM90 SM100 blog","published":"2025-12-12","fetched_at":"2025-12-25T14:07:08.169757589+00:00"}
{"url":"https://github.com/NVIDIA/cutlass/releases/tag/v4.3.2","title":"CUTLASS 4.3.2","source":"Search: last 90 days CUTLASS 3.x new GEMM kernels FP8 BF16 SM90 SM100 blog","published":"2025-12-05","fetched_at":"2025-12-25T14:07:11.916745387+00:00"}
{"url":"https://arxiv.org/abs/2512.02875","title":"SAT-MapIt: A SAT-based Modulo Scheduling Mapper for Coarse Grain Reconfigurable Architectures","source":"Search: last 90 days CUDA 12.5 12.6 WGMMA GMMA matmul instruction scheduling research","published":"2025-12-02","fetched_at":"2025-12-25T14:08:05.567147033+00:00"}
{"url":"https://docs.nvidia.com/cutlass/4.3.0/media/docs/cpp/gemm_api_3x.html","title":"CUTLASS 3.0 GEMM API — NVIDIA CUTLASS Documentation","source":"Search: last 90 days CUTLASS 3.x new GEMM kernels SM90 SM100 TMA warp-specialized tutorial","published":"2025-12-23","fetched_at":"2025-12-26T14:07:26.173472281+00:00"}
{"url":"https://pytorch.org/blog/cutlass-ping-pong-gemm-kernel/","title":"Deep Dive on CUTLASS Ping-Pong GEMM Kernel – PyTorch","source":"Search: last 90 days CUTLASS 3.x new GEMM kernels SM90 SM100 TMA warp-specialized tutorial","published":"2025-12-22","fetched_at":"2025-12-26T14:07:29.597488464+00:00"}
{"url":"https://docs.nvidia.com/cutlass/4.3.1/CHANGELOG.html","title":"Changelog — NVIDIA CUTLASS Documentation","source":"Search: last 90 days CUTLASS 3.x new GEMM kernels SM90 SM100 TMA warp-specialized tutorial","published":"2025-12-23","fetched_at":"2025-12-26T14:07:31.697878805+00:00"}
{"url":"https://arxiv.org/abs/2512.02371","title":"Pushing Tensor Accelerators Beyond MatMul in a User-Schedulable Language","source":"Search: last 90 days FlashAttention-3 GPU kernel implementation tensor cores matmul research","published":"2025-12-02","fetched_at":"2025-12-27T14:08:26.670819479+00:00"}
{"url":"https://arxiv.org/abs/2512.18134","title":"Optimal Software Pipelining and Warp Specialization for Tensor Core GPUs","source":"Search: last 90 days GPU systolic array mapping GEMM scheduling tensor core research","published":"2025-12-19","fetched_at":"2025-12-27T14:08:39.884584579+00:00"}
{"url":"https://arxiv.org/abs/2512.19250","title":"Small Language Models as Compiler Experts: Auto-Parallelization for Heterogeneous Systems","source":"Search: last 90 days MLIR LLVM CUDA GPU compiler matmul codegen tensor core research","published":"2025-12-22","fetched_at":"2025-12-27T14:08:54.061746347+00:00"}
{"url":"https://docs.nvidia.com/cuda/cublasmp/release_notes","title":"Release Notes — cuBLASMp (cuBLASMp v0.7.0 released November 24, 2025)","source":"Search: last 90 days cuBLASLt GEMM autotuning heuristics FP8 BF16 performance update","published":"2025-11-24","fetched_at":"2025-12-28T14:07:43.987863493+00:00"}
{"url":"https://arxiv.org/abs/2511.02302","title":"FP8-Flow-MoE: A Casting-Free FP8 Recipe without Double Quantization Error","source":"Search: last 90 days cuBLASLt GEMM autotuning heuristics FP8 BF16 performance update","published":"2025-11-04","fetched_at":"2025-12-28T14:07:48.743783276+00:00"}
{"url":"https://pytorch.org/blog/flashattention-3/","title":"FlashAttention-3: Fast and Accurate Attention with Asynchrony and Low-precision","source":"Search: last 90 days FlashAttention 3 CUDA kernel matmul attention tensor cores benchmark","published":"2025-12-23","fetched_at":"2025-12-28T14:08:15.748752901+00:00"}
{"url":"https://github.com/togethercomputer/flash-attention-3","title":"GitHub - togethercomputer/flash-attention-3: Fast and memory-efficient exact attention","source":"Search: last 90 days FlashAttention 3 CUDA kernel matmul attention tensor cores benchmark","published":"2025-12-07","fetched_at":"2025-12-28T14:08:20.017146772+00:00"}
{"url":"https://pytorch.org/blog/hadacore/","title":"HadaCore: Tensor Core Accelerated Hadamard Transform Kernel – PyTorch","source":"Search: last 90 days FlashAttention-3 GPU kernel matmul softmax tensor cores benchmark","published":"2025-12-23","fetched_at":"2025-12-29T14:09:02.878111626+00:00"}
{"url":"https://pytorch.org/blog/hopper-tma-unit/","title":"Deep Dive on the Hopper TMA Unit for FP8 GEMMs – PyTorch","source":"Search: last 90 days Hopper H200 tensor cores FP8 GEMM performance analysis CUTLASS cuBLAS","published":"2025-12-28","fetched_at":"2025-12-30T14:07:35.059318414+00:00"}
{"url":"https://www.emergentmind.com/topics/blackwell-gpu-architecture","title":"Blackwell GPU Architecture","source":"Search: last 90 days Hopper H200 tensor cores FP8 GEMM performance analysis CUTLASS cuBLAS","published":"2025-12-25","fetched_at":"2025-12-30T14:07:39.049097759+00:00"}
{"url":"https://arxiv.org/abs/2512.22219","title":"Mirage Persistent Kernel: A Compiler and Runtime for Mega-Kernelizing Tensor Programs","source":"Search: last 90 days GPU kernel fusion compiler MLIR CUDA matmul epilogue fusion research paper","published":"2025-12-22","fetched_at":"2025-12-30T14:09:11.658881207+00:00"}
{"url":"https://pytorch.org/blog/warp-specialization/","title":"Enabling advanced GPU features in PyTorch – Warp Specialization","source":"Search: last 90 days Triton compiler matmul performance improvements warp-specialization research","published":"2025-12-28","fetched_at":"2025-12-31T14:08:00.768698066+00:00"}
{"url":"https://www.reddit.com/r/CUDA/comments/1px5pj6/about_wgmmamma/","title":"About wgmma.mma_async.sync.aligned.m64n256k16.f16.f16.f16 instruction's descriptors and byte offsets.","source":"Search: last 90 days CUDA 12.6 PTX wgmma mma instruction GEMM optimization research","published":"2025-12-27","fetched_at":"2025-12-31T14:08:29.532262656+00:00"}
{"url":"https://arxiv.org/abs/2512.09196","title":"TritonForge: Profiling-Guided Framework for Automated Triton Kernel Optimization","source":"Search: last 90 days Triton compiler matmul kernel fusion autotuning research preprint","published":"2025-12-09","fetched_at":"2026-01-01T14:07:17.935723261+00:00"}
{"url":"https://www.emergentmind.com/topics/ml-triton","title":"ML-Triton: Multi-Level GPU Compiler","source":"Search: last 90 days GPU compiler MLIR LLVM NVPTX matmul codegen tensor core lowering research","published":"2025-12-25","fetched_at":"2026-01-01T14:08:06.542791786+00:00"}
{"url":"https://arxiv.org/abs/2512.21571","title":"nncase: An End-to-End Compiler for Efficient LLM Deployment on Heterogeneous Storage Architectures","source":"Search: last 90 days GPU compiler MLIR LLVM NVPTX matmul codegen tensor core lowering research","published":"2025-12-25","fetched_at":"2026-01-01T14:08:09.778887857+00:00"}
{"url":"https://www.emergentmind.com/papers/2512.02189","title":"Microbenchmarking NVIDIA's Blackwell Architecture: An in-depth Architectural Analysis","source":"Search: last 90 days NVIDIA Hopper H100 tensor cores GEMM research paper","published":"2025-12-01","fetched_at":"2026-01-02T14:07:00.750252307+00:00"}
{"url":"https://triton-lang.org/main/getting-started/tutorials/10-block-scaled-matmul.html","title":"Block Scaled Matrix Multiplication — Triton documentation","source":"Search: last 90 days Triton compiler matmul autotuning tensor core wgmma research","published":"2025-12-28","fetched_at":"2026-01-02T14:07:30.515300381+00:00"}
{"url":"https://arxiv.org/abs/2511.12653","title":"DPVO-QAT++: Heterogeneous QAT and CUDA Kernel Fusion for High-Performance Deep Patch Visual Odometry","source":"Search: last 90 days GPU kernel fusion compiler matmul epilogue fusion ML training research","published":"2025-11-16","fetched_at":"2026-01-02T14:08:12.580468235+00:00"}
{"url":"https://arxiv.org/abs/2512.13638","title":"Design in Tiles: Automating GEMM Deployment on Tile-Based Many-PE Accelerators","source":"Search: last 90 days Hopper H100 tensor cores FP8 GEMM research paper","published":"2025-12-15","fetched_at":"2026-01-03T14:06:47.322535017+00:00"}
{"url":"https://docs.nvidia.com/cutlass/latest/CHANGELOG.html","title":"CUTLASS 4.x Changelog — NVIDIA CUTLASS Documentation","source":"Search: last 90 days CUTLASS 3.x new GEMM kernels SM90 SM100 performance blog","published":"2025-12-12","fetched_at":"2026-01-03T14:07:13.081738999+00:00"}
{"url":"https://github.com/NVIDIA/cutlass/releases","title":"Releases · NVIDIA/cutlass · GitHub","source":"Search: last 90 days CUTLASS 3.x new GEMM kernels SM90 SM100 performance blog","published":"2025-12-24","fetched_at":"2026-01-03T14:07:16.411251463+00:00"}
{"url":"https://www.aimodels.fyi/papers/arxiv/optimal-software-pipelining-warp-specialization-tensor-core","title":"Optimal Software Pipelining and Warp Specialization for Tensor Core GPUs","source":"Search: last 90 days GPU systolic array vs SIMT tensor core matmul mapping research","published":"2025-12-23","fetched_at":"2026-01-03T14:07:49.797122306+00:00"}
{"url":"https://arxiv.org/abs/2511.13764","title":"Library Liberation: Competitive Performance Matmul Through Compiler-composed Nanokernels","source":"Search: last 90 days GPU compiler MLIR LLVM CUDA matmul scheduling tensor core codegen paper","published":"2025-11-14","fetched_at":"2026-01-03T14:08:07.219923233+00:00"}
{"url":"https://www.modular.com/blog/matrix-multiplication-on-nvidias-blackwell-part-2-using-hardware-features-to-optimize-matmul","title":"Modular: Matrix Multiplication on Blackwell: Part 2 - Using Hardware Features to Optimize Matmul","source":"Search: last 90 days GPU systolic array vs tensor cores matrix multiplication research","published":"2026-01-04","fetched_at":"2026-01-04T14:08:09.198990503+00:00"}
{"url":"https://docs.nvidia.com/cutlass/latest/media/docs/cpp/blackwell_functionality.html","title":"Blackwell SM100 GEMMs — NVIDIA CUTLASS Documentation","source":"Search: last 90 days GPU systolic array mapping GEMM tensor core MMA instruction scheduling research","published":"2026-01-02","fetched_at":"2026-01-05T14:10:11.976070985+00:00"}
{"url":"https://leimao.github.io/blog/NVIDIA-Tensor-Core-MMA-Instruction-TN-Layout/","title":"NVIDIA Tensor Core TN Layout MMA Instruction - Lei Mao's Log Book","source":"Search: last 90 days GPU systolic array mapping GEMM tensor core MMA instruction scheduling research","published":"2025-12-06","fetched_at":"2026-01-05T14:10:16.483764189+00:00"}
{"url":"https://docs.nvidia.com/cutlass/4.3.0/CHANGELOG.html","title":"Changelog — NVIDIA CUTLASS Documentation (CUTLASS 4.3.0)","source":"Search: last 90 days cuBLASLt GEMM autotuning heuristics epilogue fusion SM90 SM100","published":"2025-11-21","fetched_at":"2026-01-06T14:07:42.929625072+00:00"}
{"url":"https://docs.nvidia.com/cutlass/4.3.0/overview.html","title":"Overview — NVIDIA CUTLASS Documentation (CUTLASS 4.3.0)","source":"Search: last 90 days cuBLASLt GEMM autotuning heuristics epilogue fusion SM90 SM100","published":"2025-11-01","fetched_at":"2026-01-06T14:07:47.512551926+00:00"}
{"url":"https://nvidia.github.io/TensorRT-LLM/blogs/H100vsA100.html","title":"H100 has 4.6x A100 Performance in TensorRT LLM, achieving 10,000 tok/s at 100ms to first token — TensorRT LLM","source":"Search: last 90 days Hopper H100 tensor cores FP8 GEMM microarchitecture paper","published":"2025-12-17","fetched_at":"2026-01-07T14:08:10.408382042+00:00"}
{"url":"https://docs.nvidia.com/deeplearning/triton-inference-server/release-notes/rel-25-12.html","title":"Release Notes :: NVIDIA Deep Learning Triton Inference Server Documentation (Release 25.12)","source":"Search: last 90 days Triton compiler matmul autotuning persistent kernel research","published":"2025-12-22","fetched_at":"2026-01-07T14:09:03.802553753+00:00"}
{"url":"https://llmlaba.com/articles/flashattn-cuda-compatibility.html","title":"FlashAttention compatibility | [“LLM Laboratory”]","source":"Search: last 90 days FlashAttention-3 CUDA kernel matmul attention tensor cores SM90 benchmarks","published":"2025-12-25","fetched_at":"2026-01-08T14:11:02.182470490+00:00"}
{"url":"https://mlir.llvm.org/docs/Dialects/NVGPU/","title":"'nvgpu' Dialect - MLIR","source":"Search: last 90 days MLIR LLVM NVPTX CUDA matmul codegen tensor core MMA lowering research","published":"2026-01-04","fetched_at":"2026-01-08T14:11:32.774098031+00:00"}
{"url":"https://mlir.llvm.org/doxygen/namespacemlir_1_1nvgpu.html","title":"MLIR: mlir::nvgpu Namespace Reference","source":"Search: last 90 days MLIR LLVM NVPTX CUDA matmul codegen tensor core MMA lowering research","published":"2025-12-30","fetched_at":"2026-01-08T14:11:36.025811023+00:00"}
{"url":"https://mlir.llvm.org/python-bindings/autoapi/mlir/dialects/transform/nvgpu/index.html","title":"mlir.dialects.transform.nvgpu - MLIR Python bindings documentation","source":"Search: last 90 days MLIR LLVM NVPTX CUDA matmul codegen tensor core MMA lowering research","published":"2025-12-25","fetched_at":"2026-01-08T14:11:38.560621623+00:00"}
{"url":"https://github.com/NVIDIA/cutlass/releases/tag/v4.3.5","title":"CUTLASS 4.3.5 (GitHub Release)","source":"Search: last 90 days CUTLASS 3.x new GEMM kernels SM90 SM100 release notes","published":"2026-01-09","fetched_at":"2026-01-09T14:08:22.581881861+00:00"}
{"url":"https://blog.csdn.net/Together_CZ/article/details/143786789","title":"FlashAttention-3:Fast and Accurate Attention with Asynchrony and Low-precision——利用异步和低精度实现快速准确的注意力机制_flashattention3-CSDN博客","source":"Search: last 90 days FlashAttention-3 CUDA kernel implementation tensor core GEMM details","published":"2025-12-19","fetched_at":"2026-01-09T14:08:51.862493139+00:00"}
{"url":"https://www.reddit.com/r/comfyui/comments/1q29i5h/flashattention_283_fails_during_comfyui_sampling/","title":"FlashAttention 2.8.3 fails during ComfyUI sampling (Torch 2.9.1 + cu130, Windows 11, RTX 5090)","source":"Search: last 90 days FlashAttention 3 CUDA kernel implementation tensor cores matmul","published":"2026-01-02","fetched_at":"2026-01-11T14:07:30.281206588+00:00"}
{"url":"https://naddod.medium.com/introduction-to-tensor-cores-in-nvidia-gpus-ae2a79642733","title":"Introduction to Tensor Cores in NVIDIA GPUs","source":"Search: last 90 days systolic array vs GPU tensor cores matrix multiplication accelerator comparison","published":"2026-01-01","fetched_at":"2026-01-12T14:10:58.420314155+00:00"}
{"url":"https://www.linkedin.com/pulse/gputpu-architectures-sharada-yeluri-p5hwc","title":"GPU/TPU Architectures","source":"Search: last 90 days systolic array vs GPU tensor cores matrix multiplication accelerator comparison","published":"2025-12-30","fetched_at":"2026-01-12T14:11:04.480239032+00:00"}
{"url":"https://medium.com/@21338787/tensor-core-gpus-impact-on-performance-improvements-in-ai-2ca1597beccd","title":"Tensor core GPU’s impact on performance improvements in AI","source":"Search: last 90 days systolic array vs GPU tensor cores matrix multiplication accelerator comparison","published":"2025-12-28","fetched_at":"2026-01-12T14:11:08.131465854+00:00"}
{"url":"https://www.nvidia.com/en-us/data-center/technologies/hopper-architecture/","title":"Hopper GPU Architecture | NVIDIA","source":"Search: last 90 days Hopper H100 tensor cores FP8 GEMM microarchitecture paper","published":"2026-01-15","fetched_at":"2026-01-15T14:08:31.069971201+00:00"}
{"url":"https://tilelang.com/deeplearning_operators/matmul.html","title":"General Matrix-Matrix Multiplication with Tile Library - TileLang 0.1.7.post2 documentation","source":"Search: last 90 days Triton compiler matmul scheduling shared memory swizzle research","published":"2026-01-11","fetched_at":"2026-01-15T14:09:14.048446538+00:00"}
{"url":"https://arxiv.org/abs/2601.07048v2","title":"GPU-Accelerated ANNS: Quantized for Speed, Built for Change","source":"arXiv: AI/ML + GPU","published":"2026-01-11","fetched_at":"2026-01-16T14:07:46.129301709+00:00"}
{"url":"https://arxiv.org/abs/2512.07724","title":"The Native Spiking Microarchitecture: From Iontronic Primitives to Bit-Exact FP8 Arithmetic","source":"Search: last 90 days Hopper H100 tensor cores FP8 GEMM microarchitecture paper","published":"2025-12-08","fetched_at":"2026-01-19T14:11:22.301765679+00:00"}
{"url":"https://cursor.com/blog/kernels","title":"1.5x faster MoE training with custom MXFP8 kernels","source":"Search: last 90 days Blackwell B200 tensor core MMA instruction set GEMM performance analysis","published":"2026-01-19","fetched_at":"2026-01-19T14:11:45.110789552+00:00"}
{"url":"https://mlir.llvm.org/docs/Dialects/NVVMDialect/","title":"'nvvm' Dialect - MLIR","source":"Search: last 90 days MLIR LLVM NVPTX GPU compiler matmul tiling tensor core codegen research","published":"2026-01-14","fetched_at":"2026-01-19T14:12:41.992683934+00:00"}
{"url":"https://mlir.llvm.org/docs/Dialects/GPU/","title":"'gpu' Dialect - MLIR","source":"Search: last 90 days MLIR LLVM NVPTX GPU compiler matmul tiling tensor core codegen research","published":"2026-01-15","fetched_at":"2026-01-19T14:12:45.961226528+00:00"}
{"url":"https://zhihaojia.medium.com/generating-fast-gpu-kernels-without-programming-in-cuda-triton-3fdd4900d9bc","title":"Generating Fast GPU Kernels without Programming in CUDA/Triton | by Zhihao Jia | Medium","source":"Search: last 90 days Triton compiler matmul kernel fusion shared memory pipelining research","published":"2026-01-16","fetched_at":"2026-01-20T14:13:33.386678152+00:00"}
{"url":"https://forums.developer.nvidia.com/t/wmma-vs-wgmma-on-h100-gpu/354730","title":"Wmma vs Wgmma On H100 GPU - CUDA Programming and Performance - NVIDIA Developer Forums","source":"Search: last 90 days Hopper H100 tensor core WMMA GMMA matrix multiplication research paper","published":"2025-12-15","fetched_at":"2026-01-21T14:13:01.074544641+00:00"}
{"url":"https://www.hpc-ai.com/blog/b200","title":"HPC-AI Tech Blog | Insights on GPU Cloud, AI Training & HPC compute","source":"Search: last 90 days Blackwell B200 tensor cores GEMM performance analysis CUDA kernels","published":"2025-11-26","fetched_at":"2026-01-21T14:13:20.034927845+00:00"}
{"url":"https://lists.llvm.org/pipermail/llvm-commits/Week-of-Mon-20251229/1809915.html","title":"[llvm] 14b1d77 - [NVPTX] Add intrinsics and codegen for tensormap.replace (#172458)","source":"Search: last 90 days GPU compiler MLIR LLVM CUDA matmul codegen tensor core instruction selection","published":"2025-12-31","fetched_at":"2026-01-21T14:14:34.020914600+00:00"}
{"url":"https://llvm.org/docs/NVPTXUsage.html","title":"User Guide for NVPTX Back-end — LLVM 22.0.0git documentation","source":"Search: last 90 days GPU compiler MLIR LLVM CUDA matmul codegen tensor core instruction selection","published":"2026-01-14","fetched_at":"2026-01-21T14:14:38.285124322+00:00"}
{"url":"https://github.com/NVIDIA/cuda-tile","title":"GitHub - NVIDIA/cuda-tile: CUDA Tile IR is an MLIR-based intermediate representation and compiler infrastructure for CUDA kernel optimization, focusing on tile-based computation patterns and optimizations targeting NVIDIA tensor core units.","source":"Search: last 90 days GPU compiler MLIR LLVM CUDA matmul codegen tensor core instruction selection","published":"2025-12-31","fetched_at":"2026-01-21T14:14:41.493060268+00:00"}
{"url":"https://www.deepep.org/en/deepgemm","title":"DeepGEMM - Efficient FP8 Matrix Multiplication Library","source":"Search: last 90 days Hopper H100 tensor cores FP8 GEMM microarchitecture paper","published":"2025-12-31","fetched_at":"2026-01-22T14:13:39.784017624+00:00"}
{"url":"https://www.mlsysbook.ai/contents/core/hw_acceleration/hw_acceleration","title":"ML Systems Textbook — Hardware Acceleration (Systolic Arrays section)","source":"Search: last 90 days systolic array vs GPU tensor cores matrix multiplication roofline study","published":"2025-11-24","fetched_at":"2026-01-23T14:10:54.775501910+00:00"}
{"url":"https://www.modular.com/blog/matrix-multiplication-on-nvidias-blackwell-part-1-introduction","title":"Matrix Multiplication on NVIDIA's Blackwell: Part 1 - Introduction","source":"Search: last 90 days Blackwell B200 tensor core MMA instruction details matmul performance analysis","published":"2026-01-22","fetched_at":"2026-01-24T14:07:17.762982427+00:00"}
{"url":"https://docs.nvidia.com/cuda/nvidia-matmul-heuristics/","title":"NVIDIA Matmul Heuristics — nvMatmulHeuristics","source":"Search: last 90 days cuBLASLt FP8 GEMM autotuning heuristics performance study","published":"2026-01-22","fetched_at":"2026-01-24T14:07:48.747385872+00:00"}
{"url":"https://docs.nvidia.com/cutlass/latest/media/docs/cpp/heuristics.html","title":"GEMM Heuristics — NVIDIA CUTLASS Documentation","source":"Search: last 90 days cuBLASLt FP8 GEMM autotuning heuristics performance study","published":"2026-01-24","fetched_at":"2026-01-24T14:07:53.189155177+00:00"}
{"url":"https://docs.nvidia.com/cutlass/4.3.2/CHANGELOG.html","title":"Changelog — NVIDIA CUTLASS Documentation (CUTLASS 4.3.2)","source":"Search: last 90 days CUTLASS 3.x new GEMM kernels FP8 BF16 SM90 SM100 blog","published":"2025-12-05","fetched_at":"2026-01-25T14:07:07.333490590+00:00"}
{"url":"https://bentoml.com/llm/inference-optimization/flashattention","title":"FlashAttention | LLM Inference Handbook","source":"Search: last 90 days FlashAttention-3 CUDA kernel matmul attention IO-aware tiling tensor cores","published":"2026-01-22","fetched_at":"2026-01-25T14:07:43.764774843+00:00"}
{"url":"https://www.emergentmind.com/topics/flashattention-3-baseline","title":"FlashAttention-3 Baseline Benchmarks","source":"Search: last 90 days FlashAttention 3 GPU kernel matmul attention tensor cores benchmark","published":"2025-12-20","fetched_at":"2026-01-26T14:12:55.662343823+00:00"}
{"url":"https://arxiv.org/abs/2601.17091v1","title":"CUROCKET: Optimizing ROCKET for GPU","source":"arXiv: ML + GPU","published":"2026-01-23","fetched_at":"2026-01-27T14:13:01.386739498+00:00"}
{"url":"https://docs.nvidia.com/cuda/cublasdx/0.5.0/release_notes.html","title":"Release Notes — cuBLASDx 0.5.0","source":"Search: last 90 days cuBLASLt GEMM autotuning heuristics SM90 FP8 BF16 release notes","published":"2026-01-26","fetched_at":"2026-01-27T14:13:54.822039187+00:00"}
{"url":"https://docs.nvidia.com/cuda/cublasdx/0.3.1/release_notes.html","title":"Release Notes — cuBLASDx 0.3.1","source":"Search: last 90 days cuBLASLt GEMM autotuning heuristics SM90 FP8 BF16 release notes","published":"2026-01-22","fetched_at":"2026-01-27T14:13:56.720262705+00:00"}
{"url":"https://www.emergentmind.com/topics/flashattention-3","title":"FlashAttention-3: GPU-Optimized Attention","source":"Search: last 90 days FlashAttention 3 CUDA kernel implementation tensor cores FP8 BF16 benchmarks","published":"2026-01-24","fetched_at":"2026-01-27T14:14:20.535179016+00:00"}
{"url":"https://arxiv.org/abs/2511.12500","title":"Iris: First-Class Multi-GPU Programming Experience in Triton","source":"Search: last 90 days Triton compiler matmul kernel scheduling pipelining shared memory swizzle research","published":"2025-11-16","fetched_at":"2026-01-28T14:13:47.270348548+00:00"}
{"url":"https://arxiv.org/abs/2601.19911v1","title":"GPU-Augmented OLAP Execution Engine: GPU Offloading","source":"arXiv: GPU OR CUDA","published":"2025-12-24","fetched_at":"2026-01-29T14:18:14.933007303+00:00"}
{"url":"https://medium.com/@kvnagesh/architectural-divergence-in-ai-accelerators-beyond-the-memory-wall-and-systolic-arrays-de19dd2d973f","title":"Architectural Divergence in AI Accelerators: Beyond the Memory Wall and Systolic Arrays","source":"Search: last 90 days systolic array vs GPU tensor cores matrix multiplication throughput study","published":"2026-01-01","fetched_at":"2026-01-29T14:19:51.624946944+00:00"}
{"url":"https://docs.nvidia.com/cutlass/4.3.5/CHANGELOG.html","title":"Changelog — NVIDIA CUTLASS Documentation","source":"Search: last 90 days CUTLASS 3.x new GEMM kernels SM90 TMA warp-specialization blog","published":"2026-01-09","fetched_at":"2026-01-31T14:08:59.910366708+00:00"}
{"url":"https://docs.nvidia.com/cutlass/4.3.5/CHANGELOG.html#id3","title":"Changelog — NVIDIA CUTLASS Documentation","source":"Search: last 90 days CUTLASS 3.x new GEMM kernels SM90 TMA warp-specialization blog","published":"2025-12-22","fetched_at":"2026-01-31T14:09:01.578237143+00:00"}
{"url":"https://docs.nvidia.com/cutlass/4.3.5/CHANGELOG.html#id4","title":"Changelog — NVIDIA CUTLASS Documentation","source":"Search: last 90 days CUTLASS 3.x new GEMM kernels SM90 TMA warp-specialization blog","published":"2025-12-12","fetched_at":"2026-01-31T14:09:04.498156703+00:00"}
{"url":"https://rohany.github.io/blog/warp-specialization/","title":"Rohan Yadav — Warp Specialization","source":"Search: last 90 days systolic array vs tensor core GEMM mapping GPU architecture research","published":"2026-01-28","fetched_at":"2026-01-31T14:09:59.007340923+00:00"}
{"url":"https://arxiv.org/abs/2601.20595","title":"AutoOverlap: Enabling Fine-Grained Overlap of Computation and Communication with Chunk-Based Scheduling","source":"Search: last 90 days Triton compiler matmul kernel fusion shared memory swizzle research","published":"2026-01-28","fetched_at":"2026-02-01T14:10:47.597749871+00:00"}
{"url":"https://medium.com/@sananbintahir_87312/the-new-inference-stack-2025-flashattention-3-fp8-fp4-and-practical-patterns-for-cheap-fast-2ad7c99f8524","title":"The New Inference Stack (2025): FlashAttention-3, FP8→FP4, and Practical Patterns for Cheap, Fast LLM Serving","source":"Search: last 90 days FlashAttention 3 CUDA kernel matmul softmax tensor cores benchmark","published":"2026-01-18","fetched_at":"2026-02-01T14:11:02.407552596+00:00"}
{"url":"https://www.linkedin.com/posts/parth-dambhare_flashattention-2-faster-attention-with-better-activity-7411662328074694656-1-Oy","title":"FlashAttention 3 Boosts Inference Speed 1.5-2x with IO-Aware Algorithms | LinkedIn post","source":"Search: last 90 days FlashAttention 3 CUDA kernel matmul softmax tensor cores benchmark","published":"2026-01-25","fetched_at":"2026-02-01T14:11:07.411531047+00:00"}
{"url":"https://microsoft.github.io/Accera/Tutorials/GPU/Tensor_MatMul_GPU/","title":"Tensor MatMul GPU - Accera","source":"Search: last 90 days GPU systolic array matmul mapping tensor core MMA instruction scheduling","published":"2026-01-31","fetched_at":"2026-02-01T14:11:21.693748262+00:00"}
{"url":"https://microsoft.github.io/Accera/Tutorials/GPU/Tensor_MatMul_SchedulingPolicy_GPU/","title":"Tensor MatMul SchedulingPolicy GPU - Accera","source":"Search: last 90 days GPU systolic array matmul mapping tensor core MMA instruction scheduling","published":"2026-01-01","fetched_at":"2026-02-01T14:11:24.727676916+00:00"}
{"url":"https://llvm.org/devmtg/2025-03/","title":"The LLVM Compiler Infrastructure Project — March 2025 Meeting Schedule (Proton Dialect talk)","source":"Search: last 90 days MLIR LLVM GPU compiler matmul codegen tensor cores warp specialization","published":"2026-01-18","fetched_at":"2026-02-01T14:11:38.661997241+00:00"}
{"url":"https://arxiv.org/abs/2601.22585v1","title":"HetCCL: Accelerating LLM Training with Heterogeneous GPUs","source":"arXiv: ML + GPU","published":"2026-01-30","fetched_at":"2026-02-02T14:19:14.628878527+00:00"}
{"url":"https://modal.com/gpu-glossary/perf/roofline-model","title":"What is the roofline model? | GPU Glossary","source":"Search: last 90 days systolic array vs GPU tensor cores matrix multiplication roofline analysis","published":"2026-02-02","fetched_at":"2026-02-02T14:20:31.982786510+00:00"}
{"url":"https://arxiv.org/abs/2602.00328v1","title":"Harvest: Opportunistic Peer-to-Peer GPU Caching for LLM Inference","source":"arXiv: ML + GPU","published":"2026-01-30","fetched_at":"2026-02-03T14:21:14.548161350+00:00"}
{"url":"https://cursor.com/cn/blog/kernels","title":"1.5x faster MoE training with custom MXFP8 kernels","source":"Search: last 90 days Blackwell B200 tensor core MMA instruction set GEMM performance analysis","published":"2026-02-03","fetched_at":"2026-02-03T14:21:53.391756847+00:00"}
{"url":"https://arxiv.org/abs/2601.16294","title":"Space Filling Curves is All You Need: Communication-Avoiding Matrix Multiplication Made Simple","source":"Search: last 90 days CUTLASS 3.x new GEMM kernels SM90 SM100 tutorial blog","published":"2026-01-22","fetched_at":"2026-02-03T14:22:12.876371173+00:00"}
{"url":"https://gty111.github.io/2023/04/02/learn-cutlass-3/","title":"learn-cutlass-3 - Tianyu Guo's homepage","source":"Search: last 90 days NVIDIA PTX WGMMA warp-group MMA GEMM programming guide examples","published":"2026-01-27","fetched_at":"2026-02-03T14:23:20.463773323+00:00"}
