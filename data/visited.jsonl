{"url":"https://arxiv.org/abs/2512.02551v2","title":"CUDA-L2: Surpassing cuBLAS Performance for Matrix Multiplication through Reinforcement Learning","source":"arXiv: GPU OR CUDA","published":"2025-12-02","fetched_at":"2025-12-22T22:05:35.528485173+00:00"}
{"url":"https://arxiv.org/abs/2510.02894v1","title":"PyRadiomics-cuda: a GPU-accelerated 3D features extraction from medical images within PyRadiomics","source":"arXiv: GPU OR CUDA","published":"2025-10-03","fetched_at":"2025-12-22T22:05:39.392452519+00:00"}
{"url":"https://arxiv.org/abs/2511.02132v1","title":"Optimizing Attention on GPUs by Exploiting GPU Architectural NUMA Effects","source":"arXiv: ML + GPU","published":"2025-11-03","fetched_at":"2025-12-22T22:05:44.044068358+00:00"}
{"url":"https://arxiv.org/abs/2512.07853v1","title":"GPU Memory Prediction for Multimodal Model Training","source":"arXiv: ML + GPU","published":"2025-11-26","fetched_at":"2025-12-22T22:05:47.749870378+00:00"}
{"url":"https://arxiv.org/abs/2512.09472v1","title":"WarmServe: Enabling One-for-Many GPU Prewarming for Multi-LLM Serving","source":"arXiv: ML + GPU","published":"2025-12-10","fetched_at":"2025-12-22T22:05:53.592265725+00:00"}
{"url":"https://arxiv.org/abs/2511.00025v1","title":"On the Structure of Floating-Point Noise in Batch-Invariant GPU Matrix Multiplication","source":"arXiv: Matrix Multiplication + GPU","published":"2025-10-26","fetched_at":"2025-12-22T22:05:57.892405132+00:00"}
{"url":"https://arxiv.org/abs/2512.04226v1","title":"tritonBLAS: Triton-based Analytical Approach for GEMM Kernel Parameter Selection","source":"arXiv: Matrix Multiplication + GPU","published":"2025-12-03","fetched_at":"2025-12-22T22:06:01.960172404+00:00"}
{"url":"https://arxiv.org/abs/2510.03760v1","title":"EvoEngineer: Mastering Automated CUDA Kernel Code Evolution with Large Language Models","source":"arXiv: AI/ML + GPU","published":"2025-10-04","fetched_at":"2025-12-22T22:06:06.124690032+00:00"}
{"url":"https://arxiv.org/abs/2510.19873v1","title":"From Large to Small: Transferring CUDA Optimization Expertise via Reasoning Graph","source":"arXiv: AI/ML + GPU","published":"2025-10-22","fetched_at":"2025-12-22T22:06:10.750339230+00:00"}
{"url":"https://arxiv.org/abs/2511.17594v1","title":"AutoSAGE: Input-Aware CUDA Scheduling for Sparse GNN Aggregation (SpMM/SDDMM) and CSR Attention","source":"arXiv: AI/ML + GPU","published":"2025-11-17","fetched_at":"2025-12-22T22:06:15.264630374+00:00"}
{"url":"https://arxiv.org/abs/2511.19493v1","title":"RFX: High-Performance Random Forests with GPU Acceleration and QLORA Compression","source":"arXiv: AI/ML + GPU","published":"2025-11-23","fetched_at":"2025-12-22T22:06:20.145315106+00:00"}
{"url":"https://arxiv.org/abs/2511.01884v2","title":"CudaForge: An Agent Framework with Hardware Feedback for CUDA Kernel Optimization","source":"arXiv: AI/ML + GPU","published":"2025-10-23","fetched_at":"2025-12-22T22:06:24.278273543+00:00"}
{"url":"https://arxiv.org/abs/2512.14080","title":"SonicMoE: Accelerating MoE with IO and Tile-aware Optimizations","source":"Search: last 90 days Hopper H100 tensor cores GEMM research paper warp-group MMA FP8","published":"2025-12-16","fetched_at":"2025-12-22T22:07:08.699785573+00:00"}
{"url":"https://arxiv.org/abs/2512.02189","title":"Microbenchmarking NVIDIA's Blackwell Architecture: An in-depth Architectural Analysis","source":"Search: last 90 days Blackwell B200 GB200 tensor core architecture matmul throughput analysis","published":"2025-12-01","fetched_at":"2025-12-22T22:07:20.836961190+00:00"}
{"url":"https://arxiv.org/abs/2512.02551","title":"CUDA-L2: Surpassing cuBLAS Performance for Matrix Multiplication through Reinforcement Learning","source":"Search: last 90 days cuBLASLt FP8 GEMM autotuning heuristics performance regression study","published":"2025-12-02","fetched_at":"2025-12-22T22:07:55.454977127+00:00"}
{"url":"https://arxiv.org/abs/2511.18674","title":"Low-Rank GEMM: Efficient Matrix Multiplication via Low-Rank Approximation with FP8 Acceleration","source":"Search: last 90 days cuBLASLt FP8 GEMM autotuning heuristics performance regression study","published":"2025-11-24","fetched_at":"2025-12-22T22:07:58.897279888+00:00"}
{"url":"https://arxiv.org/abs/2510.20271v1","title":"Scalable GPU-Accelerated Euler Characteristic Curves: Optimization and Differentiable Learning for PyTorch","source":"arXiv: AI/ML + GPU","published":"2025-10-23","fetched_at":"2025-12-24T19:05:43.034999706+00:00"}
{"url":"https://github.com/NVIDIA/cutlass/releases/tag/v4.3.4","title":"CUTLASS 4.3.4 (GitHub Release)","source":"Search: last 90 days CUTLASS 3.x new GEMM kernels SM90 SM100 warp-specialized epilogue fusion","published":"2025-12-24","fetched_at":"2025-12-24T19:06:05.202135101+00:00"}
{"url":"https://triton-lang.org/main/getting-started/tutorials/09-persistent-matmul.html","title":"Persistent Matmul — Triton documentation","source":"Search: last 90 days Triton compiler matmul autotuning persistent kernels FP16 BF16 FP8","published":"2025-12-20","fetched_at":"2025-12-24T19:06:18.461976927+00:00"}
{"url":"https://lubits.ch/flash/Part-3","title":"Flash Attention from Scratch Part 3: Kernel 1","source":"Search: last 90 days FlashAttention-3 CUDA kernel details tensor core MMA pipeline shared memory scheduling","published":"2025-12-21","fetched_at":"2025-12-24T19:06:31.896526743+00:00"}
{"url":"https://lubits.ch/flash/Appendix","title":"Flash Attention from Scratch: Appendix","source":"Search: last 90 days FlashAttention-3 CUDA kernel details tensor core MMA pipeline shared memory scheduling","published":"2025-12-24","fetched_at":"2025-12-24T19:06:35.876246634+00:00"}
{"url":"https://aman.ai/primers/ai/flashattention/","title":"Aman's AI Journal • Primers • FlashAttention","source":"Search: last 90 days FlashAttention-3 CUDA kernel details tensor core MMA pipeline shared memory scheduling","published":"2025-12-20","fetched_at":"2025-12-24T19:06:38.730732064+00:00"}
{"url":"https://aman.ai/primers/ai/model-acceleration/","title":"Aman's AI Journal • Primers • Model Acceleration","source":"Search: last 90 days FlashAttention-3 CUDA kernel details tensor core MMA pipeline shared memory scheduling","published":"2025-12-20","fetched_at":"2025-12-24T19:06:41.989098601+00:00"}
{"url":"https://arxiv.org/abs/2512.12949","title":"FlashFuser: Expanding the Scale of Kernel Fusion for Compute-Intensive Operators via Inter-Core Connection","source":"Search: last 90 days GPU compiler research kernel fusion matmul epilogue fusion ML training throughput","published":"2025-12-15","fetched_at":"2025-12-24T19:07:07.168003932+00:00"}
{"url":"https://arxiv.org/abs/2512.16465","title":"cuPilot: A Strategy-Coordinated Multi-agent Framework for CUDA Kernel Evolution","source":"Search: last 90 days GPU compiler research kernel fusion matmul epilogue fusion ML training throughput","published":"2025-12-18","fetched_at":"2025-12-24T19:07:10.142445174+00:00"}
{"url":"https://arxiv.org/abs/2512.04226","title":"tritonBLAS: Triton-based Analytical Approach for GEMM Kernel Parameter Selection","source":"Search: last 90 days GPU compiler research kernel fusion matmul epilogue fusion ML training throughput","published":"2025-12-03","fetched_at":"2025-12-24T19:07:12.713762080+00:00"}
{"url":"https://www.preprints.org/manuscript/202511.1093/v1","title":"NeuronMM: High-Performance Matrix Multiplicationfor LLM Inference on AWS Trainium (Version 1)","source":"Search: last 90 days AI accelerator research matrix multiplication dataflow tensor core alternatives training inference","published":"2025-11-14","fetched_at":"2025-12-24T19:07:22.934337986+00:00"}
{"url":"https://wavespeed.ai/blogs/index.php/2025/12/10/wavespeedai-x-datacrunch-flux-real-time-image-inference-on-b200/","title":"WaveSpeedAI X DataCrunch: FLUX Real-Time Image Inference on B200 - WaveSpeedAI Blog","source":"Search: last 90 days Blackwell B200 tensor core architecture matmul throughput research","published":"2025-12-10","fetched_at":"2025-12-25T14:06:55.345130868+00:00"}
{"url":"https://github.com/NVIDIA/cutlass/releases/tag/v4.3.3","title":"CUTLASS 4.3.3","source":"Search: last 90 days CUTLASS 3.x new GEMM kernels FP8 BF16 SM90 SM100 blog","published":"2025-12-12","fetched_at":"2025-12-25T14:07:08.169757589+00:00"}
{"url":"https://github.com/NVIDIA/cutlass/releases/tag/v4.3.2","title":"CUTLASS 4.3.2","source":"Search: last 90 days CUTLASS 3.x new GEMM kernels FP8 BF16 SM90 SM100 blog","published":"2025-12-05","fetched_at":"2025-12-25T14:07:11.916745387+00:00"}
{"url":"https://arxiv.org/abs/2512.02875","title":"SAT-MapIt: A SAT-based Modulo Scheduling Mapper for Coarse Grain Reconfigurable Architectures","source":"Search: last 90 days CUDA 12.5 12.6 WGMMA GMMA matmul instruction scheduling research","published":"2025-12-02","fetched_at":"2025-12-25T14:08:05.567147033+00:00"}
{"url":"https://docs.nvidia.com/cutlass/4.3.0/media/docs/cpp/gemm_api_3x.html","title":"CUTLASS 3.0 GEMM API — NVIDIA CUTLASS Documentation","source":"Search: last 90 days CUTLASS 3.x new GEMM kernels SM90 SM100 TMA warp-specialized tutorial","published":"2025-12-23","fetched_at":"2025-12-26T14:07:26.173472281+00:00"}
{"url":"https://pytorch.org/blog/cutlass-ping-pong-gemm-kernel/","title":"Deep Dive on CUTLASS Ping-Pong GEMM Kernel – PyTorch","source":"Search: last 90 days CUTLASS 3.x new GEMM kernels SM90 SM100 TMA warp-specialized tutorial","published":"2025-12-22","fetched_at":"2025-12-26T14:07:29.597488464+00:00"}
{"url":"https://docs.nvidia.com/cutlass/4.3.1/CHANGELOG.html","title":"Changelog — NVIDIA CUTLASS Documentation","source":"Search: last 90 days CUTLASS 3.x new GEMM kernels SM90 SM100 TMA warp-specialized tutorial","published":"2025-12-23","fetched_at":"2025-12-26T14:07:31.697878805+00:00"}
{"url":"https://arxiv.org/abs/2512.02371","title":"Pushing Tensor Accelerators Beyond MatMul in a User-Schedulable Language","source":"Search: last 90 days FlashAttention-3 GPU kernel implementation tensor cores matmul research","published":"2025-12-02","fetched_at":"2025-12-27T14:08:26.670819479+00:00"}
{"url":"https://arxiv.org/abs/2512.18134","title":"Optimal Software Pipelining and Warp Specialization for Tensor Core GPUs","source":"Search: last 90 days GPU systolic array mapping GEMM scheduling tensor core research","published":"2025-12-19","fetched_at":"2025-12-27T14:08:39.884584579+00:00"}
{"url":"https://arxiv.org/abs/2512.19250","title":"Small Language Models as Compiler Experts: Auto-Parallelization for Heterogeneous Systems","source":"Search: last 90 days MLIR LLVM CUDA GPU compiler matmul codegen tensor core research","published":"2025-12-22","fetched_at":"2025-12-27T14:08:54.061746347+00:00"}
{"url":"https://docs.nvidia.com/cuda/cublasmp/release_notes","title":"Release Notes — cuBLASMp (cuBLASMp v0.7.0 released November 24, 2025)","source":"Search: last 90 days cuBLASLt GEMM autotuning heuristics FP8 BF16 performance update","published":"2025-11-24","fetched_at":"2025-12-28T14:07:43.987863493+00:00"}
{"url":"https://arxiv.org/abs/2511.02302","title":"FP8-Flow-MoE: A Casting-Free FP8 Recipe without Double Quantization Error","source":"Search: last 90 days cuBLASLt GEMM autotuning heuristics FP8 BF16 performance update","published":"2025-11-04","fetched_at":"2025-12-28T14:07:48.743783276+00:00"}
{"url":"https://pytorch.org/blog/flashattention-3/","title":"FlashAttention-3: Fast and Accurate Attention with Asynchrony and Low-precision","source":"Search: last 90 days FlashAttention 3 CUDA kernel matmul attention tensor cores benchmark","published":"2025-12-23","fetched_at":"2025-12-28T14:08:15.748752901+00:00"}
{"url":"https://github.com/togethercomputer/flash-attention-3","title":"GitHub - togethercomputer/flash-attention-3: Fast and memory-efficient exact attention","source":"Search: last 90 days FlashAttention 3 CUDA kernel matmul attention tensor cores benchmark","published":"2025-12-07","fetched_at":"2025-12-28T14:08:20.017146772+00:00"}
{"url":"https://pytorch.org/blog/hadacore/","title":"HadaCore: Tensor Core Accelerated Hadamard Transform Kernel – PyTorch","source":"Search: last 90 days FlashAttention-3 GPU kernel matmul softmax tensor cores benchmark","published":"2025-12-23","fetched_at":"2025-12-29T14:09:02.878111626+00:00"}
{"url":"https://pytorch.org/blog/hopper-tma-unit/","title":"Deep Dive on the Hopper TMA Unit for FP8 GEMMs – PyTorch","source":"Search: last 90 days Hopper H200 tensor cores FP8 GEMM performance analysis CUTLASS cuBLAS","published":"2025-12-28","fetched_at":"2025-12-30T14:07:35.059318414+00:00"}
{"url":"https://www.emergentmind.com/topics/blackwell-gpu-architecture","title":"Blackwell GPU Architecture","source":"Search: last 90 days Hopper H200 tensor cores FP8 GEMM performance analysis CUTLASS cuBLAS","published":"2025-12-25","fetched_at":"2025-12-30T14:07:39.049097759+00:00"}
{"url":"https://arxiv.org/abs/2512.22219","title":"Mirage Persistent Kernel: A Compiler and Runtime for Mega-Kernelizing Tensor Programs","source":"Search: last 90 days GPU kernel fusion compiler MLIR CUDA matmul epilogue fusion research paper","published":"2025-12-22","fetched_at":"2025-12-30T14:09:11.658881207+00:00"}
{"url":"https://pytorch.org/blog/warp-specialization/","title":"Enabling advanced GPU features in PyTorch – Warp Specialization","source":"Search: last 90 days Triton compiler matmul performance improvements warp-specialization research","published":"2025-12-28","fetched_at":"2025-12-31T14:08:00.768698066+00:00"}
{"url":"https://www.reddit.com/r/CUDA/comments/1px5pj6/about_wgmmamma/","title":"About wgmma.mma_async.sync.aligned.m64n256k16.f16.f16.f16 instruction's descriptors and byte offsets.","source":"Search: last 90 days CUDA 12.6 PTX wgmma mma instruction GEMM optimization research","published":"2025-12-27","fetched_at":"2025-12-31T14:08:29.532262656+00:00"}
{"url":"https://arxiv.org/abs/2512.09196","title":"TritonForge: Profiling-Guided Framework for Automated Triton Kernel Optimization","source":"Search: last 90 days Triton compiler matmul kernel fusion autotuning research preprint","published":"2025-12-09","fetched_at":"2026-01-01T14:07:17.935723261+00:00"}
{"url":"https://www.emergentmind.com/topics/ml-triton","title":"ML-Triton: Multi-Level GPU Compiler","source":"Search: last 90 days GPU compiler MLIR LLVM NVPTX matmul codegen tensor core lowering research","published":"2025-12-25","fetched_at":"2026-01-01T14:08:06.542791786+00:00"}
{"url":"https://arxiv.org/abs/2512.21571","title":"nncase: An End-to-End Compiler for Efficient LLM Deployment on Heterogeneous Storage Architectures","source":"Search: last 90 days GPU compiler MLIR LLVM NVPTX matmul codegen tensor core lowering research","published":"2025-12-25","fetched_at":"2026-01-01T14:08:09.778887857+00:00"}
{"url":"https://www.emergentmind.com/papers/2512.02189","title":"Microbenchmarking NVIDIA's Blackwell Architecture: An in-depth Architectural Analysis","source":"Search: last 90 days NVIDIA Hopper H100 tensor cores GEMM research paper","published":"2025-12-01","fetched_at":"2026-01-02T14:07:00.750252307+00:00"}
{"url":"https://triton-lang.org/main/getting-started/tutorials/10-block-scaled-matmul.html","title":"Block Scaled Matrix Multiplication — Triton documentation","source":"Search: last 90 days Triton compiler matmul autotuning tensor core wgmma research","published":"2025-12-28","fetched_at":"2026-01-02T14:07:30.515300381+00:00"}
{"url":"https://arxiv.org/abs/2511.12653","title":"DPVO-QAT++: Heterogeneous QAT and CUDA Kernel Fusion for High-Performance Deep Patch Visual Odometry","source":"Search: last 90 days GPU kernel fusion compiler matmul epilogue fusion ML training research","published":"2025-11-16","fetched_at":"2026-01-02T14:08:12.580468235+00:00"}
{"url":"https://arxiv.org/abs/2512.13638","title":"Design in Tiles: Automating GEMM Deployment on Tile-Based Many-PE Accelerators","source":"Search: last 90 days Hopper H100 tensor cores FP8 GEMM research paper","published":"2025-12-15","fetched_at":"2026-01-03T14:06:47.322535017+00:00"}
{"url":"https://docs.nvidia.com/cutlass/latest/CHANGELOG.html","title":"CUTLASS 4.x Changelog — NVIDIA CUTLASS Documentation","source":"Search: last 90 days CUTLASS 3.x new GEMM kernels SM90 SM100 performance blog","published":"2025-12-12","fetched_at":"2026-01-03T14:07:13.081738999+00:00"}
{"url":"https://github.com/NVIDIA/cutlass/releases","title":"Releases · NVIDIA/cutlass · GitHub","source":"Search: last 90 days CUTLASS 3.x new GEMM kernels SM90 SM100 performance blog","published":"2025-12-24","fetched_at":"2026-01-03T14:07:16.411251463+00:00"}
{"url":"https://www.aimodels.fyi/papers/arxiv/optimal-software-pipelining-warp-specialization-tensor-core","title":"Optimal Software Pipelining and Warp Specialization for Tensor Core GPUs","source":"Search: last 90 days GPU systolic array vs SIMT tensor core matmul mapping research","published":"2025-12-23","fetched_at":"2026-01-03T14:07:49.797122306+00:00"}
{"url":"https://arxiv.org/abs/2511.13764","title":"Library Liberation: Competitive Performance Matmul Through Compiler-composed Nanokernels","source":"Search: last 90 days GPU compiler MLIR LLVM CUDA matmul scheduling tensor core codegen paper","published":"2025-11-14","fetched_at":"2026-01-03T14:08:07.219923233+00:00"}
{"url":"https://www.modular.com/blog/matrix-multiplication-on-nvidias-blackwell-part-2-using-hardware-features-to-optimize-matmul","title":"Modular: Matrix Multiplication on Blackwell: Part 2 - Using Hardware Features to Optimize Matmul","source":"Search: last 90 days GPU systolic array vs tensor cores matrix multiplication research","published":"2026-01-04","fetched_at":"2026-01-04T14:08:09.198990503+00:00"}
{"url":"https://docs.nvidia.com/cutlass/latest/media/docs/cpp/blackwell_functionality.html","title":"Blackwell SM100 GEMMs — NVIDIA CUTLASS Documentation","source":"Search: last 90 days GPU systolic array mapping GEMM tensor core MMA instruction scheduling research","published":"2026-01-02","fetched_at":"2026-01-05T14:10:11.976070985+00:00"}
{"url":"https://leimao.github.io/blog/NVIDIA-Tensor-Core-MMA-Instruction-TN-Layout/","title":"NVIDIA Tensor Core TN Layout MMA Instruction - Lei Mao's Log Book","source":"Search: last 90 days GPU systolic array mapping GEMM tensor core MMA instruction scheduling research","published":"2025-12-06","fetched_at":"2026-01-05T14:10:16.483764189+00:00"}
{"url":"https://docs.nvidia.com/cutlass/4.3.0/CHANGELOG.html","title":"Changelog — NVIDIA CUTLASS Documentation (CUTLASS 4.3.0)","source":"Search: last 90 days cuBLASLt GEMM autotuning heuristics epilogue fusion SM90 SM100","published":"2025-11-21","fetched_at":"2026-01-06T14:07:42.929625072+00:00"}
{"url":"https://docs.nvidia.com/cutlass/4.3.0/overview.html","title":"Overview — NVIDIA CUTLASS Documentation (CUTLASS 4.3.0)","source":"Search: last 90 days cuBLASLt GEMM autotuning heuristics epilogue fusion SM90 SM100","published":"2025-11-01","fetched_at":"2026-01-06T14:07:47.512551926+00:00"}
{"url":"https://nvidia.github.io/TensorRT-LLM/blogs/H100vsA100.html","title":"H100 has 4.6x A100 Performance in TensorRT LLM, achieving 10,000 tok/s at 100ms to first token — TensorRT LLM","source":"Search: last 90 days Hopper H100 tensor cores FP8 GEMM microarchitecture paper","published":"2025-12-17","fetched_at":"2026-01-07T14:08:10.408382042+00:00"}
{"url":"https://docs.nvidia.com/deeplearning/triton-inference-server/release-notes/rel-25-12.html","title":"Release Notes :: NVIDIA Deep Learning Triton Inference Server Documentation (Release 25.12)","source":"Search: last 90 days Triton compiler matmul autotuning persistent kernel research","published":"2025-12-22","fetched_at":"2026-01-07T14:09:03.802553753+00:00"}
{"url":"https://llmlaba.com/articles/flashattn-cuda-compatibility.html","title":"FlashAttention compatibility | [“LLM Laboratory”]","source":"Search: last 90 days FlashAttention-3 CUDA kernel matmul attention tensor cores SM90 benchmarks","published":"2025-12-25","fetched_at":"2026-01-08T14:11:02.182470490+00:00"}
{"url":"https://mlir.llvm.org/docs/Dialects/NVGPU/","title":"'nvgpu' Dialect - MLIR","source":"Search: last 90 days MLIR LLVM NVPTX CUDA matmul codegen tensor core MMA lowering research","published":"2026-01-04","fetched_at":"2026-01-08T14:11:32.774098031+00:00"}
{"url":"https://mlir.llvm.org/doxygen/namespacemlir_1_1nvgpu.html","title":"MLIR: mlir::nvgpu Namespace Reference","source":"Search: last 90 days MLIR LLVM NVPTX CUDA matmul codegen tensor core MMA lowering research","published":"2025-12-30","fetched_at":"2026-01-08T14:11:36.025811023+00:00"}
{"url":"https://mlir.llvm.org/python-bindings/autoapi/mlir/dialects/transform/nvgpu/index.html","title":"mlir.dialects.transform.nvgpu - MLIR Python bindings documentation","source":"Search: last 90 days MLIR LLVM NVPTX CUDA matmul codegen tensor core MMA lowering research","published":"2025-12-25","fetched_at":"2026-01-08T14:11:38.560621623+00:00"}
{"url":"https://github.com/NVIDIA/cutlass/releases/tag/v4.3.5","title":"CUTLASS 4.3.5 (GitHub Release)","source":"Search: last 90 days CUTLASS 3.x new GEMM kernels SM90 SM100 release notes","published":"2026-01-09","fetched_at":"2026-01-09T14:08:22.581881861+00:00"}
{"url":"https://blog.csdn.net/Together_CZ/article/details/143786789","title":"FlashAttention-3:Fast and Accurate Attention with Asynchrony and Low-precision——利用异步和低精度实现快速准确的注意力机制_flashattention3-CSDN博客","source":"Search: last 90 days FlashAttention-3 CUDA kernel implementation tensor core GEMM details","published":"2025-12-19","fetched_at":"2026-01-09T14:08:51.862493139+00:00"}
{"url":"https://www.reddit.com/r/comfyui/comments/1q29i5h/flashattention_283_fails_during_comfyui_sampling/","title":"FlashAttention 2.8.3 fails during ComfyUI sampling (Torch 2.9.1 + cu130, Windows 11, RTX 5090)","source":"Search: last 90 days FlashAttention 3 CUDA kernel implementation tensor cores matmul","published":"2026-01-02","fetched_at":"2026-01-11T14:07:30.281206588+00:00"}
{"url":"https://naddod.medium.com/introduction-to-tensor-cores-in-nvidia-gpus-ae2a79642733","title":"Introduction to Tensor Cores in NVIDIA GPUs","source":"Search: last 90 days systolic array vs GPU tensor cores matrix multiplication accelerator comparison","published":"2026-01-01","fetched_at":"2026-01-12T14:10:58.420314155+00:00"}
{"url":"https://www.linkedin.com/pulse/gputpu-architectures-sharada-yeluri-p5hwc","title":"GPU/TPU Architectures","source":"Search: last 90 days systolic array vs GPU tensor cores matrix multiplication accelerator comparison","published":"2025-12-30","fetched_at":"2026-01-12T14:11:04.480239032+00:00"}
{"url":"https://medium.com/@21338787/tensor-core-gpus-impact-on-performance-improvements-in-ai-2ca1597beccd","title":"Tensor core GPU’s impact on performance improvements in AI","source":"Search: last 90 days systolic array vs GPU tensor cores matrix multiplication accelerator comparison","published":"2025-12-28","fetched_at":"2026-01-12T14:11:08.131465854+00:00"}
{"url":"https://www.nvidia.com/en-us/data-center/technologies/hopper-architecture/","title":"Hopper GPU Architecture | NVIDIA","source":"Search: last 90 days Hopper H100 tensor cores FP8 GEMM microarchitecture paper","published":"2026-01-15","fetched_at":"2026-01-15T14:08:31.069971201+00:00"}
{"url":"https://tilelang.com/deeplearning_operators/matmul.html","title":"General Matrix-Matrix Multiplication with Tile Library - TileLang 0.1.7.post2 documentation","source":"Search: last 90 days Triton compiler matmul scheduling shared memory swizzle research","published":"2026-01-11","fetched_at":"2026-01-15T14:09:14.048446538+00:00"}
{"url":"https://arxiv.org/abs/2601.07048v2","title":"GPU-Accelerated ANNS: Quantized for Speed, Built for Change","source":"arXiv: AI/ML + GPU","published":"2026-01-11","fetched_at":"2026-01-16T14:07:46.129301709+00:00"}
{"url":"https://arxiv.org/abs/2512.07724","title":"The Native Spiking Microarchitecture: From Iontronic Primitives to Bit-Exact FP8 Arithmetic","source":"Search: last 90 days Hopper H100 tensor cores FP8 GEMM microarchitecture paper","published":"2025-12-08","fetched_at":"2026-01-19T14:11:22.301765679+00:00"}
{"url":"https://cursor.com/blog/kernels","title":"1.5x faster MoE training with custom MXFP8 kernels","source":"Search: last 90 days Blackwell B200 tensor core MMA instruction set GEMM performance analysis","published":"2026-01-19","fetched_at":"2026-01-19T14:11:45.110789552+00:00"}
{"url":"https://mlir.llvm.org/docs/Dialects/NVVMDialect/","title":"'nvvm' Dialect - MLIR","source":"Search: last 90 days MLIR LLVM NVPTX GPU compiler matmul tiling tensor core codegen research","published":"2026-01-14","fetched_at":"2026-01-19T14:12:41.992683934+00:00"}
{"url":"https://mlir.llvm.org/docs/Dialects/GPU/","title":"'gpu' Dialect - MLIR","source":"Search: last 90 days MLIR LLVM NVPTX GPU compiler matmul tiling tensor core codegen research","published":"2026-01-15","fetched_at":"2026-01-19T14:12:45.961226528+00:00"}
{"url":"https://zhihaojia.medium.com/generating-fast-gpu-kernels-without-programming-in-cuda-triton-3fdd4900d9bc","title":"Generating Fast GPU Kernels without Programming in CUDA/Triton | by Zhihao Jia | Medium","source":"Search: last 90 days Triton compiler matmul kernel fusion shared memory pipelining research","published":"2026-01-16","fetched_at":"2026-01-20T14:13:33.386678152+00:00"}
{"url":"https://forums.developer.nvidia.com/t/wmma-vs-wgmma-on-h100-gpu/354730","title":"Wmma vs Wgmma On H100 GPU - CUDA Programming and Performance - NVIDIA Developer Forums","source":"Search: last 90 days Hopper H100 tensor core WMMA GMMA matrix multiplication research paper","published":"2025-12-15","fetched_at":"2026-01-21T14:13:01.074544641+00:00"}
{"url":"https://www.hpc-ai.com/blog/b200","title":"HPC-AI Tech Blog | Insights on GPU Cloud, AI Training & HPC compute","source":"Search: last 90 days Blackwell B200 tensor cores GEMM performance analysis CUDA kernels","published":"2025-11-26","fetched_at":"2026-01-21T14:13:20.034927845+00:00"}
{"url":"https://lists.llvm.org/pipermail/llvm-commits/Week-of-Mon-20251229/1809915.html","title":"[llvm] 14b1d77 - [NVPTX] Add intrinsics and codegen for tensormap.replace (#172458)","source":"Search: last 90 days GPU compiler MLIR LLVM CUDA matmul codegen tensor core instruction selection","published":"2025-12-31","fetched_at":"2026-01-21T14:14:34.020914600+00:00"}
{"url":"https://llvm.org/docs/NVPTXUsage.html","title":"User Guide for NVPTX Back-end — LLVM 22.0.0git documentation","source":"Search: last 90 days GPU compiler MLIR LLVM CUDA matmul codegen tensor core instruction selection","published":"2026-01-14","fetched_at":"2026-01-21T14:14:38.285124322+00:00"}
{"url":"https://github.com/NVIDIA/cuda-tile","title":"GitHub - NVIDIA/cuda-tile: CUDA Tile IR is an MLIR-based intermediate representation and compiler infrastructure for CUDA kernel optimization, focusing on tile-based computation patterns and optimizations targeting NVIDIA tensor core units.","source":"Search: last 90 days GPU compiler MLIR LLVM CUDA matmul codegen tensor core instruction selection","published":"2025-12-31","fetched_at":"2026-01-21T14:14:41.493060268+00:00"}
{"url":"https://www.deepep.org/en/deepgemm","title":"DeepGEMM - Efficient FP8 Matrix Multiplication Library","source":"Search: last 90 days Hopper H100 tensor cores FP8 GEMM microarchitecture paper","published":"2025-12-31","fetched_at":"2026-01-22T14:13:39.784017624+00:00"}
{"url":"https://www.mlsysbook.ai/contents/core/hw_acceleration/hw_acceleration","title":"ML Systems Textbook — Hardware Acceleration (Systolic Arrays section)","source":"Search: last 90 days systolic array vs GPU tensor cores matrix multiplication roofline study","published":"2025-11-24","fetched_at":"2026-01-23T14:10:54.775501910+00:00"}
{"url":"https://www.modular.com/blog/matrix-multiplication-on-nvidias-blackwell-part-1-introduction","title":"Matrix Multiplication on NVIDIA's Blackwell: Part 1 - Introduction","source":"Search: last 90 days Blackwell B200 tensor core MMA instruction details matmul performance analysis","published":"2026-01-22","fetched_at":"2026-01-24T14:07:17.762982427+00:00"}
{"url":"https://docs.nvidia.com/cuda/nvidia-matmul-heuristics/","title":"NVIDIA Matmul Heuristics — nvMatmulHeuristics","source":"Search: last 90 days cuBLASLt FP8 GEMM autotuning heuristics performance study","published":"2026-01-22","fetched_at":"2026-01-24T14:07:48.747385872+00:00"}
{"url":"https://docs.nvidia.com/cutlass/latest/media/docs/cpp/heuristics.html","title":"GEMM Heuristics — NVIDIA CUTLASS Documentation","source":"Search: last 90 days cuBLASLt FP8 GEMM autotuning heuristics performance study","published":"2026-01-24","fetched_at":"2026-01-24T14:07:53.189155177+00:00"}
{"url":"https://docs.nvidia.com/cutlass/4.3.2/CHANGELOG.html","title":"Changelog — NVIDIA CUTLASS Documentation (CUTLASS 4.3.2)","source":"Search: last 90 days CUTLASS 3.x new GEMM kernels FP8 BF16 SM90 SM100 blog","published":"2025-12-05","fetched_at":"2026-01-25T14:07:07.333490590+00:00"}
{"url":"https://bentoml.com/llm/inference-optimization/flashattention","title":"FlashAttention | LLM Inference Handbook","source":"Search: last 90 days FlashAttention-3 CUDA kernel matmul attention IO-aware tiling tensor cores","published":"2026-01-22","fetched_at":"2026-01-25T14:07:43.764774843+00:00"}
{"url":"https://www.emergentmind.com/topics/flashattention-3-baseline","title":"FlashAttention-3 Baseline Benchmarks","source":"Search: last 90 days FlashAttention 3 GPU kernel matmul attention tensor cores benchmark","published":"2025-12-20","fetched_at":"2026-01-26T14:12:55.662343823+00:00"}
{"url":"https://arxiv.org/abs/2601.17091v1","title":"CUROCKET: Optimizing ROCKET for GPU","source":"arXiv: ML + GPU","published":"2026-01-23","fetched_at":"2026-01-27T14:13:01.386739498+00:00"}
{"url":"https://docs.nvidia.com/cuda/cublasdx/0.5.0/release_notes.html","title":"Release Notes — cuBLASDx 0.5.0","source":"Search: last 90 days cuBLASLt GEMM autotuning heuristics SM90 FP8 BF16 release notes","published":"2026-01-26","fetched_at":"2026-01-27T14:13:54.822039187+00:00"}
{"url":"https://docs.nvidia.com/cuda/cublasdx/0.3.1/release_notes.html","title":"Release Notes — cuBLASDx 0.3.1","source":"Search: last 90 days cuBLASLt GEMM autotuning heuristics SM90 FP8 BF16 release notes","published":"2026-01-22","fetched_at":"2026-01-27T14:13:56.720262705+00:00"}
{"url":"https://www.emergentmind.com/topics/flashattention-3","title":"FlashAttention-3: GPU-Optimized Attention","source":"Search: last 90 days FlashAttention 3 CUDA kernel implementation tensor cores FP8 BF16 benchmarks","published":"2026-01-24","fetched_at":"2026-01-27T14:14:20.535179016+00:00"}
{"url":"https://arxiv.org/abs/2511.12500","title":"Iris: First-Class Multi-GPU Programming Experience in Triton","source":"Search: last 90 days Triton compiler matmul kernel scheduling pipelining shared memory swizzle research","published":"2025-11-16","fetched_at":"2026-01-28T14:13:47.270348548+00:00"}
{"url":"https://arxiv.org/abs/2601.19911v1","title":"GPU-Augmented OLAP Execution Engine: GPU Offloading","source":"arXiv: GPU OR CUDA","published":"2025-12-24","fetched_at":"2026-01-29T14:18:14.933007303+00:00"}
{"url":"https://medium.com/@kvnagesh/architectural-divergence-in-ai-accelerators-beyond-the-memory-wall-and-systolic-arrays-de19dd2d973f","title":"Architectural Divergence in AI Accelerators: Beyond the Memory Wall and Systolic Arrays","source":"Search: last 90 days systolic array vs GPU tensor cores matrix multiplication throughput study","published":"2026-01-01","fetched_at":"2026-01-29T14:19:51.624946944+00:00"}
{"url":"https://docs.nvidia.com/cutlass/4.3.5/CHANGELOG.html","title":"Changelog — NVIDIA CUTLASS Documentation","source":"Search: last 90 days CUTLASS 3.x new GEMM kernels SM90 TMA warp-specialization blog","published":"2026-01-09","fetched_at":"2026-01-31T14:08:59.910366708+00:00"}
{"url":"https://docs.nvidia.com/cutlass/4.3.5/CHANGELOG.html#id3","title":"Changelog — NVIDIA CUTLASS Documentation","source":"Search: last 90 days CUTLASS 3.x new GEMM kernels SM90 TMA warp-specialization blog","published":"2025-12-22","fetched_at":"2026-01-31T14:09:01.578237143+00:00"}
{"url":"https://docs.nvidia.com/cutlass/4.3.5/CHANGELOG.html#id4","title":"Changelog — NVIDIA CUTLASS Documentation","source":"Search: last 90 days CUTLASS 3.x new GEMM kernels SM90 TMA warp-specialization blog","published":"2025-12-12","fetched_at":"2026-01-31T14:09:04.498156703+00:00"}
{"url":"https://rohany.github.io/blog/warp-specialization/","title":"Rohan Yadav — Warp Specialization","source":"Search: last 90 days systolic array vs tensor core GEMM mapping GPU architecture research","published":"2026-01-28","fetched_at":"2026-01-31T14:09:59.007340923+00:00"}
{"url":"https://arxiv.org/abs/2601.20595","title":"AutoOverlap: Enabling Fine-Grained Overlap of Computation and Communication with Chunk-Based Scheduling","source":"Search: last 90 days Triton compiler matmul kernel fusion shared memory swizzle research","published":"2026-01-28","fetched_at":"2026-02-01T14:10:47.597749871+00:00"}
{"url":"https://medium.com/@sananbintahir_87312/the-new-inference-stack-2025-flashattention-3-fp8-fp4-and-practical-patterns-for-cheap-fast-2ad7c99f8524","title":"The New Inference Stack (2025): FlashAttention-3, FP8→FP4, and Practical Patterns for Cheap, Fast LLM Serving","source":"Search: last 90 days FlashAttention 3 CUDA kernel matmul softmax tensor cores benchmark","published":"2026-01-18","fetched_at":"2026-02-01T14:11:02.407552596+00:00"}
{"url":"https://www.linkedin.com/posts/parth-dambhare_flashattention-2-faster-attention-with-better-activity-7411662328074694656-1-Oy","title":"FlashAttention 3 Boosts Inference Speed 1.5-2x with IO-Aware Algorithms | LinkedIn post","source":"Search: last 90 days FlashAttention 3 CUDA kernel matmul softmax tensor cores benchmark","published":"2026-01-25","fetched_at":"2026-02-01T14:11:07.411531047+00:00"}
{"url":"https://microsoft.github.io/Accera/Tutorials/GPU/Tensor_MatMul_GPU/","title":"Tensor MatMul GPU - Accera","source":"Search: last 90 days GPU systolic array matmul mapping tensor core MMA instruction scheduling","published":"2026-01-31","fetched_at":"2026-02-01T14:11:21.693748262+00:00"}
{"url":"https://microsoft.github.io/Accera/Tutorials/GPU/Tensor_MatMul_SchedulingPolicy_GPU/","title":"Tensor MatMul SchedulingPolicy GPU - Accera","source":"Search: last 90 days GPU systolic array matmul mapping tensor core MMA instruction scheduling","published":"2026-01-01","fetched_at":"2026-02-01T14:11:24.727676916+00:00"}
{"url":"https://llvm.org/devmtg/2025-03/","title":"The LLVM Compiler Infrastructure Project — March 2025 Meeting Schedule (Proton Dialect talk)","source":"Search: last 90 days MLIR LLVM GPU compiler matmul codegen tensor cores warp specialization","published":"2026-01-18","fetched_at":"2026-02-01T14:11:38.661997241+00:00"}
{"url":"https://arxiv.org/abs/2601.22585v1","title":"HetCCL: Accelerating LLM Training with Heterogeneous GPUs","source":"arXiv: ML + GPU","published":"2026-01-30","fetched_at":"2026-02-02T14:19:14.628878527+00:00"}
{"url":"https://modal.com/gpu-glossary/perf/roofline-model","title":"What is the roofline model? | GPU Glossary","source":"Search: last 90 days systolic array vs GPU tensor cores matrix multiplication roofline analysis","published":"2026-02-02","fetched_at":"2026-02-02T14:20:31.982786510+00:00"}
{"url":"https://arxiv.org/abs/2602.00328v1","title":"Harvest: Opportunistic Peer-to-Peer GPU Caching for LLM Inference","source":"arXiv: ML + GPU","published":"2026-01-30","fetched_at":"2026-02-03T14:21:14.548161350+00:00"}
{"url":"https://cursor.com/cn/blog/kernels","title":"1.5x faster MoE training with custom MXFP8 kernels","source":"Search: last 90 days Blackwell B200 tensor core MMA instruction set GEMM performance analysis","published":"2026-02-03","fetched_at":"2026-02-03T14:21:53.391756847+00:00"}
{"url":"https://arxiv.org/abs/2601.16294","title":"Space Filling Curves is All You Need: Communication-Avoiding Matrix Multiplication Made Simple","source":"Search: last 90 days CUTLASS 3.x new GEMM kernels SM90 SM100 tutorial blog","published":"2026-01-22","fetched_at":"2026-02-03T14:22:12.876371173+00:00"}
{"url":"https://gty111.github.io/2023/04/02/learn-cutlass-3/","title":"learn-cutlass-3 - Tianyu Guo's homepage","source":"Search: last 90 days NVIDIA PTX WGMMA warp-group MMA GEMM programming guide examples","published":"2026-01-27","fetched_at":"2026-02-03T14:23:20.463773323+00:00"}
{"url":"https://developer.nvidia.com/blog/advancing-gpu-programming-with-the-cuda-tile-ir-backend-for-openai-triton/","title":"Advancing GPU Programming with the CUDA Tile IR Backend for OpenAI Triton","source":"Search: last 90 days Triton compiler matmul kernel scheduling shared memory swizzle research","published":"2026-01-30","fetched_at":"2026-02-04T14:21:29.627807175+00:00"}
{"url":"https://medium.com/%40emmanuelalo52/the-100-billion-bottleneck-why-kernel-fusion-is-the-secret-to-scaling-ai-2cd62ae04107","title":"The $100 Billion Bottleneck: Why Kernel Fusion is the Secret to Scaling AI","source":"Search: last 90 days FlashAttention-3 GPU kernel matmul softmax fusion tensor cores benchmark","published":"2026-01-01","fetched_at":"2026-02-04T14:21:44.815871508+00:00"}
{"url":"https://arxiv.org/abs/2601.07048v3","title":"GPU-Accelerated ANNS: Quantized for Speed, Built for Change","source":"arXiv: AI/ML + GPU","published":"2026-01-11","fetched_at":"2026-02-05T14:21:13.179038953+00:00"}
{"url":"https://developer.nvidia.com/blog/delivering-massive-performance-leaps-for-mixture-of-experts-inference-on-nvidia-blackwell/","title":"Delivering Massive Performance Leaps for Mixture of Experts Inference on NVIDIA Blackwell | NVIDIA Technical Blog","source":"Search: last 90 days Blackwell B200 tensor core GEMM performance analysis CUDA","published":"2026-01-07","fetched_at":"2026-02-05T14:21:49.184932017+00:00"}
{"url":"https://www.clarifai.com/blog/nvidia-b200-gpu-guide/","title":"NVIDIA B200 GPU Guide: Use Cases, Models, Benchmarks & AI Scale","source":"Search: last 90 days Blackwell B200 tensor core GEMM performance analysis CUDA","published":"2026-01-22","fetched_at":"2026-02-05T14:21:55.445901817+00:00"}
{"url":"https://docs.nvidia.com/deeplearning/cudnn/frontend/latest/operations/Attention.html","title":"Attention — NVIDIA cuDNN Frontend","source":"Search: last 90 days FlashAttention-3 CUDA kernel matmul attention tensor cores","published":"2026-02-03","fetched_at":"2026-02-05T14:22:54.347869623+00:00"}
{"url":"https://pytorch.org/blog/portable-paged-attention-in-helion/","title":"Portable Paged Attention in Helion","source":"Search: last 90 days FlashAttention-3 CUDA kernel matmul attention tensor cores","published":"2026-02-03","fetched_at":"2026-02-05T14:22:57.929588333+00:00"}
{"url":"https://www.phoronix.com/news/Intel-XeGPU-Dialect-MLIR-LLVM","title":"Intel Proposing XeGPU Dialect For LLVM MLIR - Phoronix","source":"Search: last 90 days MLIR LLVM GPU compiler GEMM codegen tensor cores","published":"2026-01-31","fetched_at":"2026-02-05T14:23:22.624757825+00:00"}
{"url":"https://pytorch.org/blog/accelerating-mamba2-with-kernel-fusion/","title":"Accelerating Mamba2 with Kernel Fusion – PyTorch","source":"Search: last 90 days Triton compiler matmul kernel fusion persistent kernels research","published":"2026-02-06","fetched_at":"2026-02-07T14:11:07.015008904+00:00"}
{"url":"https://docs.nvidia.cn/cuda/cuda-toolkit-release-notes/index.html","title":"CUDA Toolkit 13.1 Update 1 - Release Notes — Release Notes 13.1 documentation","source":"Search: last 90 days cuBLASLt FP8 GEMM autotuning heuristics release notes","published":"2026-02-04","fetched_at":"2026-02-09T14:27:47.652308128+00:00"}
{"url":"https://developer.nvidia.com/cublas","title":"cuBLAS | NVIDIA Developer","source":"Search: last 90 days cuBLASLt FP8 GEMM autotuning heuristics release notes","published":"2026-02-08","fetched_at":"2026-02-09T14:27:52.351769199+00:00"}
{"url":"https://docs.habana.ai/en/latest/Release_Notes/Release%20Notes%20v1.12.0.html","title":"Release Notes v1.12.0 — Gaudi Documentation 1.23.0 documentation","source":"Search: last 90 days cuBLASLt FP8 GEMM autotuning heuristics release notes","published":"2026-02-05","fetched_at":"2026-02-09T14:27:56.528689673+00:00"}
{"url":"https://github.com/NVIDIA/TransformerEngine/releases","title":"Releases · NVIDIA/TransformerEngine · GitHub","source":"Search: last 90 days cuBLASLt FP8 GEMM autotuning heuristics release notes","published":"2026-01-09","fetched_at":"2026-02-09T14:28:11.351381264+00:00"}
{"url":"https://developer.nvidia.com/blog/how-to-write-high-performance-matrix-multiply-in-nvidia-cuda-tile/","title":"How to Write High-Performance Matrix Multiply in NVIDIA CUDA Tile | NVIDIA Technical Blog","source":"Search: last 90 days GPU systolic array matmul mapping tensor core MMA instruction scheduling","published":"2026-01-19","fetched_at":"2026-02-09T14:28:51.416755611+00:00"}
{"url":"https://vjkrish.com/2026/01/19/Mma_Layouts.html","title":"Hopper/Blackwell Tensor Core MMA layouts | vj-krish","source":"Search: last 90 days GPU systolic array matmul mapping tensor core MMA instruction scheduling","published":"2026-01-19","fetched_at":"2026-02-09T14:28:56.617506498+00:00"}
{"url":"https://forums.developer.nvidia.com/t/sm121-cutlass-kernel-optimization-results-nvfp4-356-tflops-moe-grouped-gemm-on-dgx-spark/359960","title":"SM121 CUTLASS Kernel Optimization Results: NVFP4 356 TFLOPS, MoE Grouped GEMM on DGX Spark","source":"Search: last 90 days CUTLASS 3.x new GEMM kernels SM90 SM100 tutorial blog","published":"2026-02-07","fetched_at":"2026-02-11T14:28:00.641109305+00:00"}
{"url":"https://arxiv.org/abs/2602.03067","title":"FlashSinkhorn: IO-Aware Entropic Optimal Transport","source":"Search: last 90 days Triton compiler matmul kernel fusion persistent kernels research","published":"2026-02-03","fetched_at":"2026-02-11T14:28:27.249446305+00:00"}
{"url":"https://www.emergentmind.com/topics/fused-triton-kernels","title":"Fused Triton Kernels in LLM Optimization","source":"Search: last 90 days Triton compiler matmul kernel fusion persistent kernels research","published":"2026-01-07","fetched_at":"2026-02-11T14:28:30.863612788+00:00"}
{"url":"https://arxiv.org/abs/2602.10478v1","title":"GPU-Fuzz: Finding Memory Errors in Deep Learning Frameworks","source":"arXiv: ML + GPU","published":"2026-02-11","fetched_at":"2026-02-12T14:24:51.049542717+00:00"}
{"url":"https://docs.nvidia.com/cutlass/latest/overview.html","title":"Overview — NVIDIA CUTLASS Documentation","source":"Search: last 90 days Hopper H100 tensor cores FP8 GEMM performance analysis CUTLASS cuBLASLt","published":"2026-02-05","fetched_at":"2026-02-12T14:25:19.063321186+00:00"}
{"url":"https://hamzaelshafie.bearblog.dev/worklog-optimising-gemm-on-nvidia-h100-for-cublas-like-performance-wip/","title":"Worklog: Optimising GEMM on NVIDIA H100 for cuBLAS-like Performance (WIP) | Hamza's Blog","source":"Search: last 90 days Hopper H100 tensor cores FP8 GEMM performance analysis CUTLASS cuBLASLt","published":"2026-01-12","fetched_at":"2026-02-12T14:25:24.550573149+00:00"}
{"url":"https://medium.com/programmed-iq/the-silicon-triad-why-cpus-gpus-and-tpus-are-reshaping-the-future-of-ai-34212282ae67","title":"The Silicon Triad: Why CPUs, GPUs and TPUs Are Reshaping the Future of AI","source":"Search: last 90 days systolic array vs GPU tensor cores matmul mapping research AI accelerator comparison","published":"2026-01-20","fetched_at":"2026-02-12T14:26:41.451874498+00:00"}
{"url":"https://uplatz.com/blog/the-silicon-divergence-a-comprehensive-analysis-of-heterogeneous-computing-architectures-and-workload-placement-strategies/","title":"The Silicon Divergence: A Comprehensive Analysis of Heterogeneous Computing Architectures and Workload Placement Strategies","source":"Search: last 90 days systolic array vs GPU tensor cores matmul mapping research AI accelerator comparison","published":"2026-01-12","fetched_at":"2026-02-12T14:26:45.733938576+00:00"}
{"url":"https://arxiv.org/abs/2602.11715v1","title":"DICE: Diffusion Large Language Models Excel at Generating CUDA Kernels","source":"arXiv: AI/ML + GPU","published":"2026-02-12","fetched_at":"2026-02-13T14:20:08.132267170+00:00"}
{"url":"https://blog.fal.ai/chasing-6-tb-s-an-mxfp8-quantizer-on-blackwell/","title":"Chasing 6+ TB/s: an MXFP8 quantizer on Blackwell","source":"Search: last 90 days Blackwell B200 tensor core architecture FP4 FP8 matrix multiplication research","published":"2026-01-27","fetched_at":"2026-02-13T14:20:38.521077719+00:00"}
{"url":"https://arxiv.org/abs/2602.06252","title":"D-Legion: A Scalable Many-Core Architecture for Accelerating Matrix Multiplication in Quantized LLMs","source":"Search: last 90 days Blackwell B200 tensor core architecture FP4 FP8 matrix multiplication research","published":"2026-02-05","fetched_at":"2026-02-13T14:20:43.001901036+00:00"}
{"url":"https://arxiv.org/abs/2601.03324","title":"Bare-Metal Tensor Virtualization: Overcoming the Memory Wall in Edge-AI Inference on ARM64","source":"Search: last 90 days Blackwell B200 tensor core architecture FP4 FP8 matrix multiplication research","published":"2026-01-06","fetched_at":"2026-02-13T14:20:49.730966458+00:00"}
{"url":"https://patents.justia.com/patent/20260003590","title":"U.S. Patent Application: System and Method for Optimizing Machine Learning Inference Systems and Processes for Operating a Compiler Therefor (Application #20260003590)","source":"Search: last 90 days GPU systolic array mapping matrix multiplication tensor core MMA scheduling research","published":"2026-01-01","fetched_at":"2026-02-13T14:21:45.663846629+00:00"}
{"url":"https://forums.developer.nvidia.com/t/why-can-the-mma-instruction-only-reach-50-peak-computing-throughput/359745","title":"Why can the mma instruction only reach 50% peak computing throughput? - Jetson Thor - NVIDIA Developer Forums","source":"Search: last 90 days Blackwell B200 tensor core MMA instruction set GEMM performance analysis","published":"2026-02-05","fetched_at":"2026-02-14T14:10:52.833998643+00:00"}
{"url":"https://nvidia.github.io/TensorRT-LLM/1.1.0rc2.post1/blogs/tech_blog/blog3_Optimizing_DeepSeek_R1_Throughput_on_NVIDIA_Blackwell_GPUs.html","title":"Optimizing DeepSeek R1 Throughput on NVIDIA Blackwell GPUs: A Deep Dive for Developers — TensorRT-LLM","source":"Search: last 90 days cuBLASLt GEMM autotuning heuristics FP8 BF16 performance regression study","published":"2026-01-24","fetched_at":"2026-02-14T14:11:17.791971120+00:00"}
{"url":"https://next.redhat.com/2026/02/12/from-hand-tuned-to-generated-a-reproducible-triton-gpu-kernel-benchmark-across-different-vendors/","title":"From hand-tuned to generated: A reproducible Triton GPU kernel benchmark across different vendors - Red Hat Emerging Technologies","source":"Search: last 90 days Triton compiler matmul kernel fusion FP8 tensor cores performance","published":"2026-02-12","fetched_at":"2026-02-15T14:12:00.564810812+00:00"}
{"url":"https://www.emergentmind.com/topics/flashattention-fa","title":"FlashAttention: Efficient Tiled Self-Attention","source":"Search: last 90 days FlashAttention-3 implementation details Hopper tensor cores GEMM scheduling","published":"2026-01-25","fetched_at":"2026-02-15T14:12:11.728740618+00:00"}
{"url":"https://www.reddit.com/r/CUDA/comments/1qesi1t/high_throughput_injected_ptx_parallel_compilation/","title":"High throughput injected PTX parallel compilation","source":"Search: last 90 days CUDA 12.4 12.5 PTX wgmma GMMA matmul instruction documentation","published":"2026-01-16","fetched_at":"2026-02-15T14:12:35.503265756+00:00"}
{"url":"https://arxiv.org/abs/2602.10478v2","title":"GPU-Fuzz: Finding Memory Errors in Deep Learning Frameworks","source":"arXiv: ML + GPU","published":"2026-02-11","fetched_at":"2026-02-16T14:21:24.561482828+00:00"}
{"url":"https://arxiv.org/abs/2602.12305v1","title":"OptiML: An End-to-End Framework for Program Synthesis and CUDA Kernel Optimization","source":"arXiv: AI/ML + GPU","published":"2026-02-12","fetched_at":"2026-02-16T14:21:39.228934207+00:00"}
{"url":"https://pypi.org/project/NATTEN/","title":"NATTEN · PyPI","source":"Search: last 90 days CUTLASS 3.x new GEMM kernels SM90 SM100 blog paper","published":"2026-02-08","fetched_at":"2026-02-16T14:22:18.431590081+00:00"}
{"url":"https://pytorch.org/blog/warp-specialization-in-triton-design-and-roadmap/","title":"Warp Specialization in Triton: Design and Roadmap","source":"Search: last 90 days Triton compiler matmul kernel fusion epilogue scheduling research","published":"2026-01-08","fetched_at":"2026-02-16T14:22:40.578235863+00:00"}
{"url":"https://arxiv.org/abs/2602.14293v1","title":"KernelBlaster: Continual Cross-Task CUDA Optimization via Memory-Augmented In-Context Reinforcement Learning","source":"arXiv: AI/ML + GPU","published":"2026-02-15","fetched_at":"2026-02-17T14:22:26.655776446+00:00"}
{"url":"https://docs.nvidia.com/megatron-core/developer-guide/latest/user-guide/features/moe.html","title":"Mixture of Experts — Megatron Core (Developer Guide)","source":"Search: last 90 days cuBLASLt grouped GEMM FP8 BF16 autotuning research","published":"2026-02-15","fetched_at":"2026-02-17T14:23:16.194394007+00:00"}
{"url":"https://arxiv.org/abs/2602.11808","title":"Deep Kernel Fusion for Transformers","source":"Search: last 90 days Triton compiler matmul kernel fusion shared memory pipelining paper","published":"2026-02-12","fetched_at":"2026-02-17T14:23:34.562410243+00:00"}
{"url":"https://maknee.github.io/blog/2026/NVIDIA-TileIR-Internals-from-CuTile-to-MLIR-LLVM-to-SASS/","title":"NVIDIA TileIR Internals: from CuTile to MLIR/LLVM to SASS | Henry Zhu","source":"Search: last 90 days MLIR LLVM NVPTX GPU compiler matmul tiling tensor core codegen research","published":"2026-02-03","fetched_at":"2026-02-17T14:24:03.639064564+00:00"}
{"url":"https://docs.nvidia.com/cutlass/4.3.5/media/docs/cpp/cutlass_3x_backwards_compatibility.html","title":"CUTLASS 3.0 GEMM Backwards Compatibility — NVIDIA CUTLASS Documentation","source":"Search: last 90 days CUTLASS 3.x grouped GEMM kernel fusion epilogue research","published":"2026-01-09","fetched_at":"2026-02-18T14:22:32.475019086+00:00"}
{"url":"https://arxiv.org/abs/2602.10718","title":"SnapMLA: Efficient Long-Context MLA Decoding via Hardware-Aware FP8 Quantized Pipelining","source":"Search: last 90 days FlashAttention-3 CUDA kernel implementation tensor cores FP8 research","published":"2026-02-11","fetched_at":"2026-02-18T14:23:09.349392141+00:00"}
{"url":"https://arxiv.org/abs/2602.06072","title":"PackInfer: Compute- and I/O-Efficient Attention for Batched LLM Inference","source":"Search: last 90 days FlashAttention-3 CUDA kernel implementation tensor cores FP8 research","published":"2026-02-03","fetched_at":"2026-02-18T14:23:13.442024284+00:00"}
{"url":"https://yuv.ai/blog/flashmla","title":"FlashMLA: DeepSeek's CUDA Kernels for Lightning-Fast LLM Inference","source":"Search: last 90 days FlashAttention-3 CUDA kernel implementation tensor cores FP8 research","published":"2026-01-29","fetched_at":"2026-02-18T14:23:18.374727123+00:00"}
{"url":"https://www.cnblogs.com/notlate-cn/p/19526556","title":"如何基于 MLIR 实现自动调优 (GPU & Ascend NPU) - 稳住·能赢 - 博客园","source":"Search: last 90 days MLIR LLVM NVPTX GPU compiler matmul fusion tensor core lowering research","published":"2026-01-28","fetched_at":"2026-02-18T14:23:36.939961543+00:00"}
{"url":"https://arxiv.org/abs/2602.17206v1","title":"SoftDTW-CUDA-Torch: Memory-Efficient GPU-Accelerated Soft Dynamic Time Warping for PyTorch","source":"arXiv: GPU OR CUDA","published":"2026-02-19","fetched_at":"2026-02-20T14:18:43.224540998+00:00"}
{"url":"https://arxiv.org/abs/2602.16719v1","title":"GPU-Accelerated Algorithms for Graph Vector Search: Taxonomy, Empirical Study, and Research Directions","source":"arXiv: AI/ML + GPU","published":"2026-02-10","fetched_at":"2026-02-20T14:18:49.167677205+00:00"}
{"url":"https://forums.developer.nvidia.com/t/fp4-on-dgx-spark-why-it-doesnt-scale-like-youd-expect/360142","title":"FP4 on DGX Spark — Why It Doesn't Scale Like You'd Expect - DGX Spark / GB10 - NVIDIA Developer Forums","source":"Search: last 90 days Blackwell B200 tensor core architecture FP4 FP8 matrix multiplication research","published":"2026-02-09","fetched_at":"2026-02-20T14:19:25.003263988+00:00"}
{"url":"https://blog.vllm.ai/2026/02/03/dsr1-gb200-part1.html","title":"Driving vLLM WideEP and Large-Scale Serving Toward Maturity on Blackwell (Part I) | vLLM Blog","source":"Search: last 90 days Blackwell B200 tensor core architecture FP4 FP8 matrix multiplication research","published":"2026-02-03","fetched_at":"2026-02-20T14:19:28.059711339+00:00"}
{"url":"https://blog.ayghri.me/posts/ml/gpu/triton-kernels/","title":"GEMM Triton kernel - Ayoub's Blog","source":"Search: last 90 days Triton compiler matmul kernel scheduling shared memory pipelining research","published":"2026-02-06","fetched_at":"2026-02-20T14:20:02.583415689+00:00"}
{"url":"https://arxiv.org/abs/2602.11016","title":"From Buffers to Registers: Unlocking Fine-Grained FlashAttention with Hybrid-Bonded 3D NPU Co-Design","source":"Search: last 90 days FlashAttention-3 CUDA kernel implementation tensor cores matmul attention research","published":"2026-02-11","fetched_at":"2026-02-20T14:20:21.039529149+00:00"}
{"url":"https://www.reddit.com/r/ROCm/comments/1qt95qe/flash_attention_issues_with_rocm_linux/","title":"Flash Attention Issues With ROCm Linux","source":"Search: last 90 days FlashAttention-3 CUDA kernel implementation tensor cores matmul attention research","published":"2026-02-01","fetched_at":"2026-02-20T14:20:25.354847782+00:00"}
{"url":"https://arxiv.org/abs/2602.15166","title":"Fast and Fusiest: An Optimal Fusion-Aware Mapper for Accelerator Modeling and Evaluation","source":"Search: last 90 days GPU kernel fusion compiler passes for GEMM epilogue activation layernorm research","published":"2026-02-16","fetched_at":"2026-02-20T14:20:45.709642125+00:00"}
{"url":"https://runmat.org/docs/fusion-guide","title":"Fusion Guide | Docs | RunMat","source":"Search: last 90 days GPU kernel fusion compiler passes for GEMM epilogue activation layernorm research","published":"2026-02-06","fetched_at":"2026-02-20T14:20:48.165750650+00:00"}
{"url":"https://triton-lang.org/main/getting-started/tutorials/05-layer-norm.html","title":"Layer Normalization — Triton documentation","source":"Search: last 90 days GPU kernel fusion compiler passes for GEMM epilogue activation layernorm research","published":"2026-02-14","fetched_at":"2026-02-20T14:20:52.613122367+00:00"}
{"url":"https://www.emergentmind.com/topics/nvidia-gb10-grace-blackwell","title":"NVIDIA GB10 Grace Blackwell Architecture","source":"Search: last 90 days Blackwell B200 tensor core architecture GEMM performance analysis","published":"2026-01-31","fetched_at":"2026-02-21T14:09:25.464861856+00:00"}
{"url":"https://docs.nvidia.com/cutlass/4.3.2/overview.html","title":"Overview — NVIDIA CUTLASS Documentation","source":"Search: last 90 days Blackwell B200 tensor core architecture GEMM performance analysis","published":"2025-12-21","fetched_at":"2026-02-21T14:09:28.814116893+00:00"}
{"url":"https://medium.com/%40rlee4408/optimizing-gemm-on-rtx-4060-a-practical-guide-to-cuda-tensor-cores-cutlass-and-cublas-93a52e84081a","title":"Optimizing GEMM on RTX 4060: A Practical Guide to CUDA, Tensor Cores, CUTLASS, and cuBLAS","source":"Search: last 90 days CUTLASS 3.x new GEMM kernels SM90 SM100 tutorial blog","published":"2026-02-04","fetched_at":"2026-02-21T14:09:46.115431112+00:00"}
{"url":"https://timashov.ai/blog/2026/flash-attention/","title":"FlashAttention-2 in Triton: From GPU Mental Models to Kernel Performance | Aleksandr Timashov","source":"Search: last 90 days Triton compiler matmul kernel fusion shared memory pipelining research","published":"2026-01-31","fetched_at":"2026-02-21T14:10:14.163490422+00:00"}
{"url":"https://medium.com/%40kvnagesh/architectural-divergence-in-ai-accelerators-beyond-the-memory-wall-and-systolic-arrays-de19dd2d973f","title":"Architectural Divergence in AI Accelerators: Beyond the Memory Wall and Systolic Arrays","source":"Search: last 90 days systolic array vs GPU tensor cores matrix multiplication roofline analysis","published":"2026-01-01","fetched_at":"2026-02-21T14:10:28.906656800+00:00"}
{"url":"https://blog.vllm.ai/2026/02/01/gpt-oss-optimizations.html","title":"GPT-OSS Performance Optimizations on NVIDIA Blackwell: Pushing the Pareto Frontier | vLLM Blog","source":"Search: last 90 days Blackwell B200 GB200 tensor core architecture matmul GEMM throughput whitepaper analysis","published":"2026-02-01","fetched_at":"2026-02-22T14:10:28.371698598+00:00"}
{"url":"https://arxiv.org/abs/2601.08082","title":"Hierarchical Precision and Recursion for Accelerating Symmetric Linear Solves on MXUs","source":"Search: last 90 days Blackwell B200 GB200 tensor core architecture matmul GEMM throughput whitepaper analysis","published":"2026-01-12","fetched_at":"2026-02-22T14:10:32.726061037+00:00"}
{"url":"https://arxiv.org/abs/2601.14243","title":"Jet-RL: Enabling On-Policy FP8 Reinforcement Learning with Unified Training and Rollout Precision Flow","source":"Search: last 90 days cuBLASLt matmul heuristics algorithm selection workspace tuning FP8 BF16","published":"2026-01-20","fetched_at":"2026-02-22T14:11:23.447834309+00:00"}
{"url":"https://arxiv.org/abs/2601.18150","title":"FP8-RL: A Practical and Stable Low-Precision Stack for LLM Reinforcement Learning","source":"Search: last 90 days cuBLASLt matmul heuristics algorithm selection workspace tuning FP8 BF16","published":"2026-01-26","fetched_at":"2026-02-22T14:11:27.026190364+00:00"}
{"url":"https://arxiv.org/abs/2602.09539","title":"Tensor CUR Decomposition under the Linear-Map-Based Tensor-Tensor Multiplication","source":"Search: last 90 days systolic array vs tensor core GPU matrix multiplication mapping research preprint","published":"2026-02-10","fetched_at":"2026-02-23T14:24:25.654046267+00:00"}
{"url":"https://arxiv.org/abs/2602.11041","title":"Exploiting the Structure in Tensor Decompositions for Matrix Multiplication","source":"Search: last 90 days systolic array vs tensor core GPU matrix multiplication mapping research preprint","published":"2026-02-11","fetched_at":"2026-02-23T14:24:29.358457936+00:00"}
{"url":"https://arxiv.org/abs/2602.14293","title":"KernelBlaster: Continual Cross-Task CUDA Optimization via Memory-Augmented In-Context Reinforcement Learning","source":"Search: last 90 days CUDA 12.4 12.5 PTX wgmma mma instructions GEMM kernel optimization","published":"2026-02-15","fetched_at":"2026-02-24T14:26:24.553581904+00:00"}
{"url":"https://nvidia.github.io/TensorRT-LLM/1.2.0rc8/blogs/tech_blog/blog15_Optimizing_DeepSeek_V32_on_NVIDIA_Blackwell_GPUs.html","title":"Optimizing DeepSeek-V3.2 on NVIDIA Blackwell GPUs — TensorRT LLM","source":"Search: last 90 days Blackwell B200 GB200 tensor core matmul performance analysis","published":"2026-01-11","fetched_at":"2026-02-25T14:24:45.140646025+00:00"}
{"url":"https://docs.nvidia.com/cutlass/4.3.5/media/docs/cpp/heuristics.html","title":"GEMM Heuristics — NVIDIA CUTLASS Documentation","source":"Search: last 90 days cuBLASLt GEMM autotuning heuristics SM90 SM100 blog","published":"2026-01-01","fetched_at":"2026-02-25T14:25:09.720786655+00:00"}
{"url":"https://arxiv.org/abs/2602.19762","title":"Hexagon-MLIR: An AI Compilation Stack For Qualcomm's Neural Processing Units (NPUs)","source":"Search: last 90 days Triton compiler matmul kernel fusion persistent kernels research","published":"2026-02-23","fetched_at":"2026-02-25T14:25:29.201139453+00:00"}
{"url":"https://arxiv.org/abs/2512.23914","title":"Hardware Acceleration for Neural Networks: A Comprehensive Survey","source":"Search: last 90 days systolic array vs tensor cores matrix multiplication GPU architecture study","published":"2025-12-30","fetched_at":"2026-02-25T14:25:52.976564740+00:00"}
{"url":"https://blog.fal.ai/crafting-efficient-kernels-with-epilogue-fusion/","title":"Crafting Efficient Kernels with Epilogue Fusion","source":"Search: last 90 days CUTLASS 3.x GEMM kernel fusion Hopper Blackwell performance blog","published":"2026-02-03","fetched_at":"2026-02-26T14:25:30.764245720+00:00"}
{"url":"https://docs.nvidia.com/cuda/cublasmp/release_notes/index.html","title":"Release Notes — cuBLASMp","source":"Search: last 90 days cuBLASLt new features FP8 BF16 GEMM Hopper Blackwell release notes","published":"2026-02-09","fetched_at":"2026-02-26T14:25:44.930918441+00:00"}
{"url":"https://launchpad.net/ubuntu/resolute/ppc64el/libcublaslt12","title":"libcublaslt12 binary package in Ubuntu Resolute ppc64el","source":"Search: last 90 days cuBLASLt new features FP8 BF16 GEMM Hopper Blackwell release notes","published":"2026-01-29","fetched_at":"2026-02-26T14:25:48.708583242+00:00"}
{"url":"https://pytorch.org/blog/some-matrix-multiplication-engines-are-not-as-accurate-as-we-thought/","title":"Some Matrix Multiplication Engines Are Not As Accurate As We Thought","source":"Search: last 90 days Triton compiler matmul autotuning FP8 tensor cores research","published":"2026-02-12","fetched_at":"2026-02-26T14:26:00.066893003+00:00"}
{"url":"https://blockchain.news/news/nvidia-cutile-python-matrix-multiply-blackwell-tutorial","title":"NVIDIA cuTile Python Guide Shows 90% cuBLAS Performance for Matrix Ops","source":"Search: last 90 days CUDA 12.4 12.5 matmul performance improvements wgmma Hopper Blackwell","published":"2026-01-14","fetched_at":"2026-02-26T14:26:31.453942430+00:00"}
{"url":"https://arxiv.org/abs/2601.10035","title":"A Compute and Communication Runtime Model for Loihi 2","source":"Search: last 90 days AI accelerator research matrix multiplication dataflow SRAM bandwidth roofline 2025","published":"2026-01-15","fetched_at":"2026-02-26T14:26:45.334646930+00:00"}
{"url":"https://www.mdpi.com/3042-5999/2/1/2","title":"A Novel SRAM In-Memory Computing Accelerator Design Approach with R2R-Ladder for AI Sensors and Eddy Current Testing","source":"Search: last 90 days AI accelerator research matrix multiplication dataflow SRAM bandwidth roofline 2025","published":"2026-01-15","fetched_at":"2026-02-26T14:26:48.770576759+00:00"}
