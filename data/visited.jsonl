{"url":"https://arxiv.org/abs/2512.02551v2","title":"CUDA-L2: Surpassing cuBLAS Performance for Matrix Multiplication through Reinforcement Learning","source":"arXiv: GPU OR CUDA","published":"2025-12-02","fetched_at":"2025-12-22T22:05:35.528485173+00:00"}
{"url":"https://arxiv.org/abs/2510.02894v1","title":"PyRadiomics-cuda: a GPU-accelerated 3D features extraction from medical images within PyRadiomics","source":"arXiv: GPU OR CUDA","published":"2025-10-03","fetched_at":"2025-12-22T22:05:39.392452519+00:00"}
{"url":"https://arxiv.org/abs/2511.02132v1","title":"Optimizing Attention on GPUs by Exploiting GPU Architectural NUMA Effects","source":"arXiv: ML + GPU","published":"2025-11-03","fetched_at":"2025-12-22T22:05:44.044068358+00:00"}
{"url":"https://arxiv.org/abs/2512.07853v1","title":"GPU Memory Prediction for Multimodal Model Training","source":"arXiv: ML + GPU","published":"2025-11-26","fetched_at":"2025-12-22T22:05:47.749870378+00:00"}
{"url":"https://arxiv.org/abs/2512.09472v1","title":"WarmServe: Enabling One-for-Many GPU Prewarming for Multi-LLM Serving","source":"arXiv: ML + GPU","published":"2025-12-10","fetched_at":"2025-12-22T22:05:53.592265725+00:00"}
{"url":"https://arxiv.org/abs/2511.00025v1","title":"On the Structure of Floating-Point Noise in Batch-Invariant GPU Matrix Multiplication","source":"arXiv: Matrix Multiplication + GPU","published":"2025-10-26","fetched_at":"2025-12-22T22:05:57.892405132+00:00"}
{"url":"https://arxiv.org/abs/2512.04226v1","title":"tritonBLAS: Triton-based Analytical Approach for GEMM Kernel Parameter Selection","source":"arXiv: Matrix Multiplication + GPU","published":"2025-12-03","fetched_at":"2025-12-22T22:06:01.960172404+00:00"}
{"url":"https://arxiv.org/abs/2510.03760v1","title":"EvoEngineer: Mastering Automated CUDA Kernel Code Evolution with Large Language Models","source":"arXiv: AI/ML + GPU","published":"2025-10-04","fetched_at":"2025-12-22T22:06:06.124690032+00:00"}
{"url":"https://arxiv.org/abs/2510.19873v1","title":"From Large to Small: Transferring CUDA Optimization Expertise via Reasoning Graph","source":"arXiv: AI/ML + GPU","published":"2025-10-22","fetched_at":"2025-12-22T22:06:10.750339230+00:00"}
{"url":"https://arxiv.org/abs/2511.17594v1","title":"AutoSAGE: Input-Aware CUDA Scheduling for Sparse GNN Aggregation (SpMM/SDDMM) and CSR Attention","source":"arXiv: AI/ML + GPU","published":"2025-11-17","fetched_at":"2025-12-22T22:06:15.264630374+00:00"}
{"url":"https://arxiv.org/abs/2511.19493v1","title":"RFX: High-Performance Random Forests with GPU Acceleration and QLORA Compression","source":"arXiv: AI/ML + GPU","published":"2025-11-23","fetched_at":"2025-12-22T22:06:20.145315106+00:00"}
{"url":"https://arxiv.org/abs/2511.01884v2","title":"CudaForge: An Agent Framework with Hardware Feedback for CUDA Kernel Optimization","source":"arXiv: AI/ML + GPU","published":"2025-10-23","fetched_at":"2025-12-22T22:06:24.278273543+00:00"}
{"url":"https://arxiv.org/abs/2512.14080","title":"SonicMoE: Accelerating MoE with IO and Tile-aware Optimizations","source":"Search: last 90 days Hopper H100 tensor cores GEMM research paper warp-group MMA FP8","published":"2025-12-16","fetched_at":"2025-12-22T22:07:08.699785573+00:00"}
{"url":"https://arxiv.org/abs/2512.02189","title":"Microbenchmarking NVIDIA's Blackwell Architecture: An in-depth Architectural Analysis","source":"Search: last 90 days Blackwell B200 GB200 tensor core architecture matmul throughput analysis","published":"2025-12-01","fetched_at":"2025-12-22T22:07:20.836961190+00:00"}
{"url":"https://arxiv.org/abs/2512.02551","title":"CUDA-L2: Surpassing cuBLAS Performance for Matrix Multiplication through Reinforcement Learning","source":"Search: last 90 days cuBLASLt FP8 GEMM autotuning heuristics performance regression study","published":"2025-12-02","fetched_at":"2025-12-22T22:07:55.454977127+00:00"}
{"url":"https://arxiv.org/abs/2511.18674","title":"Low-Rank GEMM: Efficient Matrix Multiplication via Low-Rank Approximation with FP8 Acceleration","source":"Search: last 90 days cuBLASLt FP8 GEMM autotuning heuristics performance regression study","published":"2025-11-24","fetched_at":"2025-12-22T22:07:58.897279888+00:00"}
{"url":"https://arxiv.org/abs/2510.20271v1","title":"Scalable GPU-Accelerated Euler Characteristic Curves: Optimization and Differentiable Learning for PyTorch","source":"arXiv: AI/ML + GPU","published":"2025-10-23","fetched_at":"2025-12-24T19:05:43.034999706+00:00"}
{"url":"https://github.com/NVIDIA/cutlass/releases/tag/v4.3.4","title":"CUTLASS 4.3.4 (GitHub Release)","source":"Search: last 90 days CUTLASS 3.x new GEMM kernels SM90 SM100 warp-specialized epilogue fusion","published":"2025-12-24","fetched_at":"2025-12-24T19:06:05.202135101+00:00"}
{"url":"https://triton-lang.org/main/getting-started/tutorials/09-persistent-matmul.html","title":"Persistent Matmul — Triton documentation","source":"Search: last 90 days Triton compiler matmul autotuning persistent kernels FP16 BF16 FP8","published":"2025-12-20","fetched_at":"2025-12-24T19:06:18.461976927+00:00"}
{"url":"https://lubits.ch/flash/Part-3","title":"Flash Attention from Scratch Part 3: Kernel 1","source":"Search: last 90 days FlashAttention-3 CUDA kernel details tensor core MMA pipeline shared memory scheduling","published":"2025-12-21","fetched_at":"2025-12-24T19:06:31.896526743+00:00"}
{"url":"https://lubits.ch/flash/Appendix","title":"Flash Attention from Scratch: Appendix","source":"Search: last 90 days FlashAttention-3 CUDA kernel details tensor core MMA pipeline shared memory scheduling","published":"2025-12-24","fetched_at":"2025-12-24T19:06:35.876246634+00:00"}
{"url":"https://aman.ai/primers/ai/flashattention/","title":"Aman's AI Journal • Primers • FlashAttention","source":"Search: last 90 days FlashAttention-3 CUDA kernel details tensor core MMA pipeline shared memory scheduling","published":"2025-12-20","fetched_at":"2025-12-24T19:06:38.730732064+00:00"}
{"url":"https://aman.ai/primers/ai/model-acceleration/","title":"Aman's AI Journal • Primers • Model Acceleration","source":"Search: last 90 days FlashAttention-3 CUDA kernel details tensor core MMA pipeline shared memory scheduling","published":"2025-12-20","fetched_at":"2025-12-24T19:06:41.989098601+00:00"}
{"url":"https://arxiv.org/abs/2512.12949","title":"FlashFuser: Expanding the Scale of Kernel Fusion for Compute-Intensive Operators via Inter-Core Connection","source":"Search: last 90 days GPU compiler research kernel fusion matmul epilogue fusion ML training throughput","published":"2025-12-15","fetched_at":"2025-12-24T19:07:07.168003932+00:00"}
{"url":"https://arxiv.org/abs/2512.16465","title":"cuPilot: A Strategy-Coordinated Multi-agent Framework for CUDA Kernel Evolution","source":"Search: last 90 days GPU compiler research kernel fusion matmul epilogue fusion ML training throughput","published":"2025-12-18","fetched_at":"2025-12-24T19:07:10.142445174+00:00"}
{"url":"https://arxiv.org/abs/2512.04226","title":"tritonBLAS: Triton-based Analytical Approach for GEMM Kernel Parameter Selection","source":"Search: last 90 days GPU compiler research kernel fusion matmul epilogue fusion ML training throughput","published":"2025-12-03","fetched_at":"2025-12-24T19:07:12.713762080+00:00"}
{"url":"https://www.preprints.org/manuscript/202511.1093/v1","title":"NeuronMM: High-Performance Matrix Multiplicationfor LLM Inference on AWS Trainium (Version 1)","source":"Search: last 90 days AI accelerator research matrix multiplication dataflow tensor core alternatives training inference","published":"2025-11-14","fetched_at":"2025-12-24T19:07:22.934337986+00:00"}
