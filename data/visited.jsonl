{"url":"https://arxiv.org/abs/2512.02551v2","title":"CUDA-L2: Surpassing cuBLAS Performance for Matrix Multiplication through Reinforcement Learning","source":"arXiv: GPU OR CUDA","published":"2025-12-02","fetched_at":"2025-12-22T22:05:35.528485173+00:00"}
{"url":"https://arxiv.org/abs/2510.02894v1","title":"PyRadiomics-cuda: a GPU-accelerated 3D features extraction from medical images within PyRadiomics","source":"arXiv: GPU OR CUDA","published":"2025-10-03","fetched_at":"2025-12-22T22:05:39.392452519+00:00"}
{"url":"https://arxiv.org/abs/2511.02132v1","title":"Optimizing Attention on GPUs by Exploiting GPU Architectural NUMA Effects","source":"arXiv: ML + GPU","published":"2025-11-03","fetched_at":"2025-12-22T22:05:44.044068358+00:00"}
{"url":"https://arxiv.org/abs/2512.07853v1","title":"GPU Memory Prediction for Multimodal Model Training","source":"arXiv: ML + GPU","published":"2025-11-26","fetched_at":"2025-12-22T22:05:47.749870378+00:00"}
{"url":"https://arxiv.org/abs/2512.09472v1","title":"WarmServe: Enabling One-for-Many GPU Prewarming for Multi-LLM Serving","source":"arXiv: ML + GPU","published":"2025-12-10","fetched_at":"2025-12-22T22:05:53.592265725+00:00"}
{"url":"https://arxiv.org/abs/2511.00025v1","title":"On the Structure of Floating-Point Noise in Batch-Invariant GPU Matrix Multiplication","source":"arXiv: Matrix Multiplication + GPU","published":"2025-10-26","fetched_at":"2025-12-22T22:05:57.892405132+00:00"}
{"url":"https://arxiv.org/abs/2512.04226v1","title":"tritonBLAS: Triton-based Analytical Approach for GEMM Kernel Parameter Selection","source":"arXiv: Matrix Multiplication + GPU","published":"2025-12-03","fetched_at":"2025-12-22T22:06:01.960172404+00:00"}
{"url":"https://arxiv.org/abs/2510.03760v1","title":"EvoEngineer: Mastering Automated CUDA Kernel Code Evolution with Large Language Models","source":"arXiv: AI/ML + GPU","published":"2025-10-04","fetched_at":"2025-12-22T22:06:06.124690032+00:00"}
{"url":"https://arxiv.org/abs/2510.19873v1","title":"From Large to Small: Transferring CUDA Optimization Expertise via Reasoning Graph","source":"arXiv: AI/ML + GPU","published":"2025-10-22","fetched_at":"2025-12-22T22:06:10.750339230+00:00"}
{"url":"https://arxiv.org/abs/2511.17594v1","title":"AutoSAGE: Input-Aware CUDA Scheduling for Sparse GNN Aggregation (SpMM/SDDMM) and CSR Attention","source":"arXiv: AI/ML + GPU","published":"2025-11-17","fetched_at":"2025-12-22T22:06:15.264630374+00:00"}
{"url":"https://arxiv.org/abs/2511.19493v1","title":"RFX: High-Performance Random Forests with GPU Acceleration and QLORA Compression","source":"arXiv: AI/ML + GPU","published":"2025-11-23","fetched_at":"2025-12-22T22:06:20.145315106+00:00"}
{"url":"https://arxiv.org/abs/2511.01884v2","title":"CudaForge: An Agent Framework with Hardware Feedback for CUDA Kernel Optimization","source":"arXiv: AI/ML + GPU","published":"2025-10-23","fetched_at":"2025-12-22T22:06:24.278273543+00:00"}
{"url":"https://arxiv.org/abs/2512.14080","title":"SonicMoE: Accelerating MoE with IO and Tile-aware Optimizations","source":"Search: last 90 days Hopper H100 tensor cores GEMM research paper warp-group MMA FP8","published":"2025-12-16","fetched_at":"2025-12-22T22:07:08.699785573+00:00"}
{"url":"https://arxiv.org/abs/2512.02189","title":"Microbenchmarking NVIDIA's Blackwell Architecture: An in-depth Architectural Analysis","source":"Search: last 90 days Blackwell B200 GB200 tensor core architecture matmul throughput analysis","published":"2025-12-01","fetched_at":"2025-12-22T22:07:20.836961190+00:00"}
{"url":"https://arxiv.org/abs/2512.02551","title":"CUDA-L2: Surpassing cuBLAS Performance for Matrix Multiplication through Reinforcement Learning","source":"Search: last 90 days cuBLASLt FP8 GEMM autotuning heuristics performance regression study","published":"2025-12-02","fetched_at":"2025-12-22T22:07:55.454977127+00:00"}
{"url":"https://arxiv.org/abs/2511.18674","title":"Low-Rank GEMM: Efficient Matrix Multiplication via Low-Rank Approximation with FP8 Acceleration","source":"Search: last 90 days cuBLASLt FP8 GEMM autotuning heuristics performance regression study","published":"2025-11-24","fetched_at":"2025-12-22T22:07:58.897279888+00:00"}
{"url":"https://arxiv.org/abs/2510.20271v1","title":"Scalable GPU-Accelerated Euler Characteristic Curves: Optimization and Differentiable Learning for PyTorch","source":"arXiv: AI/ML + GPU","published":"2025-10-23","fetched_at":"2025-12-24T19:05:43.034999706+00:00"}
{"url":"https://github.com/NVIDIA/cutlass/releases/tag/v4.3.4","title":"CUTLASS 4.3.4 (GitHub Release)","source":"Search: last 90 days CUTLASS 3.x new GEMM kernels SM90 SM100 warp-specialized epilogue fusion","published":"2025-12-24","fetched_at":"2025-12-24T19:06:05.202135101+00:00"}
{"url":"https://triton-lang.org/main/getting-started/tutorials/09-persistent-matmul.html","title":"Persistent Matmul — Triton documentation","source":"Search: last 90 days Triton compiler matmul autotuning persistent kernels FP16 BF16 FP8","published":"2025-12-20","fetched_at":"2025-12-24T19:06:18.461976927+00:00"}
{"url":"https://lubits.ch/flash/Part-3","title":"Flash Attention from Scratch Part 3: Kernel 1","source":"Search: last 90 days FlashAttention-3 CUDA kernel details tensor core MMA pipeline shared memory scheduling","published":"2025-12-21","fetched_at":"2025-12-24T19:06:31.896526743+00:00"}
{"url":"https://lubits.ch/flash/Appendix","title":"Flash Attention from Scratch: Appendix","source":"Search: last 90 days FlashAttention-3 CUDA kernel details tensor core MMA pipeline shared memory scheduling","published":"2025-12-24","fetched_at":"2025-12-24T19:06:35.876246634+00:00"}
{"url":"https://aman.ai/primers/ai/flashattention/","title":"Aman's AI Journal • Primers • FlashAttention","source":"Search: last 90 days FlashAttention-3 CUDA kernel details tensor core MMA pipeline shared memory scheduling","published":"2025-12-20","fetched_at":"2025-12-24T19:06:38.730732064+00:00"}
{"url":"https://aman.ai/primers/ai/model-acceleration/","title":"Aman's AI Journal • Primers • Model Acceleration","source":"Search: last 90 days FlashAttention-3 CUDA kernel details tensor core MMA pipeline shared memory scheduling","published":"2025-12-20","fetched_at":"2025-12-24T19:06:41.989098601+00:00"}
{"url":"https://arxiv.org/abs/2512.12949","title":"FlashFuser: Expanding the Scale of Kernel Fusion for Compute-Intensive Operators via Inter-Core Connection","source":"Search: last 90 days GPU compiler research kernel fusion matmul epilogue fusion ML training throughput","published":"2025-12-15","fetched_at":"2025-12-24T19:07:07.168003932+00:00"}
{"url":"https://arxiv.org/abs/2512.16465","title":"cuPilot: A Strategy-Coordinated Multi-agent Framework for CUDA Kernel Evolution","source":"Search: last 90 days GPU compiler research kernel fusion matmul epilogue fusion ML training throughput","published":"2025-12-18","fetched_at":"2025-12-24T19:07:10.142445174+00:00"}
{"url":"https://arxiv.org/abs/2512.04226","title":"tritonBLAS: Triton-based Analytical Approach for GEMM Kernel Parameter Selection","source":"Search: last 90 days GPU compiler research kernel fusion matmul epilogue fusion ML training throughput","published":"2025-12-03","fetched_at":"2025-12-24T19:07:12.713762080+00:00"}
{"url":"https://www.preprints.org/manuscript/202511.1093/v1","title":"NeuronMM: High-Performance Matrix Multiplicationfor LLM Inference on AWS Trainium (Version 1)","source":"Search: last 90 days AI accelerator research matrix multiplication dataflow tensor core alternatives training inference","published":"2025-11-14","fetched_at":"2025-12-24T19:07:22.934337986+00:00"}
{"url":"https://wavespeed.ai/blogs/index.php/2025/12/10/wavespeedai-x-datacrunch-flux-real-time-image-inference-on-b200/","title":"WaveSpeedAI X DataCrunch: FLUX Real-Time Image Inference on B200 - WaveSpeedAI Blog","source":"Search: last 90 days Blackwell B200 tensor core architecture matmul throughput research","published":"2025-12-10","fetched_at":"2025-12-25T14:06:55.345130868+00:00"}
{"url":"https://github.com/NVIDIA/cutlass/releases/tag/v4.3.3","title":"CUTLASS 4.3.3","source":"Search: last 90 days CUTLASS 3.x new GEMM kernels FP8 BF16 SM90 SM100 blog","published":"2025-12-12","fetched_at":"2025-12-25T14:07:08.169757589+00:00"}
{"url":"https://github.com/NVIDIA/cutlass/releases/tag/v4.3.2","title":"CUTLASS 4.3.2","source":"Search: last 90 days CUTLASS 3.x new GEMM kernels FP8 BF16 SM90 SM100 blog","published":"2025-12-05","fetched_at":"2025-12-25T14:07:11.916745387+00:00"}
{"url":"https://arxiv.org/abs/2512.02875","title":"SAT-MapIt: A SAT-based Modulo Scheduling Mapper for Coarse Grain Reconfigurable Architectures","source":"Search: last 90 days CUDA 12.5 12.6 WGMMA GMMA matmul instruction scheduling research","published":"2025-12-02","fetched_at":"2025-12-25T14:08:05.567147033+00:00"}
{"url":"https://docs.nvidia.com/cutlass/4.3.0/media/docs/cpp/gemm_api_3x.html","title":"CUTLASS 3.0 GEMM API — NVIDIA CUTLASS Documentation","source":"Search: last 90 days CUTLASS 3.x new GEMM kernels SM90 SM100 TMA warp-specialized tutorial","published":"2025-12-23","fetched_at":"2025-12-26T14:07:26.173472281+00:00"}
{"url":"https://pytorch.org/blog/cutlass-ping-pong-gemm-kernel/","title":"Deep Dive on CUTLASS Ping-Pong GEMM Kernel – PyTorch","source":"Search: last 90 days CUTLASS 3.x new GEMM kernels SM90 SM100 TMA warp-specialized tutorial","published":"2025-12-22","fetched_at":"2025-12-26T14:07:29.597488464+00:00"}
{"url":"https://docs.nvidia.com/cutlass/4.3.1/CHANGELOG.html","title":"Changelog — NVIDIA CUTLASS Documentation","source":"Search: last 90 days CUTLASS 3.x new GEMM kernels SM90 SM100 TMA warp-specialized tutorial","published":"2025-12-23","fetched_at":"2025-12-26T14:07:31.697878805+00:00"}
{"url":"https://arxiv.org/abs/2512.02371","title":"Pushing Tensor Accelerators Beyond MatMul in a User-Schedulable Language","source":"Search: last 90 days FlashAttention-3 GPU kernel implementation tensor cores matmul research","published":"2025-12-02","fetched_at":"2025-12-27T14:08:26.670819479+00:00"}
{"url":"https://arxiv.org/abs/2512.18134","title":"Optimal Software Pipelining and Warp Specialization for Tensor Core GPUs","source":"Search: last 90 days GPU systolic array mapping GEMM scheduling tensor core research","published":"2025-12-19","fetched_at":"2025-12-27T14:08:39.884584579+00:00"}
{"url":"https://arxiv.org/abs/2512.19250","title":"Small Language Models as Compiler Experts: Auto-Parallelization for Heterogeneous Systems","source":"Search: last 90 days MLIR LLVM CUDA GPU compiler matmul codegen tensor core research","published":"2025-12-22","fetched_at":"2025-12-27T14:08:54.061746347+00:00"}
{"url":"https://docs.nvidia.com/cuda/cublasmp/release_notes","title":"Release Notes — cuBLASMp (cuBLASMp v0.7.0 released November 24, 2025)","source":"Search: last 90 days cuBLASLt GEMM autotuning heuristics FP8 BF16 performance update","published":"2025-11-24","fetched_at":"2025-12-28T14:07:43.987863493+00:00"}
{"url":"https://arxiv.org/abs/2511.02302","title":"FP8-Flow-MoE: A Casting-Free FP8 Recipe without Double Quantization Error","source":"Search: last 90 days cuBLASLt GEMM autotuning heuristics FP8 BF16 performance update","published":"2025-11-04","fetched_at":"2025-12-28T14:07:48.743783276+00:00"}
{"url":"https://pytorch.org/blog/flashattention-3/","title":"FlashAttention-3: Fast and Accurate Attention with Asynchrony and Low-precision","source":"Search: last 90 days FlashAttention 3 CUDA kernel matmul attention tensor cores benchmark","published":"2025-12-23","fetched_at":"2025-12-28T14:08:15.748752901+00:00"}
{"url":"https://github.com/togethercomputer/flash-attention-3","title":"GitHub - togethercomputer/flash-attention-3: Fast and memory-efficient exact attention","source":"Search: last 90 days FlashAttention 3 CUDA kernel matmul attention tensor cores benchmark","published":"2025-12-07","fetched_at":"2025-12-28T14:08:20.017146772+00:00"}
{"url":"https://pytorch.org/blog/hadacore/","title":"HadaCore: Tensor Core Accelerated Hadamard Transform Kernel – PyTorch","source":"Search: last 90 days FlashAttention-3 GPU kernel matmul softmax tensor cores benchmark","published":"2025-12-23","fetched_at":"2025-12-29T14:09:02.878111626+00:00"}
{"url":"https://pytorch.org/blog/hopper-tma-unit/","title":"Deep Dive on the Hopper TMA Unit for FP8 GEMMs – PyTorch","source":"Search: last 90 days Hopper H200 tensor cores FP8 GEMM performance analysis CUTLASS cuBLAS","published":"2025-12-28","fetched_at":"2025-12-30T14:07:35.059318414+00:00"}
{"url":"https://www.emergentmind.com/topics/blackwell-gpu-architecture","title":"Blackwell GPU Architecture","source":"Search: last 90 days Hopper H200 tensor cores FP8 GEMM performance analysis CUTLASS cuBLAS","published":"2025-12-25","fetched_at":"2025-12-30T14:07:39.049097759+00:00"}
{"url":"https://arxiv.org/abs/2512.22219","title":"Mirage Persistent Kernel: A Compiler and Runtime for Mega-Kernelizing Tensor Programs","source":"Search: last 90 days GPU kernel fusion compiler MLIR CUDA matmul epilogue fusion research paper","published":"2025-12-22","fetched_at":"2025-12-30T14:09:11.658881207+00:00"}
{"url":"https://pytorch.org/blog/warp-specialization/","title":"Enabling advanced GPU features in PyTorch – Warp Specialization","source":"Search: last 90 days Triton compiler matmul performance improvements warp-specialization research","published":"2025-12-28","fetched_at":"2025-12-31T14:08:00.768698066+00:00"}
{"url":"https://www.reddit.com/r/CUDA/comments/1px5pj6/about_wgmmamma/","title":"About wgmma.mma_async.sync.aligned.m64n256k16.f16.f16.f16 instruction's descriptors and byte offsets.","source":"Search: last 90 days CUDA 12.6 PTX wgmma mma instruction GEMM optimization research","published":"2025-12-27","fetched_at":"2025-12-31T14:08:29.532262656+00:00"}
{"url":"https://arxiv.org/abs/2512.09196","title":"TritonForge: Profiling-Guided Framework for Automated Triton Kernel Optimization","source":"Search: last 90 days Triton compiler matmul kernel fusion autotuning research preprint","published":"2025-12-09","fetched_at":"2026-01-01T14:07:17.935723261+00:00"}
{"url":"https://www.emergentmind.com/topics/ml-triton","title":"ML-Triton: Multi-Level GPU Compiler","source":"Search: last 90 days GPU compiler MLIR LLVM NVPTX matmul codegen tensor core lowering research","published":"2025-12-25","fetched_at":"2026-01-01T14:08:06.542791786+00:00"}
{"url":"https://arxiv.org/abs/2512.21571","title":"nncase: An End-to-End Compiler for Efficient LLM Deployment on Heterogeneous Storage Architectures","source":"Search: last 90 days GPU compiler MLIR LLVM NVPTX matmul codegen tensor core lowering research","published":"2025-12-25","fetched_at":"2026-01-01T14:08:09.778887857+00:00"}
{"url":"https://www.emergentmind.com/papers/2512.02189","title":"Microbenchmarking NVIDIA's Blackwell Architecture: An in-depth Architectural Analysis","source":"Search: last 90 days NVIDIA Hopper H100 tensor cores GEMM research paper","published":"2025-12-01","fetched_at":"2026-01-02T14:07:00.750252307+00:00"}
{"url":"https://triton-lang.org/main/getting-started/tutorials/10-block-scaled-matmul.html","title":"Block Scaled Matrix Multiplication — Triton documentation","source":"Search: last 90 days Triton compiler matmul autotuning tensor core wgmma research","published":"2025-12-28","fetched_at":"2026-01-02T14:07:30.515300381+00:00"}
{"url":"https://arxiv.org/abs/2511.12653","title":"DPVO-QAT++: Heterogeneous QAT and CUDA Kernel Fusion for High-Performance Deep Patch Visual Odometry","source":"Search: last 90 days GPU kernel fusion compiler matmul epilogue fusion ML training research","published":"2025-11-16","fetched_at":"2026-01-02T14:08:12.580468235+00:00"}
{"url":"https://arxiv.org/abs/2512.13638","title":"Design in Tiles: Automating GEMM Deployment on Tile-Based Many-PE Accelerators","source":"Search: last 90 days Hopper H100 tensor cores FP8 GEMM research paper","published":"2025-12-15","fetched_at":"2026-01-03T14:06:47.322535017+00:00"}
{"url":"https://docs.nvidia.com/cutlass/latest/CHANGELOG.html","title":"CUTLASS 4.x Changelog — NVIDIA CUTLASS Documentation","source":"Search: last 90 days CUTLASS 3.x new GEMM kernels SM90 SM100 performance blog","published":"2025-12-12","fetched_at":"2026-01-03T14:07:13.081738999+00:00"}
{"url":"https://github.com/NVIDIA/cutlass/releases","title":"Releases · NVIDIA/cutlass · GitHub","source":"Search: last 90 days CUTLASS 3.x new GEMM kernels SM90 SM100 performance blog","published":"2025-12-24","fetched_at":"2026-01-03T14:07:16.411251463+00:00"}
{"url":"https://www.aimodels.fyi/papers/arxiv/optimal-software-pipelining-warp-specialization-tensor-core","title":"Optimal Software Pipelining and Warp Specialization for Tensor Core GPUs","source":"Search: last 90 days GPU systolic array vs SIMT tensor core matmul mapping research","published":"2025-12-23","fetched_at":"2026-01-03T14:07:49.797122306+00:00"}
{"url":"https://arxiv.org/abs/2511.13764","title":"Library Liberation: Competitive Performance Matmul Through Compiler-composed Nanokernels","source":"Search: last 90 days GPU compiler MLIR LLVM CUDA matmul scheduling tensor core codegen paper","published":"2025-11-14","fetched_at":"2026-01-03T14:08:07.219923233+00:00"}
{"url":"https://www.modular.com/blog/matrix-multiplication-on-nvidias-blackwell-part-2-using-hardware-features-to-optimize-matmul","title":"Modular: Matrix Multiplication on Blackwell: Part 2 - Using Hardware Features to Optimize Matmul","source":"Search: last 90 days GPU systolic array vs tensor cores matrix multiplication research","published":"2026-01-04","fetched_at":"2026-01-04T14:08:09.198990503+00:00"}
{"url":"https://docs.nvidia.com/cutlass/latest/media/docs/cpp/blackwell_functionality.html","title":"Blackwell SM100 GEMMs — NVIDIA CUTLASS Documentation","source":"Search: last 90 days GPU systolic array mapping GEMM tensor core MMA instruction scheduling research","published":"2026-01-02","fetched_at":"2026-01-05T14:10:11.976070985+00:00"}
{"url":"https://leimao.github.io/blog/NVIDIA-Tensor-Core-MMA-Instruction-TN-Layout/","title":"NVIDIA Tensor Core TN Layout MMA Instruction - Lei Mao's Log Book","source":"Search: last 90 days GPU systolic array mapping GEMM tensor core MMA instruction scheduling research","published":"2025-12-06","fetched_at":"2026-01-05T14:10:16.483764189+00:00"}
